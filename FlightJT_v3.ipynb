{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.30:4041\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1608204283559)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "path: String = /home/georges/IASDfev20/3_BigData/DarioC/TP/MasterIASD_Flights_Project/data/\n",
       "pathA: String = /home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/ONTIME/\n",
       "pathQ: String = /home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/QCLCD/\n",
       "date1: String = 201311\n",
       "date2: String = 201511\n",
       "date3: String = 201711\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialisation chemin des données\n",
    "val path = \"/home/georges/IASDfev20/3_BigData/DarioC/TP/MasterIASD_Flights_Project/data/\"\n",
    "val pathA = \"/home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/ONTIME/\"\n",
    "val pathQ = \"/home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/QCLCD/\"\n",
    "\n",
    "// Initialisation des variables de date\n",
    "val date1 = \"201311\"\n",
    "val date2 = \"201511\"\n",
    "val date3 = \"201711\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Liste des aéroports de référence\n",
    "Les fichiers de vols sont utilisés pour constituer une base de référence des aéroports, via leur code à 3 lettres.\n",
    "\n",
    "Cette liste (une par année par exemple) permet de délimiter le périmètre des stations météo nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_airp1: org.apache.spark.sql.DataFrame = [origin: string, dest: string]\n",
       "data_airp1o: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp1d: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "list_airp1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp2: org.apache.spark.sql.DataFrame = [origin: string, dest: string]\n",
       "data_airp2o: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp2d: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "list_airp2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp3: org.apache.spark.sql.DataFrame = [origin: string, dest: string]\n",
       "data_airp3o: org.apache.spark.sql.Dat...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// liste des aéroports (sur la base de l'origine et de la destination des vols)\n",
    "// utilisation de 3 dates nov 2013, 2015 et 2017\n",
    "\n",
    "val data_airp1 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date1+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .select(\"origin\",\"dest\")\n",
    "val data_airp1o = data_airp1.withColumnRenamed(\"origin\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val data_airp1d = data_airp1.withColumnRenamed(\"dest\",\"CallSign\").select(\"CallSign\").distinct\n",
    "\n",
    "val list_airp1 = data_airp1o.union(data_airp1d).distinct\n",
    "\n",
    "val data_airp2 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date2+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .select(\"origin\",\"dest\")\n",
    "val data_airp2o = data_airp2.withColumnRenamed(\"origin\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val data_airp2d = data_airp2.withColumnRenamed(\"dest\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val list_airp2 = data_airp2o.union(data_airp2d).distinct\n",
    "\n",
    "val data_airp3 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date3+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .select(\"origin\",\"dest\")\n",
    "val data_airp3o = data_airp3.withColumnRenamed(\"origin\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val data_airp3d = data_airp3.withColumnRenamed(\"dest\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val list_airp3 = data_airp3o.union(data_airp3d).distinct\n",
    "\n",
    "val liste_airp = list_airp1.union(list_airp2).union(list_airp3).distinct\n",
    "                          .orderBy($\"CallSign\".asc)\n",
    "println(liste_airp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Préparation des données météo : Weather Observation Table (WOT)\n",
    "\n",
    "La mise en forme des données météo consiste :\n",
    "\n",
    "* Etape 1 - Dans un premier temps à lire les fichiers récupérer le lien dans les fichiers stations.txt entre le code aéroport de trois letttres et le Wban d'identification des stations. Cela permet :\n",
    "    * d'une part de faire limiter le périmètre des stations à la liste des aéroports de référence => Weather Observation Table (OT)\n",
    "    * d'autre part de croiser les données météo avec celle des vols.\n",
    "    \n",
    "* Etape 2 - Dans un second temps à construire, pour chaque station et chaque jour, le relevé des informations météo pour chaque heure (de 0 à 23h) :\n",
    "    * les stations/dates conservées sont celles où l'on retrouve un relevé complet sur 24 heures ;\n",
    "    * les informations météo utilisées (pression, température...) sont de niveau horaire, journalier ou mensuelles et sont conservées pour chaque heure dans un vecteur et indéxées pour les informations de nature qualitative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1/2 - Lecture des fichiers QCLCD => Weather Observation File (WOF)\n",
    "Réduction aux stations correspondant aux aéeroports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Fichier des stations restreint à la liste de référence des aéroports\n",
    "A noter qu'il existe des aéroports pour lesquels aucune station n'est trouvée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb stations : 312\n",
      "+--------+-----+-------------------+-----+------------+---------+--------+\n",
      "|CallSign|WBAN |Name               |State|GroundHeight|Barometer|TimeZone|\n",
      "+--------+-----+-------------------+-----+------------+---------+--------+\n",
      "|BGM     |04725|BINGHAMTON         |NY   |1595        |1630     |-5      |\n",
      "|INL     |14918|INTERNATIONAL FALLS|MN   |1183        |1185     |-6      |\n",
      "|MSY     |12916|NEW ORLEANS        |LA   |4           |7        |-6      |\n",
      "|GEG     |24157|SPOKANE            |WA   |2353        |2384     |-8      |\n",
      "|SNA     |93184|SANTA ANA          |CA   |54          |52       |-8      |\n",
      "+--------+-----+-------------------+-----+------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Nb missed airports : 8\n",
      "+--------+\n",
      "|CallSign|\n",
      "+--------+\n",
      "|PSE     |\n",
      "|PPG     |\n",
      "|SCE     |\n",
      "|SPN     |\n",
      "|UST     |\n",
      "|BKG     |\n",
      "|FCA     |\n",
      "|CLD     |\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_station0: org.apache.spark.sql.DataFrame = [WBAN: string, WMO: string ... 13 more fields]\n",
       "data_station: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 5 more fields]\n",
       "data_missed_airp: org.apache.spark.sql.DataFrame = [CallSign: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "\n",
    "val data_station0 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"|\")\n",
    "                              .load(pathQ+date1+\"station.txt\")\n",
    "                              .withColumnRenamed(\"CallSign\", \"c_CallSign\")\n",
    "\n",
    "val data_station = data_station0.join(liste_airp,data_station0(\"c_CallSign\") === liste_airp(\"CallSign\"),\"inner\")\n",
    "                                .select(\"CallSign\", \"WBAN\",\"Name\",\"State\",\"GroundHeight\", \"Barometer\", \"TimeZone\")\n",
    "\n",
    "print(\"Nb stations : \") \n",
    "println(data_station.count())\n",
    "data_station.show(5, false)\n",
    "\n",
    "\n",
    "// Identification des aéroports sans station\n",
    "val data_missed_airp = liste_airp.join(data_station0, liste_airp(\"CallSign\")===data_station0(\"c_CallSign\"),\"left\")\n",
    "                                 .filter(\"c_CallSign is null\")\n",
    "                                 .select(\"CallSign\")\n",
    "print(\"Nb missed airports : \")\n",
    "println(data_missed_airp.count())\n",
    "data_missed_airp.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Fichier des relevés mensuels (monthly.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly data : 1225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_monthly: org.apache.spark.sql.DataFrame = [c_WBAN: string, YearMonth: string ... 48 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "val data_monthly = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                             .load(pathQ+date1+\"monthly.txt\")\n",
    "                             .withColumnRenamed(\"WBAN\",\"c_WBAN\")\n",
    "print(\"monthly data : \")\n",
    "println(data_monthly.count())\n",
    "\n",
    "/* Finaliser les données mensuelles à récupérer \n",
    "\n",
    "data_monthly.printSchema\n",
    "data_monthly.select(\"Wban\",\"YearMonth\",\"AvgMaxTemp\",\"AvgMinTemp\",\"AvgTemp\",\"HeatingDegreeDays\",\n",
    "                    \"HDDSeasonToDate\",\"ThunderstormDays\",\"HeavyFogDays\").show(false)\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Fichier des relevés quotidiens (daily.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily data : 36750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_daily: org.apache.spark.sql.DataFrame = [c_WBAN: string, YearMonthDay: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "val data_daily = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                             .load(pathQ+date1+\"daily.txt\")\n",
    "                             .withColumnRenamed(\"WBAN\",\"c_WBAN\")\n",
    "                             .select(\"c_WBAN\",\"YearMonthDay\",\"Tmax\",\"Tmin\",\"Tavg\",\"DewPoint\",\"WetBulb\", \"Heat\",\n",
    "                                     \"Cool\",\"Sunrise\",\"Sunset\",\"CodeSum\",\"Depth\",\"Water1\", \"SnowFall\",\n",
    "                                     \"StnPressure\", \"SeaLevel\", \"ResultSpeed\", \"ResultDir\", \"AvgSpeed\", \"Max5Speed\",\n",
    "                                     \"Max5Dir\", \"Max2Speed\", \"Max2Dir\")\n",
    "print(\"daily data : \")\n",
    "println(data_daily.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Fichier des relevés horaires (hourly.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hourly data : 4060568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_hourly: org.apache.spark.sql.DataFrame = [c_WBAN: string, Date: string ... 14 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "val data_hourly = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                       .load(pathQ+date1+\"hourly.txt\")\n",
    "                       .withColumnRenamed(\"WBAN\",\"c_WBAN\")\n",
    "                       .withColumn(\"c_Month\",substring($\"Date\", 1, 6))\n",
    "                       .select(\"c_WBAN\",\"Date\",\"c_Month\",\"Time\",\"StationType\",\"SkyCondition\",\"Visibility\",\"WeatherType\",\n",
    "                               \"DryBulbFarenheit\",\"WetBulbFarenheit\",\"DewPointFarenheit\",\n",
    "                               \"WindSpeed\",\"WindDirection\",\"StationPressure\",\"SeaLevelPressure\",\"HourlyPrecip\")\n",
    "print(\"hourly data : \")\n",
    "println(data_hourly.count())\n",
    "/*\n",
    "\"ValueForWindCharacter\" à voir ?\n",
    "\"PressureTendency\",\"PressureChange\" très peu renseignées (10 obs)\n",
    "data_hourly.select(\n",
    "\"PressureTendency\",\"PressureChange\"\n",
    ").where($\"PressureTendency\" !== \" \").show(50,false)\n",
    "\"RecordType\" : signification ?\n",
    "*/                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (v) Fichier des précipitations (precip.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* L'information semble être déjà reprise dans la variable HourlyPrecip du fichier hourly.txt\n",
    "\n",
    "val data_precip = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                             .load(pathQ+date1+\"precip.txt\")\n",
    "\n",
    "data_hourly.select(\"Wban\", \"Date\", \"Time\", \"HourlyPrecip\").where($\"HourlyPrecip\" !== \" \")\n",
    ".join(data_precip.select(\"Wban\",\"YearMonthDay\",\"Hour\",\"Precipitation\"), \n",
    "     data_hourly(\"Wban\")===data_precip(\"Wban\") && data_hourly(\"Date\")===data_precip(\"YearMonthDay\"), \"inner\")\n",
    ".where($\"Precipitation\" !== \" \")\n",
    ".show(80,false)\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (vi) => Dataframe Weather Observation File (WOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meteo_a : 296924\n",
      "meteo_b : 296924\n",
      "WOF_201311 : 296924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_meteo_a: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 20 more fields]\n",
       "data_meteo_b: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 43 more fields]\n",
       "WOF_201311: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 89 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DATAFRAME DE SYNTHESE regroupant l'ensemble des informations météo pertinentes\n",
    "val data_meteo_a = data_station.join(data_hourly, data_station(\"WBAN\") === data_hourly(\"c_WBAN\"), \"inner\")\n",
    "                               .drop(\"c_WBAN\")\n",
    "print(\"meteo_a : \")\n",
    "println(data_meteo_a.count())\n",
    "\n",
    "val data_meteo_b = data_meteo_a.join(data_daily, data_meteo_a(\"WBAN\") === data_daily(\"c_WBAN\") &&\n",
    "                                                 data_meteo_a(\"Date\") === data_daily(\"YearMonthDay\"), \"left\")\n",
    "                               .drop(\"c_WBAN\")\n",
    "print(\"meteo_b : \")\n",
    "println(data_meteo_b.count())\n",
    "\n",
    "val WOF_201311= data_meteo_b.join(data_monthly, data_meteo_b(\"WBAN\") === data_monthly(\"c_WBAN\") &&\n",
    "                                                data_meteo_b(\"c_Month\") === data_monthly(\"yearMonth\"), \"left\")\n",
    "                            .drop(\"c_WBAN\",\"c_Month\", \"YearMonth\",\"YearMonthDay\")\n",
    "print(\"WOF_201311 : \")\n",
    "println(WOF_201311.count())\n",
    "\n",
    "// WOT_201311.printSchema       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb Wban absents data daily : 784\n",
      "Nb Wban absents data monthly :27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_h: org.apache.spark.sql.DataFrame = [WBAN: string, Date: string]\n",
       "data_d: org.apache.spark.sql.DataFrame = [WBAN: string, Date: string]\n",
       "data_sub: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [WBAN: string, Date: string]\n",
       "data_h2: org.apache.spark.sql.DataFrame = [WBAN: string, c_Month: string]\n",
       "data_m: org.apache.spark.sql.DataFrame = [WBAN: string, c_Month: string]\n",
       "data_sub2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [WBAN: string, c_Month: string]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Remarque : présence de couples WBAN/Date dans le fichier hourly mais absents de daily ou de monthly.\n",
    "\n",
    "A voir si l'on se restreint aux observations pour lesquelles nous disposons à la fois des données horaires,\n",
    "quotidiennes et mensuelles. Ici le périmètre est celui - plus large des données horaires avec valeurs\n",
    "manquantes sur les daily ou monthly donc */\n",
    "\n",
    "// écarts hourly et daily\n",
    "val data_h = data_meteo_a.select(\"WBAN\",\"Date\")\n",
    "val data_d = data_daily.withColumnRenamed(\"c_WBAN\", \"WBAN\")\n",
    "                               .withColumnRenamed(\"YearMonthDay\",\"Date\")\n",
    "                               .select(\"WBAN\",\"Date\")\n",
    "\n",
    "val data_sub = data_h.except(data_d)\n",
    "print(\"Nb Wban absents data daily : \")\n",
    "println(data_sub.count())\n",
    "\n",
    "// écarts hourly et monthly\n",
    "val data_h2 = data_meteo_a.select(\"WBAN\",\"c_Month\")\n",
    "val data_m = data_monthly.withColumnRenamed(\"c_WBAN\", \"WBAN\")\n",
    "                               .withColumnRenamed(\"YearMonth\",\"c_Month\")\n",
    "                               .select(\"WBAN\",\"c_Month\")\n",
    "\n",
    "val data_sub2 = data_h2.except(data_m)\n",
    "print(\"Nb Wban absents data monthly :\")\n",
    "println(data_sub2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2/2 - Mise en forme des données météo => Weather Observation Table (WOT)\n",
    "Cette étape se déroule en deux temps :\n",
    "* Ajustement du périmètre des relevés : uniquement les stations et les jours pour lesquels les relevés horaires sont disponibles chaque heure sur 24 heures (0 à 23h).\n",
    "* Création d'une colonne Weather_h (Array) reprenant les 24 relevés horaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+----+--------------------+---------+---------------+----+----+-------+\n",
      "|CallSign|State|Date    |Time|SkyCondition        |WindSpeed|StationPressure|Tmax|Tmin|AvgTemp|\n",
      "+--------+-----+--------+----+--------------------+---------+---------------+----+----+-------+\n",
      "|MEI     |MS   |20131101|0056|SCT010 BKN017 OVC060| 7       |29.52          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0058|FEW010 BKN014 OVC060| 8       |29.52          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0113|FEW010 BKN015 BKN020| 8       |29.52          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0123|FEW012 SCT020 BKN055| 5       |29.51          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0158|CLR                 | 5       |29.51          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0258|FEW010 SCT050       | 5       |29.51          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0316|FEW008 SCT050       | 0       |29.51          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0330|BKN008 BKN050       | 0       |29.51          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0356|FEW007 BKN013 BKN050| 7       |29.51          |74  |48  |51.2   |\n",
      "|MEI     |MS   |20131101|0358|SCT011 SCT016 BKN050| 7       |29.51          |74  |48  |51.2   |\n",
      "+--------+-----+--------+----+--------------------+---------+---------------+----+----+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "296924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echan_hourly: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Echantillon de travail : on se limite ici à quelques infos météo \n",
    "\n",
    "val echan_hourly = WOF_201311.select(\"CallSign\", \"State\", \"Date\", \"Time\",\n",
    "                                     \"SkyCondition\", \"WindSpeed\",\"StationPressure\",\"Tmax\",\"Tmin\",\"AvgTemp\")\n",
    "echan_hourly.show(10, false)\n",
    "println(echan_hourly.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Limitation du périmètre aux stations/Date avec 24 relevés horaires\n",
    "\n",
    "En vue du croisement avec les données sur les vols :\n",
    "* Création de la variable DateH  correspondant à Date en format date\n",
    "\n",
    "* Création de la variable TimeH correspondant à l'heure pleine la plus proche de l'heure de relevé (Time)\n",
    "\n",
    "* Limitation aux dates pour lesquelles il existe 24 relevés météo par jour (un par heure). En effet, dans certains cas, il y a des relevés intermédiaires ou il y en a moins que 24. La variable _t_ correspond au numéro de l'heure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb stations avec relevé météo sur 24h : 204600\n",
      "+--------+--------+----------+----+--------+---+\n",
      "|CallSign|Date    |DateH     |Time|TimeH   |_t_|\n",
      "+--------+--------+----------+----+--------+---+\n",
      "|DTW     |20131101|2013-11-01|0053|00:00:00|0  |\n",
      "|DTW     |20131101|2013-11-01|0153|01:00:00|1  |\n",
      "|DTW     |20131101|2013-11-01|0253|02:00:00|2  |\n",
      "|DTW     |20131101|2013-11-01|0353|03:00:00|3  |\n",
      "|DTW     |20131101|2013-11-01|0453|04:00:00|4  |\n",
      "|DTW     |20131101|2013-11-01|0553|05:00:00|5  |\n",
      "|DTW     |20131101|2013-11-01|0653|06:00:00|6  |\n",
      "|DTW     |20131101|2013-11-01|0753|07:00:00|7  |\n",
      "|DTW     |20131101|2013-11-01|0853|08:00:00|8  |\n",
      "|DTW     |20131101|2013-11-01|0953|09:00:00|9  |\n",
      "|DTW     |20131101|2013-11-01|1053|10:00:00|10 |\n",
      "|DTW     |20131101|2013-11-01|1153|11:00:00|11 |\n",
      "|DTW     |20131101|2013-11-01|1253|12:00:00|12 |\n",
      "|DTW     |20131101|2013-11-01|1353|13:00:00|13 |\n",
      "|DTW     |20131101|2013-11-01|1453|14:00:00|14 |\n",
      "|DTW     |20131101|2013-11-01|1553|15:00:00|15 |\n",
      "|DTW     |20131101|2013-11-01|1653|16:00:00|16 |\n",
      "|DTW     |20131101|2013-11-01|1753|17:00:00|17 |\n",
      "|DTW     |20131101|2013-11-01|1853|18:00:00|18 |\n",
      "|DTW     |20131101|2013-11-01|1953|19:00:00|19 |\n",
      "|DTW     |20131101|2013-11-01|2053|20:00:00|20 |\n",
      "|DTW     |20131101|2013-11-01|2153|21:00:00|21 |\n",
      "|DTW     |20131101|2013-11-01|2253|22:00:00|22 |\n",
      "|DTW     |20131101|2013-11-01|2353|23:00:00|23 |\n",
      "|DTW     |20131102|2013-11-02|0053|00:00:00|0  |\n",
      "+--------+--------+----------+----+--------+---+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.unix_timestamp\n",
       "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
       "import org.apache.spark.sql.functions.{hour, minute, second}\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "w_deltaNb: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5c093d5c\n",
       "w_deltaRg: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7cbf51dc\n",
       "hourly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string, State: string ... 12 more fields]\n",
       "hourly2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string, State: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp\n",
    "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
    "import org.apache.spark.sql.functions.{hour,minute,second}\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "// Fonctions d'agrégation (window)\n",
    "val w_deltaNb = Window\n",
    "                    .partitionBy(\"CallSign\",\"Date\",\"Delta\")\n",
    "                    .orderBy($\"Delta\" .asc)\n",
    "\n",
    "val w_deltaRg = Window\n",
    "                    .partitionBy(\"CallSign\",\"Date\")\n",
    "                    .orderBy($\"Delta\" .asc)\n",
    "\n",
    "// Gestion des dates et limitation à 24 du nombre de relevés (CountData)\n",
    "val hourly = echan_hourly.withColumn(\"DateH\",to_date(unix_timestamp($\"Date\",\"yyyyMMdd\").cast(\"timestamp\")))\n",
    "                         .withColumn(\"DTime\",from_unixtime(unix_timestamp($\"Time\",\"HHmm\"),\"HH:mm:ss\"))\n",
    "                         .withColumn(\"minute\", minute($\"DTime\"))\n",
    "                         .withColumn(\"new_min\", (floor($\"minute\"/60))*60)\n",
    "                         .withColumn(\"min_add\", $\"new_min\"- $\"minute\")\n",
    "                         .withColumn(\"TimeH\", from_unixtime(unix_timestamp($\"DTime\",\"HH:mm:ss\")+$\"min_add\"*60,\"HH:mm:ss\"))\n",
    "                         .withColumn(\"Delta\",minute($\"Dtime\")-minute($\"TimeH\"))\n",
    "                         .withColumn(\"CountDelta\",count(\"Delta\") over w_deltaNb )\n",
    "                         .where($\"CountDelta\"===24)\n",
    "                         .drop(\"minute\",\"new_min\",\"min_add\",\"Dtime\")\n",
    "                         .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "\n",
    "// On ne conserve qu'un relevé horaire de 24 heures lorsqu'il y en a plusieurs sur une même journée\n",
    "val hourly2 = hourly.withColumn(\"RgDelta\", rank() over w_deltaRg )\n",
    "                    .where($\"RgDelta\" ===1)               \n",
    "                    .withColumn(\"_t_\", hour($\"TimeH\"))\n",
    "                    //.where($\"_t_\"<=2)\n",
    "                    .drop(\"Delta\",\"CountDelta\",\"RgDelta\")\n",
    "                    //.sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "\n",
    "print(\"Nb stations avec relevé météo sur 24h : \")\n",
    "println(hourly2.count())\n",
    "\n",
    "hourly2.select(\"CallSign\",\"Date\",\"DateH\",\"Time\",\"TimeH\",\"_t_\")\n",
    "       .where($\"CallSign\"===\"DTW\").show(25, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Création de la colonne Weather_h reprenant sous forme d'un Array les 24 relevés météo\n",
    "WOT_201311 est une table de niveau station / Date en heure pleine (DateH) donnant les infos météo sous la forme d'un Array. 3 colonnes : CallSign, DateH et Weather_h\n",
    "* La colonne SkyCondition est splitée en plusiers colonnes correspondant aux différentes couches nuageuses mesurées (jusque 3 colonnes pour une station automatique). Chaque colonne correspondant à une couche nuageuse est elle-même splitée entre le code de 3 lettres (CLR, OVC, SCT...) et la hauteur de la couche. Le code de couverture nuageuse est ensuite indexé.\n",
    "* Les infos météo de type numérique (windspeed, station pressure) sont transformées en Double\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+--------------------+--------+---------+---------+---------+---------+---------+---------+\n",
      "|CallSign|    Date|Time|        SkyCondition|NumLayer|SC_1_code|SC_1_high|SC_2_code|SC_2_high|SC_3_code|SC_3_high|\n",
      "+--------+--------+----+--------------------+--------+---------+---------+---------+---------+---------+---------+\n",
      "|     ABE|20131101|0051|              FEW016|       1|      FEW|      016|      000|      000|      000|      000|\n",
      "|     ABE|20131101|0151|BKN033 BKN047 OVC055|       3|      BKN|      033|      BKN|      047|      OVC|      055|\n",
      "|     ABE|20131101|0251|              OVC028|       1|      OVC|      028|      000|      000|      000|      000|\n",
      "|     ABE|20131101|0351|BKN023 BKN033 OVC095|       3|      BKN|      023|      BKN|      033|      OVC|      095|\n",
      "|     ABE|20131101|0451|FEW022 BKN050 OVC100|       3|      FEW|      022|      BKN|      050|      OVC|      100|\n",
      "|     ABE|20131101|0551|SCT023 BKN029 OVC037|       3|      SCT|      023|      BKN|      029|      OVC|      037|\n",
      "|     ABE|20131101|0651|       BKN028 OVC034|       2|      BKN|      028|      OVC|      034|      000|      000|\n",
      "|     ABE|20131101|0751|       SCT024 OVC055|       2|      SCT|      024|      OVC|      055|      000|      000|\n",
      "|     ABE|20131101|0851|              OVC065|       1|      OVC|      065|      000|      000|      000|      000|\n",
      "|     ABE|20131101|0951|       FEW075 OVC100|       2|      FEW|      075|      OVC|      100|      000|      000|\n",
      "+--------+--------+----+--------------------+--------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.Column\n",
       "skyCond: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 12 more fields]\n",
       "splitCode: (DFrame: org.apache.spark.sql.DataFrame, df_col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame\n",
       "numCols: Int = 3\n",
       "skyCond_first_split: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 15 more fields]\n",
       "listSC: List[String]\n",
       "resultDF: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 21 more fields]\n",
       "skyCond_second_split: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 18 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformation of SkyCondition column\n",
    "\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val skyCond = hourly2.withColumn(\"SkyC\", split($\"SkyCondition\", \" \"))\n",
    "\n",
    "// Function to split code part and layer high from SkyCondition observation\n",
    "def splitCode(DFrame : DataFrame, df_col : Column): DataFrame = {\n",
    "    val fields = List( (df_col.toString+\"_code\",\"CLR\"),(df_col.toString+\"_high\",\"000\") )\n",
    "\n",
    "    fields.foldLeft( (DFrame,1) ){ \n",
    "    (tempdf, field) =>\n",
    "    val newdf = tempdf._1.withColumn(field._1, when(df_col ===\"CLR\",\"000\").\n",
    "                                               otherwise(substring(df_col, tempdf._2,3)))\n",
    "                         .withColumn(field._1, when(col(field._1) .isNull,\"000\").\n",
    "                                               otherwise(col(field._1)))    \n",
    "    (newdf, tempdf._2 + 3)\n",
    "  }._1\n",
    "}\n",
    "\n",
    "// first split \n",
    "val numCols = skyCond\n",
    "              .withColumn(\"SkyC_size\", size($\"SkyC\"))\n",
    "              .agg(max($\"SkyC_size\"))\n",
    "              .head()\n",
    "              .getInt(0)\n",
    "    \n",
    "val skyCond_first_split = skyCond.select(col(\"*\") +:\n",
    "                            (1 until numCols + 1).map(i => $\"SkyC\".getItem(i-1).as(s\"SC_$i\")): _*)\n",
    "                          .withColumn(\"NumLayer\",size($\"SkyC\"))\n",
    "                          .drop($\"SkyC\")\n",
    "\n",
    "// second split, colonnes \"SC_i\", i=1 to numCols\n",
    "def listSC = for (i <- (1 to numCols).toList) yield (\"SC_\"+i)\n",
    "\n",
    "var resultDF = skyCond_first_split\n",
    "for (x <- listSC) {\n",
    "    resultDF = splitCode(resultDF,col(x))\n",
    "}\n",
    "val skyCond_second_split = resultDF.drop(listSC:_*)\n",
    "\n",
    "skyCond_second_split.select(\"CallSign\",\"Date\",\"Time\",\"SkyCondition\", \"NumLayer\",\"SC_1_code\", \"SC_1_high\", \n",
    "                        \"SC_2_code\", \"SC_2_high\", \"SC_3_code\", \"SC_3_high\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+--------+---------+------------+---------+------------+---------+------------+\n",
      "|CallSign|    Date|Time|NumLayer|SC_1_code|SC_1_code_ix|SC_2_code|SC_2_code_ix|SC_3_code|SC_3_code_ix|\n",
      "+--------+--------+----+--------+---------+------------+---------+------------+---------+------------+\n",
      "|     ABE|20131101|0051|       1|      FEW|         4.0|      000|         5.0|      000|         5.0|\n",
      "|     ABE|20131101|0151|       3|      BKN|         6.0|      BKN|         4.0|      OVC|         1.0|\n",
      "|     ABE|20131101|0251|       1|      OVC|         2.0|      000|         5.0|      000|         5.0|\n",
      "|     ABE|20131101|0351|       3|      BKN|         6.0|      BKN|         4.0|      OVC|         1.0|\n",
      "|     ABE|20131101|0451|       3|      FEW|         4.0|      BKN|         4.0|      OVC|         1.0|\n",
      "+--------+--------+----+--------+---------+------------+---------+------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "skyCond_index: org.apache.spark.sql.DataFrame = [CallSign: string, Date: string ... 4 more fields]\n",
       "SCindexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_4f21c8d19b53, strIdx_5bfe3a018d5f, strIdx_e2a6dc55ad51)\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_a80c00dbdf9d\n",
       "skyCond_splitted: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 21 more fields]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// string index the categorical attributes SC_i_code\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import  org.apache.spark.ml.Pipeline \n",
    "\n",
    "val skyCond_index = skyCond_second_split.select(\"CallSign\",\"Date\",\"Time\",\"SC_1_code\",\"SC_2_code\",\"SC_3_code\")\n",
    "\n",
    "val SCindexers = skyCond_second_split.columns.filter(_.contains(\"code\")).map\n",
    "{field => \n",
    "    new StringIndexer()\n",
    "    .setInputCol(field)\n",
    "    .setStringOrderType(\"alphabetDesc\") \n",
    "    .setOutputCol(field+\"_ix\") \n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "                  .setStages(SCindexers)\n",
    "val skyCond_splitted = pipeline.fit(skyCond_second_split).transform(skyCond_second_split)\n",
    "\n",
    "skyCond_splitted.select(\"CallSign\",\"Date\",\"Time\",\"NumLayer\",\"SC_1_code\",\"SC_1_code_ix\",\"SC_2_code\",\"SC_2_code_ix\",\n",
    "                       \"SC_3_code\",\"SC_3_code_ix\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+--------+---+----------------------------------------------------------------------+\n",
      "|CallSign|DateH     |Time|TimeH   |_t_|Weather                                                               |\n",
      "+--------+----------+----+--------+---+----------------------------------------------------------------------+\n",
      "|ABE     |2013-11-01|0051|00:00:00|0  |[4.0, 5.0, 5.0, 13.0, 29.3, 70.0, 47.0, 40.5, 1.0, 16.0, 0.0, 0.0]    |\n",
      "|ABE     |2013-11-01|0151|01:00:00|1  |[6.0, 4.0, 1.0, 11.0, 29.27, 70.0, 47.0, 40.5, 3.0, 33.0, 47.0, 55.0] |\n",
      "|ABE     |2013-11-01|0251|02:00:00|2  |[2.0, 5.0, 5.0, 15.0, 29.23, 70.0, 47.0, 40.5, 1.0, 28.0, 0.0, 0.0]   |\n",
      "|ABE     |2013-11-01|0351|03:00:00|3  |[6.0, 4.0, 1.0, 15.0, 29.19, 70.0, 47.0, 40.5, 3.0, 23.0, 33.0, 95.0] |\n",
      "|ABE     |2013-11-01|0451|04:00:00|4  |[4.0, 4.0, 1.0, 16.0, 29.15, 70.0, 47.0, 40.5, 3.0, 22.0, 50.0, 100.0]|\n",
      "|ABE     |2013-11-01|0551|05:00:00|5  |[1.0, 4.0, 1.0, 20.0, 29.13, 70.0, 47.0, 40.5, 3.0, 23.0, 29.0, 37.0] |\n",
      "|ABE     |2013-11-01|0651|06:00:00|6  |[6.0, 1.0, 5.0, 20.0, 29.12, 70.0, 47.0, 40.5, 2.0, 28.0, 34.0, 0.0]  |\n",
      "|ABE     |2013-11-01|0751|07:00:00|7  |[1.0, 1.0, 5.0, 13.0, 29.19, 70.0, 47.0, 40.5, 2.0, 24.0, 55.0, 0.0]  |\n",
      "|ABE     |2013-11-01|0851|08:00:00|8  |[2.0, 5.0, 5.0, 7.0, 29.19, 70.0, 47.0, 40.5, 1.0, 65.0, 0.0, 0.0]    |\n",
      "|ABE     |2013-11-01|0951|09:00:00|9  |[4.0, 1.0, 5.0, 14.0, 29.2, 70.0, 47.0, 40.5, 2.0, 75.0, 100.0, 0.0]  |\n",
      "|ABE     |2013-11-01|1051|10:00:00|10 |[6.0, 5.0, 5.0, 14.0, 29.2, 70.0, 47.0, 40.5, 1.0, 110.0, 0.0, 0.0]   |\n",
      "|ABE     |2013-11-01|1151|11:00:00|11 |[4.0, 4.0, 5.0, 11.0, 29.19, 70.0, 47.0, 40.5, 2.0, 30.0, 100.0, 0.0] |\n",
      "|ABE     |2013-11-01|1251|12:00:00|12 |[4.0, 5.0, 5.0, 21.0, 29.17, 70.0, 47.0, 40.5, 1.0, 120.0, 0.0, 0.0]  |\n",
      "|ABE     |2013-11-01|1351|13:00:00|13 |[4.0, 0.0, 5.0, 21.0, 29.17, 70.0, 47.0, 40.5, 2.0, 45.0, 110.0, 0.0] |\n",
      "|ABE     |2013-11-01|1451|14:00:00|14 |[7.0, 5.0, 5.0, 20.0, 29.17, 70.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]    |\n",
      "|ABE     |2013-11-01|1551|15:00:00|15 |[7.0, 5.0, 5.0, 14.0, 29.16, 70.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]    |\n",
      "|ABE     |2013-11-01|1651|16:00:00|16 |[7.0, 5.0, 5.0, 10.0, 29.19, 70.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]    |\n",
      "|ABE     |2013-11-01|1751|17:00:00|17 |[7.0, 5.0, 5.0, 6.0, 29.21, 70.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]     |\n",
      "|ABE     |2013-11-01|1851|18:00:00|18 |[7.0, 5.0, 5.0, 5.0, 29.24, 70.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]     |\n",
      "|ABE     |2013-11-01|1951|19:00:00|19 |[7.0, 5.0, 5.0, 0.0, 29.25, 70.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]     |\n",
      "+--------+----------+----+--------+---+----------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- TimeH: string (nullable = true)\n",
      " |-- _t_: integer (nullable = true)\n",
      " |-- Weather: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listSC_high: List[String]\n",
       "listVars: List[String] = List(WindSpeed, StationPressure, Tmax, Tmin, AvgTemp, NumLayer)\n",
       "fields: List[String] = List(WindSpeed, StationPressure, Tmax, Tmin, AvgTemp, NumLayer, SC_1_high, SC_2_high, SC_3_high)\n",
       "hourly2_double: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 21 more fields]\n",
       "import org.apache.spark.sql.functions.{col, concat_ws}\n",
       "listSC_code: List[String]\n",
       "needed_cols: List[String] = List(SC_1_code_ix, SC_2_code_ix, SC_3_code_ix, WindSpeed, StationPressure, Tmax, Tmin, AvgTemp, NumLayer, SC_1_high, SC_2_high, SC_3_high)\n",
       "hourly3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string, DateH: date ... 4 more fields]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Tranformation of numeric measures from string en double\n",
    "\n",
    "def listSC_high = for (i <- (1 to numCols).toList) yield (\"SC_\"+i+\"_high\")\n",
    "val listVars = List(\"WindSpeed\", \"StationPressure\",\"Tmax\",\"Tmin\",\"AvgTemp\", \"NumLayer\")\n",
    "val fields = listVars ++ listSC_high\n",
    "\n",
    "\n",
    "val hourly2_double = fields.foldLeft(skyCond_splitted){ \n",
    "                     (tempdf, colName) => tempdf.withColumn(colName, col(colName).cast(\"Double\")) }\n",
    "\n",
    "\n",
    "// vectorization of weather observation in a unique Weather column : IMPOSSIBLE WITH COLLECT_LIST\n",
    "/*import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "def listSC_code = for (i <- (1 to numCols).toList) yield (\"SC_\"+i+\"_code_ix\")\n",
    "val cols = (listSC_code ++ fields).toArray\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(cols)\n",
    "  .setOutputCol(\"Weather\")\n",
    "\n",
    "val hourly3 = assembler.transform(hourly2_double)\n",
    "                       .select(\"CallSign\",\"DateH\", \"Time\",\"TimeH\",\"_t_\",\"Weather\")\n",
    "                       .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "*/\n",
    "\n",
    "// Concatenation in a struct \n",
    "/*import org.apache.spark.sql.functions.struct\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def listSC_code = for (i <- (1 to numCols).toList) yield (\"SC_\"+i+\"_code_ix\")\n",
    "val needed_cols : List[String] = (listSC_code ++ fields)\n",
    "\n",
    "// Varable Weather regroupant, pour chaque heure, les infos météo retenues\n",
    "val hourly3 = hourly2_double.withColumn(\"Weather\", struct(needed_cols.map(col): _*))\n",
    "                            .select(\"CallSign\",\"DateH\", \"Time\",\"TimeH\",\"_t_\",\"Weather\")\n",
    "                            .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "*/\n",
    "\n",
    "// Concatenation in an array\n",
    "import org.apache.spark.sql.functions.{col,concat_ws}\n",
    "\n",
    "def listSC_code = for (i <- (1 to numCols).toList) yield (\"SC_\"+i+\"_code_ix\")\n",
    "val needed_cols : List[String] = (listSC_code ++ fields)\n",
    "\n",
    "val hourly3 = hourly2_double.withColumn(\"Weather\",array(needed_cols.map(col): _*))\n",
    "                            .select(\"CallSign\",\"DateH\", \"Time\",\"TimeH\",\"_t_\",\"Weather\")\n",
    "                            .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "hourly3.show(false)\n",
    "hourly3.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CallSign|DateH     |Weather_hv                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+--------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ACV     |2013-11-20|[4.0,4.0,1.0,5.0,29.65,56.0,46.0,49.6,3.0,3.0,11.0,19.0,0.0,5.0,5.0,10.0,29.65,56.0,46.0,49.6,1.0,2.0,0.0,0.0,0.0,5.0,5.0,8.0,29.65,56.0,46.0,49.6,1.0,2.0,0.0,0.0,2.0,5.0,5.0,10.0,29.65,56.0,46.0,49.6,1.0,2.0,0.0,0.0,2.0,5.0,5.0,11.0,29.66,56.0,46.0,49.6,1.0,6.0,0.0,0.0,2.0,5.0,5.0,3.0,29.66,56.0,46.0,49.6,1.0,3.0,0.0,0.0,0.0,5.0,5.0,3.0,29.67,56.0,46.0,49.6,1.0,2.0,0.0,0.0,2.0,5.0,5.0,0.0,29.68,56.0,46.0,49.6,1.0,3.0,0.0,0.0,6.0,4.0,1.0,3.0,29.69,56.0,46.0,49.6,3.0,13.0,33.0,100.0,2.0,5.0,5.0,3.0,29.69,56.0,46.0,49.6,1.0,9.0,0.0,0.0,2.0,5.0,5.0,5.0,29.69,56.0,46.0,49.6,1.0,10.0,0.0,0.0,4.0,5.0,5.0,0.0,29.67,56.0,46.0,49.6,1.0,9.0,0.0,0.0,6.0,5.0,5.0,6.0,29.67,56.0,46.0,49.6,1.0,8.0,0.0,0.0,2.0,5.0,5.0,0.0,29.67,56.0,46.0,49.6,1.0,9.0,0.0,0.0,2.0,5.0,5.0,3.0,29.67,56.0,46.0,49.6,1.0,10.0,0.0,0.0,2.0,5.0,5.0,8.0,29.68,56.0,46.0,49.6,1.0,10.0,0.0,0.0,2.0,5.0,5.0,5.0,29.68,56.0,46.0,49.6,1.0,9.0,0.0,0.0,2.0,5.0,5.0,9.0,29.69,56.0,46.0,49.6,1.0,8.0,0.0,0.0,4.0,5.0,5.0,0.0,29.7,56.0,46.0,49.6,1.0,9.0,0.0,0.0,2.0,5.0,5.0,0.0,29.71,56.0,46.0,49.6,1.0,9.0,0.0,0.0,2.0,5.0,5.0,0.0,29.73,56.0,46.0,49.6,1.0,6.0,0.0,0.0,2.0,5.0,5.0,0.0,29.73,56.0,46.0,49.6,1.0,6.0,0.0,0.0,2.0,5.0,5.0,0.0,29.74,56.0,46.0,49.6,1.0,1.0,0.0,0.0,0.0,5.0,5.0,5.0,29.75,56.0,46.0,49.6,1.0,2.0,0.0,0.0]|\n",
      "+--------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- Weather_hv: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WOT_201311: org.apache.spark.sql.DataFrame = [CallSign: string, DateH: date ... 1 more field]\n",
       "WOT_201311_flattened: org.apache.spark.sql.DataFrame = [CallSign: string, DateH: date ... 1 more field]\n",
       "import org.apache.spark.ml.linalg._\n",
       "convertUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5305/1650481884@1d3c0bf5,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,List(Some(class[value[0]: array<double>])),None,true,true)\n",
       "WOT_201311_vector: org.apache.spark.sql.DataFrame = [CallSign: string, DateH: date ... 1 more field]\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Table Variable Weather_h\n",
    "val WOT_201311 = hourly3\n",
    "  .groupBy($\"CallSign\",$\"DateH\")\n",
    "  .agg(collect_list(\"Weather\").as(\"Weather_h\"))\n",
    "\n",
    "\n",
    "println(s\"Nb stations / Date : ${WOT_201311.count()}\")\n",
    "WOT_201311.show(1, false) \n",
    "WOT_201311.printSchema\n",
    "\n",
    "//---------------------------------------------------------------------------------------\n",
    "// Remark : to flatten the array of array column to array : à utiliser en étape 5/5 du 4.\n",
    "val WOT_201311_flattened = WOT_201311.withColumn(\"Weather_hf\",flatten($\"Weather_h\"))\n",
    "                                     .drop(\"Weather_h\")\n",
    "\n",
    "// ... puis en Vector\n",
    "import org.apache.spark.ml.linalg._\n",
    "\n",
    "val convertUDF = udf((array : Seq[Double]) => {\n",
    "  Vectors.dense(array.toArray)\n",
    "})\n",
    "\n",
    "val WOT_201311_vector = WOT_201311_flattened.withColumn(\"Weather_hv\", convertUDF($\"Weather_hf\"))\n",
    "                                            .drop(\"Weather_hf\")\n",
    "WOT_201311_vector.printSchema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Préparation des données de vols => Flight Table (FT)\n",
    "La Table des données de vols contient les informations sur les vols et est limitée aux vols non détournés (diverted) ou annulés (canceled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examen des variables :\n",
    "* dep_time et arr_time : heure de départ effective\n",
    "\n",
    "* crs_dep_time et crs_arr_time : heure de départ et d'arrivée prévus\n",
    "    * Computerized reservation system (CRS) departure/arrival time is the scheduled departure/arrival time of the flight,\n",
    "    * wheels off is the time when the wheels of the aircraftleave the ground at the origin airport, and \n",
    "    * wheels on is the time when the wheels of the aircraft touch theground at the destination airport.\n",
    "\n",
    "* The departure delay of  an  aircraft  is  the  difference  between  the  actual departure  time  and  the  CRS  departure time of the flight. \n",
    "\n",
    "* Arrival delay equals actual arrival time minusthe scheduled arrival time.\n",
    "\n",
    "* \"dep_time_blk\" : créneau horaire. ex. crs_dep_time=1301 => 1300-1359\n",
    "\n",
    "* différence entre dep_delay et dep_delay_new ? ici dep_delay peut être négatif et traduire un vol en avance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1/2 - Limitation du périmètre des vols\n",
    "* Limitation aux vols pours lesquels correspond une sation météo au départ ou à l'arrivée\n",
    "* Limitation aux vols non annulés (canceled) et détournés (diverted)\n",
    "* Récupération, à partir des données de stations (data_station), de la TimeZone pour les aéroports de départ et d'arrivée et calcul du Time_lag entre aéroports\n",
    "* Création d'une clé entre les vols au départ et les vols à l'arrivée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de vols non annulés ou non détournés : 498420\n",
      "Nb de vols non annulés ou non détournés avec station météo : 497177\n",
      "+--------------------+----------+------+--------+--------+----+--------+-------------------+--------+\n",
      "|                 cle|   fl_date|origin|tail_num|dep_time|dest|arr_time|ACTUAL_ELAPSED_TIME|Time_lag|\n",
      "+--------------------+----------+------+--------+--------+----+--------+-------------------+--------+\n",
      "|2013-11-01N8747BD...|2013-11-01|   DTW|  N8747B|    2126| BGM|    2238|              72.00|     0.0|\n",
      "|2013-11-03N8883ED...|2013-11-03|   DTW|  N8883E|    2136| BGM|    2251|              75.00|     0.0|\n",
      "|2013-11-04N8709AD...|2013-11-04|   DTW|  N8709A|    2145| BGM|    2259|              74.00|     0.0|\n",
      "|2013-11-05N8918BD...|2013-11-05|   DTW|  N8918B|    2136| BGM|    2245|              69.00|     0.0|\n",
      "|2013-11-06N8914AD...|2013-11-06|   DTW|  N8914A|    2133| BGM|    2242|              69.00|     0.0|\n",
      "+--------------------+----------+------+--------+--------+----+--------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_vols1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [YEAR: string, QUARTER: string ... 60 more fields]\n",
       "data_vols2: org.apache.spark.sql.DataFrame = [YEAR: string, QUARTER: string ... 64 more fields]\n",
       "FT_201311: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "\n",
    "val data_vols1 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date1+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .where( ($\"Diverted\" !== 1) || ($\"Cancelled\" != 1) )\n",
    "                      .filter(\"tail_num is not null\")\n",
    "                      .filter(\"dep_time is not null\")\n",
    "\n",
    "print(\"Nb de vols non annulés ou non détournés : \")\n",
    "println(data_vols1.count())\n",
    "\n",
    "\n",
    "// Limitations aux vols pour lesquels une station météo est trouvée pour le départ ou l'arrivé\n",
    "val data_vols2  = data_vols1.join(data_station.select(\"CallSign\",\"TimeZone\"), data_vols1(\"origin\") === data_station(\"CallSign\"),\"inner\")\n",
    "                           .drop(\"CallSign\")\n",
    "                           .withColumnRenamed(\"TimeZone\", \"dep_TZone\")\n",
    "                           .join(data_station.select(\"CallSign\",\"TimeZone\"), data_vols1(\"dest\") === data_station(\"CallSign\"),\"inner\")\n",
    "                           .drop(\"CallSign\")\n",
    "                           .withColumnRenamed(\"TimeZone\", \"arr_TZone\")\n",
    "                           .withColumn(\"Time_lag\",$\"arr_TZone\"-$\"dep_TZone\")\n",
    "                           .withColumn(\"Cle\", concat($\"fl_date\", $\"tail_num\",$\"origin\",$\"dep_time\",$\"dest\",$\"arr_time\"))\n",
    "\n",
    "print(\"Nb de vols non annulés ou non détournés avec station météo : \")\n",
    "println(data_vols2.count())\n",
    "data_vols2.select(\"cle\",\"fl_date\",\"origin\", \"tail_num\", \"dep_time\",\n",
    "                                   \"dest\",\"arr_time\",\"ACTUAL_ELAPSED_TIME\", \"Time_lag\").show(5)\n",
    "\n",
    "// Table des vols\n",
    "val FT_201311 = data_vols2.select(\"cle\",\"fl_date\",\"origin\", \"tail_num\", \"dest\",\"dep_time\",\"crs_dep_time\",\"dep_delay\",\n",
    "                                  \"dep_delay_new\",\"dep_del15\", \"dep_delay_group\",\n",
    "                                  \"arr_time\",\"crs_arr_time\",\"arr_delay\",\"arr_delay_new\",\"arr_del15\",\n",
    "                                  \"ACTUAL_ELAPSED_TIME\",\"distance\",\"Time_lag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2/2 - Préparation de la table des vols avant jointure avec les données météo => flightD et flightA\n",
    "\n",
    "En vue du croisement avec les données météo sur les douze dernières heures, deux tables sont créés l'une pour les vols au départ (flightD), l'autre pour les vols à l'arrivée (flightA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Gestion des dates pour les vols\n",
    "\n",
    "La gestion des dates et des heures concernent à la fois les informations au départ et les informations à l'arrivée.\n",
    "La date d'arrivée n'est pas renseignée. Elle est reconstituée (ADate) à partir de la date de départ, de l'heure de départ, de la durée totale du vol (actual_elapsed_time) et du time_lag entre les deux aéroports.\n",
    "Afin de pouvoir croiser les données de vols avec les relevés météo, des heures de référence pour le départ et l'arrivée (DTimeF et ATimeF) sont créées : elles correspondent à l'heure pleine immédiatement inférieure.\n",
    "\n",
    "* Création de la variable DDate correspondant à fl_date en format date\n",
    "\n",
    "* Création de la variable ADate correspondant à la date d'arrivée \n",
    "\n",
    "* Création de la variable DDateF (departure) et ADateF (arrival) correspondant à la date de référence (heure pleine immédiatement inférieure) pour le rapprochement avec les relevés météo\n",
    "    * si l'horaire de vol est après 10h, DateF = fl_date ;\n",
    "    * pour les vols dont une partie des douze dernières heures couvrent le jour précédent, nous créons une nouvelle observation avec DateF au jour précédent (passage de 502507 obs à 706897 obs).\n",
    "\n",
    "* Création de la variable DTime / ATime correspondant à l'heure de départ et DTimeF / ATimeF correspondant à l'heure pleine la plus proche de l'heure de départ\n",
    "\n",
    "* La variable _d_ (departure) et _a_ (arrival) correspond au numéro de l'heure de départ et _d0_ / _a0_ au numéro de l'heure la plus petite du jour de départ(DDateF) ou d'arrivée (ADateF) pour laquelle un relevé météo est à récupérer :\n",
    "    * si l'heure de départ de référence est DTimeF=12:00, pour ce jour : _d_=12 et _d0_=1 et les relevés horaires 1, ..., 12 seront nécessaires (idem pour _a_ et _a0_);\n",
    "    * si l'heure de départ de référence est DTimeF=08:00, pour ce jour : le numéro de l'heure de départ est _d_ = 8 et _d0_=0 et les relevés horaires 0,1,...,8 seront nécessaires (idem pour _a_ et _a0_). A noter que dans ce cas, il sera nécessaire de créer un enregistrement complémentaire avec la date du jour précédent et _d_=23 et _d0_=21 (relevés 21,22,23) seront nécessaires ( voir (ii) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract de vols au départ\n",
      "+----------+--------+--------+----------+--------+---+----+-----+------+\n",
      "|DDate     |dep_time|DTime   |DDateF    |DTimeF  |_d_|_d0_|_ddb_|_ddb0_|\n",
      "+----------+--------+--------+----------+--------+---+----+-----+------+\n",
      "|2013-11-01|2126    |21:26:00|2013-11-01|21:00:00|21 |10  |null |null  |\n",
      "|2013-11-03|2136    |21:36:00|2013-11-03|21:00:00|21 |10  |null |null  |\n",
      "|2013-11-04|2145    |21:45:00|2013-11-04|21:00:00|21 |10  |null |null  |\n",
      "|2013-11-05|2136    |21:36:00|2013-11-05|21:00:00|21 |10  |null |null  |\n",
      "|2013-11-06|2133    |21:33:00|2013-11-06|21:00:00|21 |10  |null |null  |\n",
      "+----------+--------+--------+----------+--------+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Extract de vols à l'arrivée\n",
      "+----------+----------+--------+----------+--------+---+----+-----+------+\n",
      "|DDate     |ADate     |ATime   |ADateF    |ATimeF  |_a_|_a0_|_adb_|_adb0_|\n",
      "+----------+----------+--------+----------+--------+---+----+-----+------+\n",
      "|2013-11-01|2013-11-01|22:38:00|2013-11-01|22:00:00|22 |11  |null |null  |\n",
      "|2013-11-03|2013-11-03|22:51:00|2013-11-03|22:00:00|22 |11  |null |null  |\n",
      "|2013-11-04|2013-11-04|22:59:00|2013-11-04|22:00:00|22 |11  |null |null  |\n",
      "|2013-11-05|2013-11-05|22:45:00|2013-11-05|22:00:00|22 |11  |null |null  |\n",
      "|2013-11-06|2013-11-06|22:42:00|2013-11-06|22:00:00|22 |11  |null |null  |\n",
      "+----------+----------+--------+----------+--------+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.unix_timestamp\n",
       "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
       "import org.apache.spark.sql.functions.{hour, minute, second}\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "data_vols3: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 15 more fields]\n",
       "data_vols4: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp\n",
    "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
    "import org.apache.spark.sql.functions.{hour,minute,second}\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "// gestion des dates pour le départ : DDate, DTime, DDateF, DTimeF, _d_ et _d0_\n",
    "val data_vols3 = data_vols2.select(\"cle\",\"fl_date\",\"origin\", \"tail_num\", \"dep_time\",\n",
    "                                   \"dest\",\"arr_time\",\"ACTUAL_ELAPSED_TIME\", \"Time_lag\")\n",
    "                           .withColumn(\"DDate\",to_date(unix_timestamp($\"fl_date\",\"yyyy-MM-dd\").cast(\"timestamp\")))\n",
    "                           .withColumn(\"DDateF\",to_date(unix_timestamp($\"fl_date\",\"yyyy-MM-dd\").cast(\"timestamp\")))\n",
    "                           .withColumn(\"DTime\",from_unixtime(unix_timestamp($\"dep_time\",\"HHmm\"),\"HH:mm:ss\"))\n",
    "                           .withColumn(\"Dminute\", minute($\"DTime\"))\n",
    "                           .withColumn(\"Dnew_min\", (floor($\"Dminute\"/60))*60)\n",
    "                           .withColumn(\"Dmin_sub\", $\"Dnew_min\"- $\"Dminute\")\n",
    "                           .withColumn(\"DTimeF\", from_unixtime(unix_timestamp($\"DTime\",\"HH:mm:ss\")+$\"Dmin_sub\"*60,\"HH:mm:ss\"))\n",
    "                           .withColumn(\"_d_\", hour($\"DTimeF\"))\n",
    "                           .withColumn(\"_d0_\", when($\"_d_\" > 11, $\"_d_\"-11).otherwise(0))\n",
    "                           .withColumn(\"_ddb_\",when($\"_d_\" <11, 23))            // renseigné si chevauchement sur deux dates\n",
    "                           .withColumn(\"_ddb0_\", when($\"_d_\" <11, $\"_d_\"+13))   // renseigné si chevauchement sur deux dates           \n",
    "                           .drop(\"Dminute\",\"Dnew_min\",\"Dmin_sub\")\n",
    "\n",
    "println(\"Extract de vols au départ : \")\n",
    "data_vols3.select(\"DDate\",\"dep_time\",\"DTime\",\"DDateF\",\"DTimeF\",\"_d_\", \"_d0_\", \"_ddb_\", \"_ddb0_\").show(5, false)\n",
    "\n",
    "// gestion des dates pour l'arrivée : ADate, ATime, ADateF, ATimeF, _a_ et _a0_\n",
    "val data_vols4 = data_vols3.withColumn(\"ATime\",from_unixtime(unix_timestamp($\"arr_time\",\"HHmm\"),\"HH:mm:ss\"))\n",
    "                           .withColumn(\"Time0\",from_unixtime(unix_timestamp(lit(\"2359\"),\"HHmm\"),\"HH:mm:ss\"))\n",
    "                           .withColumn(\"Timeto0\",round( (unix_timestamp($\"Time0\",\"HH:mm:ss\") - \n",
    "                                                 unix_timestamp($\"DTime\",\"HH:mm:ss\")+1)/60, 2))\n",
    "                           .withColumn(\"CORR_ELAPSED_TIME\",$\"ACTUAL_ELAPSED_TIME\" + $\"Time_lag\"*60)\n",
    "                           .withColumn(\"ADate\", when($\"CORR_ELAPSED_TIME\" > $\"TimeTo0\", date_add(to_date($\"DDate\",\"yyyy-MM-dd\"),+1))\n",
    "                                                .otherwise($\"DDate\"))\n",
    "                           .withColumn(\"ADateF\",$\"ADate\")\n",
    "                           .withColumn(\"Aminute\", minute($\"ATime\"))\n",
    "                           .withColumn(\"Anew_min\", (floor($\"Aminute\"/60))*60)\n",
    "                           .withColumn(\"Amin_sub\", $\"Anew_min\"- $\"Aminute\")\n",
    "                           .withColumn(\"ATimeF\", from_unixtime(unix_timestamp($\"ATime\",\"HH:mm:ss\")+$\"Amin_sub\"*60,\"HH:mm:ss\"))\n",
    "                           .withColumn(\"_a_\", hour($\"ATimeF\"))\n",
    "                           .withColumn(\"_a0_\", when($\"_a_\" > 11, $\"_a_\"-11).otherwise(0))\n",
    "                           .withColumn(\"_adb_\",when($\"_a_\" <11, 23))\n",
    "                           .withColumn(\"_adb0_\", when($\"_a_\" <11, $\"_a_\"+13))            \n",
    "                           .drop(\"Aminute\",\"Anew_min\",\"Amin_sub\")\n",
    "\n",
    "println(\"Extract de vols à l'arrivée : \")\n",
    "data_vols4.select(\"DDate\",\"ADate\",\"ATime\",\"ADateF\",\"ATimeF\",\"_a_\", \"_a0_\", \"_adb_\", \"_adb0_\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Création des enregistrements supplémentaires\n",
    "Lorque les douzes heures qui précèdent l'heure de départ ou l'heure d'arrivée des vols chevauchent le jour qui précède le départ ou l'arrivée, de nouveaux enregistrements sont créés avec comme DDateF ou ADateF la date du jours qui précède. Ces enregistrements permettront de récupérer les données météo de la veille.\n",
    "Deux tables sont créées :\n",
    "* flightD pour les informations de départ ;\n",
    "* filghtA pour les informations d'arrivée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb vols/Date au départ après enrichissement : 668074\n",
      "+----------+----------+----------+------+--------+--------+--------+--------+---+----+\n",
      "|fl_date   |DDate     |DDateF    |origin|tail_num|dep_time|DTime   |DTimeF  |_d_|_d0_|\n",
      "+----------+----------+----------+------+--------+--------+--------+--------+---+----+\n",
      "|2013-11-28|2013-11-28|2013-11-28|ADK   |N764AS  |1713    |17:13:00|17:00:00|17 |6   |\n",
      "|2013-11-21|2013-11-21|2013-11-21|ADK   |N713AS  |1733    |17:33:00|17:00:00|17 |6   |\n",
      "|2013-11-24|2013-11-24|2013-11-24|ADK   |N799AS  |1753    |17:53:00|17:00:00|17 |6   |\n",
      "|2013-11-03|2013-11-03|2013-11-03|ADK   |N771AS  |1639    |16:39:00|16:00:00|16 |5   |\n",
      "|2013-11-10|2013-11-10|2013-11-10|ADK   |N799AS  |1815    |18:15:00|18:00:00|18 |7   |\n",
      "+----------+----------+----------+------+--------+--------+--------+--------+---+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------+----------+------+--------+--------+--------+--------+---+----+\n",
      "|fl_date   |DDate     |DDateF    |origin|tail_num|dep_time|DTime   |DTimeF  |_d_|_d0_|\n",
      "+----------+----------+----------+------+--------+--------+--------+--------+---+----+\n",
      "|2013-11-01|2013-11-01|2013-10-31|DTW   |N8747B  |0958    |09:58:00|09:00:00|23 |22  |\n",
      "|2013-11-03|2013-11-03|2013-11-02|DTW   |N8794B  |0955    |09:55:00|09:00:00|23 |22  |\n",
      "|2013-11-04|2013-11-04|2013-11-03|DTW   |N8390A  |0955    |09:55:00|09:00:00|23 |22  |\n",
      "|2013-11-05|2013-11-05|2013-11-04|DTW   |N8903A  |0952    |09:52:00|09:00:00|23 |22  |\n",
      "|2013-11-06|2013-11-06|2013-11-05|DTW   |N8913A  |1012    |10:12:00|10:00:00|23 |23  |\n",
      "+----------+----------+----------+------+--------+--------+--------+--------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vols_dep: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 11 more fields]\n",
       "vols_dep_new: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 11 more fields]\n",
       "flightD: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols au départ\n",
    "val vols_dep = data_vols4.select(\"cle\",\"fl_date\",\"DDate\",\"DDateF\",\"origin\",\"tail_num\",\"dep_time\",\"DTime\",\n",
    "                                   \"DTimeF\",\"_d_\",\"_d0_\", \"_ddb_\", \"_ddb0_\") \n",
    "\n",
    "// Nouvel enregistrement pour les vols dont l'heure de départ est inférieure à 12h (DDateF moins un jour)\n",
    "val vols_dep_new = vols_dep.where($\"_d_\"<11)\n",
    "                           .withColumn(\"_d0_\",$\"_ddb0_\")\n",
    "                           .withColumn(\"_d_\",$\"_ddb_\")\n",
    "                           .withColumn(\"DDateF1\", date_add(to_date($\"DDateF\",\"yyyy-MM-dd\"),-1))\n",
    "                           .withColumn(\"DDateF\",$\"DDateF1\")\n",
    "                           .drop(\"DDateF1\")          \n",
    "\n",
    "val flightD = vols_dep.union(vols_dep_new)\n",
    "                      .select(\"cle\",\"fl_date\",\"DDate\",\"DDateF\",\"origin\",\"tail_num\",\"dep_time\",\"DTime\",\n",
    "                              \"DTimeF\",\"_d_\",\"_d0_\")\n",
    "\n",
    "print(\"Table flightD. Nb vols/Date au départ après enrichissement : \")\n",
    "println(flightD.count())\n",
    "\n",
    "println(\"Extract de vols au départ, aéroport ADK : \")\n",
    "flightD.where($\"origin\"===\"ADK\").drop(\"cle\").show(5, false)\n",
    "println(\"Extract de vols au départ, nouveaux enregistrements : \")\n",
    "flightD.where($\"DDate\" !== $\"DDateF\").drop(\"cle\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb vols/Date à l'arrivée après enrichissement : 610650\n",
      "Extract de vols à l'arrivée, aéroport ADK : \n",
      "+----------+----------+----------+----+--------+--------+--------+--------+---+----+\n",
      "|fl_date   |ADate     |ADateF    |dest|tail_num|arr_time|ATime   |ATimeF  |_a_|_a0_|\n",
      "+----------+----------+----------+----+--------+--------+--------+--------+---+----+\n",
      "|2013-11-28|2013-11-28|2013-11-28|ADK |N764AS  |1606    |16:06:00|16:00:00|16 |5   |\n",
      "|2013-11-21|2013-11-21|2013-11-21|ADK |N713AS  |1549    |15:49:00|15:00:00|15 |4   |\n",
      "|2013-11-24|2013-11-24|2013-11-24|ADK |N799AS  |1645    |16:45:00|16:00:00|16 |5   |\n",
      "|2013-11-03|2013-11-03|2013-11-03|ADK |N771AS  |1544    |15:44:00|15:00:00|15 |4   |\n",
      "|2013-11-10|2013-11-10|2013-11-10|ADK |N799AS  |1633    |16:33:00|16:00:00|16 |5   |\n",
      "+----------+----------+----------+----+--------+--------+--------+--------+---+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Extract de vols à l'arrivée, nouveaux enregistrements : \n",
      "+----------+----------+----------+----+--------+--------+--------+--------+---+----+\n",
      "|fl_date   |ADate     |ADateF    |dest|tail_num|arr_time|ATime   |ATimeF  |_a_|_a0_|\n",
      "+----------+----------+----------+----+--------+--------+--------+--------+---+----+\n",
      "|2013-11-17|2013-11-18|2013-11-17|BGM |N8477R  |0019    |00:19:00|00:00:00|23 |13  |\n",
      "|2013-11-10|2013-11-10|2013-11-09|BGM |N601XJ  |1055    |10:55:00|10:00:00|23 |23  |\n",
      "|2013-11-17|2013-11-17|2013-11-16|BGM |N8938A  |1057    |10:57:00|10:00:00|23 |23  |\n",
      "|2013-11-19|2013-11-19|2013-11-18|BGM |N8541D  |1059    |10:59:00|10:00:00|23 |23  |\n",
      "|2013-11-26|2013-11-26|2013-11-25|BGM |N840AY  |1059    |10:59:00|10:00:00|23 |23  |\n",
      "+----------+----------+----------+----+--------+--------+--------+--------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vols_arr: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 11 more fields]\n",
       "vols_arr_new: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 11 more fields]\n",
       "flightA: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols à l'arrivée\n",
    "\n",
    "val vols_arr = data_vols4.select(\"cle\",\"fl_date\",\"ATime\",\"ADate\",\"ADateF\",\"ATimeF\",\"dest\",\"tail_num\", \"arr_time\",\n",
    "                                 \"_a_\", \"_a0_\", \"_adb_\", \"_adb0_\")\n",
    "\n",
    "// Nouvel enregistrement pour les vols dont l'heure d'arrivée est inférieure à 12h (ADateF moins un jour)\n",
    "val vols_arr_new = vols_arr.where($\"_a_\"<11)\n",
    "                           .withColumn(\"_a0_\",$\"_adb0_\")\n",
    "                           .withColumn(\"_a_\",$\"_adb_\")\n",
    "                           .withColumn(\"ADateF1\", date_add(to_date($\"ADateF\",\"yyyy-MM-dd\"),-1))\n",
    "                           .withColumn(\"ADateF\",$\"ADateF1\")\n",
    "                           .drop(\"ADateF1\")          \n",
    "\n",
    "val flightA = vols_arr.union(vols_arr_new)\n",
    "                      .select(\"cle\",\"fl_date\",\"ADate\",\"ADateF\",\"dest\",\"tail_num\",\"arr_time\",\"ATime\",\n",
    "                              \"ATimeF\",\"_a_\",\"_a0_\")\n",
    "\n",
    "print(\"Table flightA. Nb vols/Date à l'arrivée après enrichissement : \")\n",
    "println(flightA.count())\n",
    "println(\"Extract de vols à l'arrivée, aéroport ADK : \")\n",
    "flightA.where($\"dest\"===\"ADK\").drop(\"cle\").show(5, false)\n",
    "println(\"Extract de vols à l'arrivée, nouveaux enregistrements : \")\n",
    "flightA.where($\"ADate\" !== $\"ADateF\").drop(\"cle\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Table des données de vols enrichie des données météo => JT_201311\n",
    "Construction du dataframe reprenant les vols avec un colonne Weather_d donnant les relevés météo des douze dernières heures précédant l'heure de départ et Weather_a donnant les relevés météo des douze heures précédant l'heure d'arrivée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1/5 : croisement entre le dataframe des vols et le dataframe reprenant le relevé météo par heure\n",
    "\n",
    "ATTENTION, le croisement correspondant à la première date du jour du mois n'est pas géré ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- Weather_h: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- SkyCondition: string (nullable = true)\n",
      " |    |    |-- WindSpeed: string (nullable = true)\n",
      " |    |    |-- StationPressure: string (nullable = true)\n",
      " |    |    |-- Tmax: string (nullable = true)\n",
      " |    |    |-- Tmin: string (nullable = true)\n",
      " |    |    |-- AvgTemp: string (nullable = true)\n",
      "\n",
      "8525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_hourly: org.apache.spark.sql.DataFrame = [CallSign: string, DateH: date ... 1 more field]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_hourly = WOT_201311\n",
    "df_hourly.printSchema\n",
    "println(df_hourly.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+--------+---+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|fl_date   |DDateF    |origin|tail_num|DTime   |_d_|_d0_|Weather_d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+----------+----------+------+--------+--------+---+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-01|2013-11-01|ABE   |N29906  |11:40:00|11 |0   |[[FEW016, 13, 29.30, 70, 47, 40.5], [BKN033 BKN047 OVC055, 11, 29.27, 70, 47, 40.5], [OVC028, 15, 29.23, 70, 47, 40.5], [BKN023 BKN033 OVC095, 15, 29.19, 70, 47, 40.5], [FEW022 BKN050 OVC100, 16, 29.15, 70, 47, 40.5], [SCT023 BKN029 OVC037, 20, 29.13, 70, 47, 40.5], [BKN028 OVC034, 20, 29.12, 70, 47, 40.5], [SCT024 OVC055, 13, 29.19, 70, 47, 40.5], [OVC065,  7, 29.19, 70, 47, 40.5], [FEW075 OVC100, 14, 29.20, 70, 47, 40.5], [BKN110, 14, 29.20, 70, 47, 40.5], [FEW030 BKN100, 11, 29.19, 70, 47, 40.5], [FEW120, 21, 29.17, 70, 47, 40.5], [FEW045 SCT110, 21, 29.17, 70, 47, 40.5], [CLR, 20, 29.17, 70, 47, 40.5], [CLR, 14, 29.16, 70, 47, 40.5], [CLR, 10, 29.19, 70, 47, 40.5], [CLR,  6, 29.21, 70, 47, 40.5], [CLR,  5, 29.24, 70, 47, 40.5], [CLR,  0, 29.25, 70, 47, 40.5], [CLR,  0, 29.27, 70, 47, 40.5], [BKN075,  0, 29.30, 70, 47, 40.5], [BKN080,  6, 29.27, 70, 47, 40.5], [OVC080,  5, 29.26, 70, 47, 40.5]]|\n",
      "+----------+----------+------+--------+--------+---+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Nb vols/Date au départ avec relevé météo : 624273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightD_H: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, fl_date: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols au départ\n",
    "\n",
    "val flightD_H = flightD.join(df_hourly, flightD(\"origin\") === df_hourly(\"CallSign\") &&\n",
    "                                         flightD(\"DDateF\") === df_hourly(\"DateH\"), \"inner\" )\n",
    "                       .withColumnRenamed(\"Weather_h\",\"Weather_d\")\n",
    "                       .drop(\"DateH\", \"CallSign\",\"Date\")\n",
    "                       .sort($\"origin\" .asc, $\"DDateF\" .asc, $\"tail_num\" .asc, $\"DTime\" .asc)\n",
    "\n",
    "flightD_H.select(\"fl_date\",\"DDateF\",\"origin\", \"tail_num\",\"DTime\",\"_d_\",\"_d0_\",\"Weather_d\").show(1,false)\n",
    "print(\"Table flightD_H. Nb vols/Date au départ avec relevé météo : \")\n",
    "println(flightD_H.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+--------+--------+---+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|fl_date   |ADateF    |dest|tail_num|ATime   |_a_|_a0_|Weather_a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+----------+----------+----+--------+--------+---+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-01|2013-11-01|ABE |N29906  |10:59:00|10 |0   |[[FEW016, 13, 29.30, 70, 47, 40.5], [BKN033 BKN047 OVC055, 11, 29.27, 70, 47, 40.5], [OVC028, 15, 29.23, 70, 47, 40.5], [BKN023 BKN033 OVC095, 15, 29.19, 70, 47, 40.5], [FEW022 BKN050 OVC100, 16, 29.15, 70, 47, 40.5], [SCT023 BKN029 OVC037, 20, 29.13, 70, 47, 40.5], [BKN028 OVC034, 20, 29.12, 70, 47, 40.5], [SCT024 OVC055, 13, 29.19, 70, 47, 40.5], [OVC065,  7, 29.19, 70, 47, 40.5], [FEW075 OVC100, 14, 29.20, 70, 47, 40.5], [BKN110, 14, 29.20, 70, 47, 40.5], [FEW030 BKN100, 11, 29.19, 70, 47, 40.5], [FEW120, 21, 29.17, 70, 47, 40.5], [FEW045 SCT110, 21, 29.17, 70, 47, 40.5], [CLR, 20, 29.17, 70, 47, 40.5], [CLR, 14, 29.16, 70, 47, 40.5], [CLR, 10, 29.19, 70, 47, 40.5], [CLR,  6, 29.21, 70, 47, 40.5], [CLR,  5, 29.24, 70, 47, 40.5], [CLR,  0, 29.25, 70, 47, 40.5], [CLR,  0, 29.27, 70, 47, 40.5], [BKN075,  0, 29.30, 70, 47, 40.5], [BKN080,  6, 29.27, 70, 47, 40.5], [OVC080,  5, 29.26, 70, 47, 40.5]]|\n",
      "+----------+----------+----+--------+--------+---+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Table flightA_H. Nb vols/Date à l'arrivée avec relevé météo : 570760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightA_H: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, fl_date: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols à l'arrivée\n",
    "\n",
    "val flightA_H = flightA.join(df_hourly, flightA(\"dest\") === df_hourly(\"CallSign\") &&\n",
    "                                        flightA(\"ADateF\") === df_hourly(\"DateH\"), \"inner\" )\n",
    "                       .withColumnRenamed(\"Weather_h\",\"Weather_a\")\n",
    "                       .drop(\"DateH\", \"CallSign\",\"Date\")\n",
    "                       .sort($\"dest\" .asc, $\"ADateF\" .asc, $\"tail_num\" .asc, $\"ATime\" .asc)\n",
    "\n",
    "flightA_H.select(\"fl_date\",\"ADateF\",\"dest\", \"tail_num\",\"ATime\",\"_a_\",\"_a0_\",\"Weather_a\").show(1,false)\n",
    "print(\"Table flightA_H. Nb vols/Date à l'arrivée avec relevé météo : \")\n",
    "println(flightA_H.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2/5 : élimination des relevés météo qui ne correspondant pas aux douze dernières heures précédant le départ ou l'arrivée du vol\n",
    "Pour éliminer les relevés météo non nécessaires, les relevés horaires sont repositionnés en ligne à ligne (explode).\n",
    "Seuls les vols avec 12 relevés horaires disponibles sont retenus (ce qui suppose que les relevés horaires existent pour ce vol sur deux dates lorsque les vols partent ou arrivent après 10h du matin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightD_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : 5535744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_hourD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3e85b89f\n",
       "w_obsD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7f9d0aea\n",
       "flightD_H_explode: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, origin: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{explode, flatten}\n",
    "\n",
    "// cas des vols au départ\n",
    "\n",
    "val w_hourD = Window\n",
    "                .partitionBy(\"origin\",\"DDate\",\"DDateF\",\"tail_num\",\"DTime\")\n",
    "                .orderBy($\"DTime\" .asc)\n",
    "val w_obsD = Window\n",
    "                .partitionBy(\"origin\",\"DDate\",\"tail_num\",\"DTime\")\n",
    "                .orderBy($\"DTime\" .asc)\n",
    "\n",
    "val flightD_H_explode = flightD_H.withColumn(\"Weather_t\",explode($\"Weather_d\"))\n",
    "                                 .withColumn(\"rn\", row_number over w_hourD)\n",
    "                                 .withColumn(\"_t_\", $\"rn\"-1 )\n",
    "                                 .where($\"_t_\">=$\"_d0_\" && $\"_t_\"<= $\"_d_\")  // sélection des relevés horaires\n",
    "                                 .withColumn(\"obs\", count(\"DTime\") over w_obsD)\n",
    "                                 .where($\"obs\" === 12)                       // limitation à 12 relevés horaires\n",
    "                                 .select(\"cle\",\"origin\",\"dep_time\",\"fl_date\",\"DDate\",\"DDateF\",\"tail_num\",\"DTime\",\"_d_\",\"_d0_\",\"_t_\",\"obs\",\"Weather_t\")\n",
    "                                 .sort($\"origin\" .asc, $\"DDate\" .asc, $\"tail_num\" .asc, $\"DTime\" .asc, $\"_t_\" .asc)\n",
    "\n",
    "print(\"Table flightD_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : \")\n",
    "println(flightD_H_explode.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightA_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : 5557104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_hourA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@34366af9\n",
       "w_obsA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@25a67762\n",
       "flightA_H_explode: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, dest: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cas des vols à l'arrivée\n",
    "\n",
    "val w_hourA = Window\n",
    "                .partitionBy(\"dest\",\"ADate\",\"ADateF\",\"tail_num\",\"ATime\")\n",
    "                .orderBy($\"ATime\" .asc)\n",
    "val w_obsA = Window\n",
    "                .partitionBy(\"dest\",\"ADate\",\"tail_num\",\"ATime\")\n",
    "                .orderBy($\"ATime\" .asc)\n",
    "\n",
    "val flightA_H_explode = flightA_H.withColumn(\"Weather_t\",explode($\"Weather_a\"))\n",
    "                                 .withColumn(\"rn\", row_number over w_hourA)\n",
    "                                 .withColumn(\"_t_\", $\"rn\"-1 )\n",
    "                                 .where($\"_t_\">=$\"_a0_\" && $\"_t_\"<= $\"_a_\")\n",
    "                                 .withColumn(\"obs\", count(\"ATime\") over w_obsA)\n",
    "                                 .where($\"obs\"===12)\n",
    "                                 .select(\"cle\",\"dest\",\"arr_time\",\"fl_date\",\"ADate\",\"ADateF\",\"tail_num\",\"ATime\",\"_a_\",\"_a0_\",\"_t_\",\"obs\",\"Weather_t\")\n",
    "                                 .sort($\"dest\" .asc, $\"ADate\" .asc, $\"tail_num\" .asc, $\"ATime\" .asc, $\"_t_\" .asc)\n",
    "\n",
    "print(\"Table flightA_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : \")\n",
    "println(flightA_H_explode.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3/5 : vols avec leurs 12 relevés météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cle: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- fl_date: string (nullable = true)\n",
      " |-- DDate: date (nullable = true)\n",
      " |-- DDateF: date (nullable = true)\n",
      " |-- tail_num: string (nullable = true)\n",
      " |-- DTime: string (nullable = true)\n",
      " |-- _d_: integer (nullable = true)\n",
      " |-- _d0_: integer (nullable = true)\n",
      " |-- _t_: integer (nullable = true)\n",
      " |-- obs: long (nullable = false)\n",
      " |-- Weather_t: struct (nullable = false)\n",
      " |    |-- SkyCondition: string (nullable = true)\n",
      " |    |-- WindSpeed: string (nullable = true)\n",
      " |    |-- StationPressure: string (nullable = true)\n",
      " |    |-- Tmax: string (nullable = true)\n",
      " |    |-- Tmin: string (nullable = true)\n",
      " |    |-- AvgTemp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightD_H_explode.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb vols/Date après récupération 12 dernières heures infos météo : 461312\n",
      "+------------------------------+------+----------+--------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+---+\n",
      "|cle                           |origin|fl_date   |tail_num|dep_time|Weather_d                                                                                                                                                                                                                                                                                                                                                                                                                                                     |rn |obs|\n",
      "+------------------------------+------+----------+--------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+---+\n",
      "|2013-11-18N678CAABE0805ATL1007|ABE   |2013-11-18|N678CA  |0805    |[[OVC014, 11, 29.27, 63, 47, 40.5], [BKN014 BKN022 OVC026, 14, 29.22, 63, 47, 40.5], [SCT008 OVC012, 14, 29.21, 63, 47, 40.5], [FEW049 OVC070,  5, 29.24, 63, 47, 40.5], [OVC095, 13, 29.25, 63, 47, 40.5], [FEW100 SCT120, 10, 29.25, 63, 47, 40.5], [CLR,  0, 29.26, 63, 47, 40.5], [CLR,  6, 29.28, 63, 47, 40.5], [CLR, 10, 29.29, 63, 47, 40.5], [OVC014, 10, 29.42, 63, 50, 40.5], [OVC014,  8, 29.38, 63, 50, 40.5], [OVC014, 14, 29.32, 63, 50, 40.5]]|12 |12 |\n",
      "+------------------------------+------+----------+--------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_weatherD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@65d0633\n",
       "w_rnD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@473994e5\n",
       "flightD_H12: org.apache.spark.sql.DataFrame = [cle: string, origin: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vols au départ\n",
    "\n",
    "val w_weatherD = Window\n",
    "                .partitionBy(\"origin\",\"fl_date\",\"tail_num\")\n",
    "                .orderBy($\"dep_time\" .asc)\n",
    "\n",
    "val w_rnD = Window\n",
    "           .partitionBy(\"origin\",\"fl_date\",\"tail_num\",\"dep_time\")\n",
    "           .orderBy($\"dep_time\" .asc)\n",
    "\n",
    "val flightD_H12 = flightD_H_explode.withColumn(\"Weather_d\", collect_list(\"Weather_t\") over w_weatherD)\n",
    "                                   .withColumn(\"rn\", row_number over w_rnD )\n",
    "                                   .where($\"rn\"===12)\n",
    "                                   .select(\"cle\",\"Weather_d\")\n",
    "\n",
    "print(\"Nb vols/Date après récupération 12 dernières heures infos météo : \")\n",
    "println(flightD_H12.count())\n",
    "      \n",
    "flightD_H12.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb vols/Date après récupération 12 dernières heures infos météo : 463090\n",
      "+------------------------------+----+----------+--------+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|c_cle                         |dest|fl_date   |tail_num|arr_time|Weather_a                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+------------------------------+----+----------+--------+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-21N976EVATL1449ABE1643|ABE |2013-11-21|N976EV  |1643    |[[CLR,  0, 30.17, 50, 21, 40.5], [CLR,  0, 30.17, 50, 21, 40.5], [CLR,  0, 30.16, 50, 21, 40.5], [BKN030,  0, 30.18, 50, 21, 40.5], [BKN029 OVC060,  6, 30.17, 50, 21, 40.5], [OVC031,  5, 30.14, 50, 21, 40.5], [FEW060,  3, 30.11, 50, 21, 40.5], [BKN055,  0, 30.08, 50, 21, 40.5], [OVC050,  0, 30.05, 50, 21, 40.5], [OVC050,  0, 30.05, 50, 21, 40.5], [OVC044,  3, 30.03, 50, 21, 40.5], [OVC038,  0, 30.02, 50, 21, 40.5]]|\n",
      "+------------------------------+----+----------+--------+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_weatherA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@702c856e\n",
       "w_rnA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1021e0a2\n",
       "flightA_H12: org.apache.spark.sql.DataFrame = [c_cle: string, dest: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vols à l'arrivée\n",
    "\n",
    "val w_weatherA = Window\n",
    "                .partitionBy(\"dest\",\"fl_date\",\"tail_num\")\n",
    "                .orderBy($\"arr_time\" .asc)\n",
    "\n",
    "val w_rnA = Window\n",
    "           .partitionBy(\"dest\",\"fl_date\",\"tail_num\",\"arr_time\")\n",
    "           .orderBy($\"arr_time\" .asc)\n",
    "\n",
    "val flightA_H12 = flightA_H_explode.withColumn(\"Weather_a\", collect_list(\"Weather_t\") over w_weatherA)\n",
    "                                   .withColumn(\"rn\", row_number over w_rnA )\n",
    "                                   .where($\"rn\"===12)\n",
    "                                   .withColumnRenamed(\"cle\",\"c_cle\")\n",
    "                                   .select(\"c_cle\",\"dest\", \"fl_date\", \"tail_num\",\"arr_time\", \"Weather_a\")\n",
    "\n",
    "print(\"Nb vols/Date après récupération 12 dernières heures infos météo : \")\n",
    "println(flightA_H12.count())\n",
    "      \n",
    "flightA_H12.show(1,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4/5 : regroupement des vols (départ / arrivée) via la clé => JT_201311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433248\n",
      "Table JT_201311. Nb vols/Date après récupération des données météo : 433248\n",
      "+------------------------------+----------+------+--------+----+--------+------------+---------+-------------+---------+---------------+----+--------+------------+---------+-------------+---------+-------------------+--------+--------+------+----------+--------+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+---+----+----------+--------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cle                           |fl_date   |origin|tail_num|dest|dep_time|crs_dep_time|dep_delay|dep_delay_new|dep_del15|dep_delay_group|dest|arr_time|crs_arr_time|arr_delay|arr_delay_new|arr_del15|ACTUAL_ELAPSED_TIME|distance|Time_lag|origin|fl_date   |tail_num|dep_time|Weather_d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |rn |obs|dest|fl_date   |tail_num|arr_time|Weather_a                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------------------------------+----------+------+--------+----+--------+------------+---------+-------------+---------+---------------+----+--------+------------+---------+-------------+---------+-------------------+--------+--------+------+----------+--------+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+---+----+----------+--------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-01N11176DEN1738SLC1959|2013-11-01|DEN   |N11176  |SLC |1738    |1742        |-4.00    |0.00         |0.00     |-1             |SLC |1959    |1911        |48.00    |48.00        |1.00     |141.00             |391.00  |0.0     |DEN   |2013-11-01|N11176  |1738    |[[BKN080 OVC090,  7, 24.54, 51, 27, 41.1], [FEW075 BKN085,  5, 24.55, 51, 27, 41.1], [SCT080,  0, 24.57, 51, 27, 41.1], [FEW070 BKN110, 13, 24.60, 51, 27, 41.1], [BKN070 BKN100,  0, 24.63, 51, 27, 41.1], [BKN095, 11, 24.65, 51, 27, 41.1], [BKN080 BKN100, 11, 24.68, 51, 27, 41.1], [SCT060 BKN070 OVC085, 11, 24.69, 51, 27, 41.1], [BKN045 BKN055, 18, 24.71, 51, 27, 41.1], [SCT049 BKN060, 16, 24.70, 51, 27, 41.1], [BKN050 BKN070, 11, 24.69, 51, 27, 41.1], [FEW055 BKN075 BKN085,  9, 24.69, 51, 27, 41.1], [BKN070 BKN100,  0, 24.63, 51, 27, 41.1], [BKN095, 11, 24.65, 51, 27, 41.1], [BKN080 BKN100, 11, 24.68, 51, 27, 41.1], [SCT060 BKN070 OVC085, 11, 24.69, 51, 27, 41.1], [BKN045 BKN055, 18, 24.71, 51, 27, 41.1], [SCT049 BKN060, 16, 24.70, 51, 27, 41.1], [BKN050 BKN070, 11, 24.69, 51, 27, 41.1], [FEW055 BKN075 BKN085,  9, 24.69, 51, 27, 41.1], [SCT070,  7, 24.70, 51, 27, 41.1], [SCT070,  3, 24.70, 51, 27, 41.1], [SCT070,  5, 24.71, 51, 27, 41.1], [FEW070,  7, 24.72, 51, 27, 41.1]]|12 |12 |SLC |2013-11-01|N11176  |1959    |[[FEW045 BKN060,  0, 25.98, 60, 43, 44.4], [FEW045TCU SCT055,  0, 25.98, 60, 43, 44.4], [FEW045TCU SCT060,  0, 25.98, 60, 43, 44.4], [SCT050TCU,  0, 25.97, 60, 43, 44.4], [SCT046,  6, 25.95, 60, 43, 44.4], [FEW045,  0, 25.94, 60, 43, 44.4], [FEW050,  6, 25.93, 60, 43, 44.4], [FEW050,  5, 25.92, 60, 43, 44.4], [CLR,  3, 25.92, 60, 43, 44.4], [CLR,  3, 25.92, 60, 43, 44.4], [CLR,  3, 25.92, 60, 43, 44.4], [CLR,  8, 25.91, 60, 43, 44.4]]|\n",
      "+------------------------------+----------+------+--------+----+--------+------------+---------+-------------+---------+---------------+----+--------+------------+---------+-------------+---------+-------------------+--------+--------+------+----------+--------+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+---+----+----------+--------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDA_H12: org.apache.spark.sql.DataFrame = [c_cle: string, origin: string ... 11 more fields]\n",
       "JT_201311: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 30 more fields]\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDA_H12 = flightD_H12.join(flightA_H12, flightD_H12(\"cle\")===flightA_H12(\"c_cle\"), \"inner\")\n",
    "                              .drop(\"c_cle\")\n",
    "                              .withColumnRenamed(\"cle\",\"c_cle\")\n",
    "\n",
    "println(flightDA_H12.count())\n",
    "\n",
    "val JT_201311 = FT_201311.join(flightDA_H12, \n",
    "                               FT_201311(\"cle\") === flightDA_H12(\"c_cle\"), \"inner\")\n",
    "                              .drop(\"c_cle\")\n",
    "\n",
    "print(\"Table JT_201311. Nb vols/Date après récupération des données météo : \")\n",
    "println(JT_201311.count())\n",
    "JT_201311.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin: string (nullable = true)\n",
      " |-- fl_date: string (nullable = true)\n",
      " |-- tail_num: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- Weather_h: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- SkyCondition: string (nullable = true)\n",
      " |    |    |-- WindSpeed: string (nullable = true)\n",
      " |    |    |-- StationPressure: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JT_201311.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5/5 : transformation des variables Weather_d et Weather_a qui sont des array de struct en array\n",
    "to be done..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Sample data:\n",
    "val df = Seq(\n",
    "  (\"id1\", \"t1\", Array((\"n1\", 4L), (\"n2\", 5L))),\n",
    "  (\"id2\", \"t2\", Array((\"n3\", 6L), (\"n4\", 7L)))\n",
    ").toDF(\"ID\", \"Time\", \"Items\")\n",
    "\n",
    "// Create UDF converting array of (String, Long) structs to Map[String, Long]\n",
    "val arrayToMap = udf[Map[String, Long], Seq[Row]] {\n",
    "  array => array.map { case Row(key: String, value: Long) => (key, value) }.toMap\n",
    "}\n",
    "\n",
    "// apply UDF\n",
    "val result = df.withColumn(\"Items\", arrayToMap($\"Items\"))\n",
    "\n",
    "result.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Label and target data creation\n",
    "\n",
    "Evaluation of random forest algorithm is done on a combination of 4 parameters :\n",
    "* Target dataset (D1, D2, D3, D4)\n",
    "    * D1 contains only delayed flights due to extreme value weather or NAS, or a combination of them\n",
    "    * D2 contains delayed flights due to extreme value weather + NAS delay if NAS delay > DT\n",
    "    * D3 containes delayed flights due to extreme value weather or NAS, even if not exclusively\n",
    "    * D4 contains all delayed flights\n",
    "* Delay Threshold (DT)\n",
    "    We consider 5 delay threshold (DT) in minutes : DT = 15,30,45,60,90)\n",
    "* Number of hourly weather observations at origin airport (m) : m = 0,1,3,5,7,9,11\n",
    "* Number of hourly weather observations at destination airport (n) : n = 0,1,3,45,7,9,11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+---------+--------------+-------------+-----+--------+--------+--------+--------+\n",
      "|arr_delay|weather_delay|nas_delay|security_delay|carrier_delay|l_a_d|target_1|target_2|target_3|target_4|\n",
      "+---------+-------------+---------+--------------+-------------+-----+--------+--------+--------+--------+\n",
      "|    32.00|         0.00|    25.00|          0.00|         0.00| 7.00|       0|       1|       1|       1|\n",
      "|   123.00|         0.00|   123.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    66.00|        10.00|    56.00|          0.00|         0.00| 0.00|       1|       1|       1|       1|\n",
      "|    33.00|         0.00|    20.00|          0.00|        13.00| 0.00|       0|       1|       1|       1|\n",
      "|    20.00|         0.00|    20.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    45.00|         0.00|    45.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    16.00|         0.00|    16.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    77.00|         0.00|    77.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    16.00|         0.00|    16.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    22.00|        21.00|     1.00|          0.00|         0.00| 0.00|       1|       1|       1|       1|\n",
      "|   187.00|         0.00|    35.00|          0.00|       152.00| 0.00|       0|       1|       1|       1|\n",
      "|    28.00|         0.00|    28.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    32.00|         0.00|    32.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    19.00|         0.00|    19.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    25.00|         0.00|    25.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    46.00|         0.00|    35.00|          0.00|         2.00| 9.00|       0|       1|       1|       1|\n",
      "|    62.00|         0.00|    27.00|          0.00|        19.00|16.00|       0|       1|       1|       1|\n",
      "|    32.00|         0.00|    32.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    45.00|         0.00|    43.00|          0.00|         2.00| 0.00|       0|       1|       1|       1|\n",
      "|    22.00|         0.00|    22.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    19.00|         0.00|    19.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    22.00|         0.00|    22.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    85.00|         0.00|    34.00|          0.00|        51.00| 0.00|       0|       1|       1|       1|\n",
      "|    21.00|         0.00|    21.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    36.00|         0.00|    36.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    22.00|         0.00|    22.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|   180.00|         0.00|    35.00|          0.00|       140.00| 5.00|       0|       1|       1|       1|\n",
      "|    32.00|         0.00|    17.00|          0.00|         5.00|10.00|       0|       1|       1|       1|\n",
      "|    23.00|         0.00|    23.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    15.00|         9.00|     0.00|          0.00|         0.00| 6.00|       0|       1|       1|       1|\n",
      "|    30.00|         0.00|    30.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    38.00|         0.00|    18.00|          0.00|         5.00|15.00|       0|       1|       1|       1|\n",
      "|    17.00|         0.00|    17.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    78.00|         0.00|    22.00|          0.00|         5.00|51.00|       0|       1|       1|       1|\n",
      "|    37.00|         0.00|    33.00|          0.00|         4.00| 0.00|       0|       1|       1|       1|\n",
      "|    48.00|        19.00|    29.00|          0.00|         0.00| 0.00|       1|       1|       1|       1|\n",
      "|    92.00|         0.00|    21.00|          0.00|        19.00|52.00|       0|       1|       1|       1|\n",
      "|    21.00|         0.00|    21.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    27.00|         0.00|    27.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|   338.00|         0.00|   338.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    44.00|         0.00|    34.00|          0.00|         3.00| 7.00|       0|       1|       1|       1|\n",
      "|    32.00|         0.00|    25.00|          0.00|         7.00| 0.00|       0|       1|       1|       1|\n",
      "|    22.00|         0.00|    22.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    16.00|         0.00|    16.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "|    38.00|         0.00|    17.00|          0.00|        21.00| 0.00|       0|       1|       1|       1|\n",
      "|    67.00|         0.00|    21.00|          0.00|        46.00| 0.00|       0|       1|       1|       1|\n",
      "|    21.00|         0.00|    17.00|          0.00|         4.00| 0.00|       0|       1|       1|       1|\n",
      "|    54.00|         0.00|    25.00|          0.00|        29.00| 0.00|       0|       1|       1|       1|\n",
      "|    21.00|         0.00|    20.00|          0.00|         1.00| 0.00|       0|       1|       1|       1|\n",
      "|    24.00|         0.00|    24.00|          0.00|         0.00| 0.00|       0|       1|       1|       1|\n",
      "+---------+-------------+---------+--------------+-------------+-----+--------+--------+--------+--------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DT: Array[Int] = Array(15, 30, 45, 60, 90)\n",
       "FT_target15: org.apache.spark.sql.DataFrame = [YEAR: string, QUARTER: string ... 68 more fields]\n",
       "test: Unit = ()\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DT = Array(15,30,45,60,90)\n",
    "\n",
    "\n",
    "// DT = 15\n",
    "// Voir pour itérer automatiquement\n",
    "\n",
    "val FT_target15 = data_vols2.withColumn(\"target_4\", when(($\"arr_delay\" >= DT(0)) && ($\"arr_delay\" !== $\"late_aircraft_delay\"), 1).otherwise(0))\n",
    "                      .withColumn(\"target_3\", when(($\"target_4\" === 1) && (($\"weather_delay\" > 0) || ($\"nas_delay\" > 0)), 1).otherwise(0))\n",
    "                      .withColumn(\"target_2\", when(($\"target_4\" === 1) && (($\"weather_delay\" > 0) || ($\"nas_delay\"> DT(0))), 1).otherwise(0))\n",
    "                      .withColumn(\"target_1\", when(($\"target_4\" === 1) && ($\"weather_delay\" > 0) && ($\"nas_delay\" > 0) && \n",
    "                                                   ($\"security_delay\" === 0) && ($\"carrier_delay\" === 0), 1).otherwise(0))\n",
    "\n",
    "val test = FT_target15.where($\"target_4\" === 1)\n",
    "                      .withColumnRenamed(\"late_aircraft_delay\",\"l_a_d\")\n",
    "                      //.where($\"target_2\"=== 1)\n",
    "                      .select(\n",
    "                             \"arr_delay\",\n",
    "                             \"weather_delay\",\"nas_delay\",\"security_delay\",\"carrier_delay\",\"l_a_d\",\n",
    "                             \"target_1\", \"target_2\", \"target_3\", \"target_4\"\n",
    "                             ).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
