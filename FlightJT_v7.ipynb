{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.30:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1610523683936)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "path: String = /home/georges/IASDfev20/3_BigData/DarioC/TP/MasterIASD_Flights_Project/data/\n",
       "pathA: String = /home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/ONTIME/\n",
       "pathQ: String = /home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/QCLCD/\n",
       "date1: String = 201311\n",
       "date2: String = 201511\n",
       "date3: String = 201711\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Initialisation chemin des données\n",
    "val path = \"/home/georges/IASDfev20/3_BigData/DarioC/TP/MasterIASD_Flights_Project/data/\"\n",
    "val pathA = \"/home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/ONTIME/\"\n",
    "val pathQ = \"/home/georges/IASDfev20/3_BigData/DarioC/TP_all/data/QCLCD/\"\n",
    "\n",
    "// Initialisation des variables de date\n",
    "val date1 = \"201311\"\n",
    "val date2 = \"201511\"\n",
    "val date3 = \"201711\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Liste des aéroports de référence\n",
    "Les fichiers de vols sont utilisés pour constituer une base de référence des aéroports, via leur code à 3 lettres.\n",
    "\n",
    "Cette liste (une par année par exemple) permet de délimiter le périmètre des stations météo nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb aéroports : 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_airp1: org.apache.spark.sql.DataFrame = [origin: string, dest: string]\n",
       "data_airp1o: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp1d: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "list_airp1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp2: org.apache.spark.sql.DataFrame = [origin: string, dest: string]\n",
       "data_airp2o: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp2d: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "list_airp2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string]\n",
       "data_airp3: org.apache.spark.sql.DataFrame = [origin: string, dest: string]\n",
       "data_airp3o: org.apache.spark.sql.Dat...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// liste des aéroports (sur la base de l'origine et de la destination des vols)\n",
    "// utilisation de 3 dates nov 2013, 2015 et 2017\n",
    "\n",
    "val data_airp1 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date1+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .select(\"origin\",\"dest\")\n",
    "val data_airp1o = data_airp1.withColumnRenamed(\"origin\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val data_airp1d = data_airp1.withColumnRenamed(\"dest\",\"CallSign\").select(\"CallSign\").distinct\n",
    "\n",
    "val list_airp1 = data_airp1o.union(data_airp1d).distinct\n",
    "\n",
    "val data_airp2 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date2+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .select(\"origin\",\"dest\")\n",
    "val data_airp2o = data_airp2.withColumnRenamed(\"origin\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val data_airp2d = data_airp2.withColumnRenamed(\"dest\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val list_airp2 = data_airp2o.union(data_airp2d).distinct\n",
    "\n",
    "val data_airp3 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date3+\"_T_ONTIME_REPORTING.csv\")\n",
    "                      .select(\"origin\",\"dest\")\n",
    "val data_airp3o = data_airp3.withColumnRenamed(\"origin\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val data_airp3d = data_airp3.withColumnRenamed(\"dest\",\"CallSign\").select(\"CallSign\").distinct\n",
    "val list_airp3 = data_airp3o.union(data_airp3d).distinct\n",
    "\n",
    "val liste_airp = list_airp1.union(list_airp2).union(list_airp3).distinct\n",
    "                          .orderBy($\"CallSign\".asc)\n",
    "\n",
    "println(s\"Nb aéroports : ${liste_airp.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Préparation des données météo : Weather Observation Table (WOT)\n",
    "\n",
    "La mise en forme des données météo consiste :\n",
    "\n",
    "* Etape 1 - Dans un premier temps, à récupérer le lien dans les fichiers stations.txt entre le code aéroport de trois letttres et le Wban d'identification des stations. Cela permet :\n",
    "    * d'une part de faire limiter le périmètre des stations à la liste des aéroports de référence => Weather Observation Table (OT);\n",
    "    * d'autre part de croiser les données météo avec celle des vols.\n",
    "    \n",
    "* Etape 2 - Dans un second temps, à construire, pour chaque station et chaque jour, le relevé des informations météo pour chaque heure (de 0 à 23h) :\n",
    "    * les stations/dates conservées sont celles où l'on retrouve un relevé complet sur 24 heures ;\n",
    "    * les informations météo utilisées (pression, température...) sont de niveau horaire, journalier ou mensuelles et sont conservées pour chaque heure dans un vecteur et indéxées pour les informations de nature qualitative.\n",
    "    \n",
    "Remarque : dans l'article, seules des données météo horaires semblent prises en compte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1/2 - Lecture des fichiers QCLCD => Weather Observation File (WOF)\n",
    "Réduction aux stations correspondant aux aéeroports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Fichier des stations restreint à la liste de référence des aéroports\n",
    "A noter qu'il existe des aéroports pour lesquels aucune station n'est trouvée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_station0: org.apache.spark.sql.DataFrame = [WBAN: string, WMO: string ... 13 more fields]\n",
       "data_station: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "\n",
    "val data_station0 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"|\")\n",
    "                              .load(pathQ+date1+\"station.txt\")\n",
    "                              .withColumnRenamed(\"CallSign\", \"c_CallSign\")\n",
    "\n",
    "val data_station = data_station0.join(liste_airp,data_station0(\"c_CallSign\") === liste_airp(\"CallSign\"),\"inner\")\n",
    "                                .select(\"CallSign\", \"WBAN\",\"WMO\",\"Name\",\"State\",\"GroundHeight\", \"Barometer\", \"TimeZone\")\n",
    "\n",
    "//println(s\"Nb stations : ${data_station.count()}\")\n",
    "//data_station.show(5, false)\n",
    "\n",
    "\n",
    "// Identification des aéroports sans station\n",
    "/*\n",
    "val data_missed_airp = liste_airp.join(data_station0, liste_airp(\"CallSign\")===data_station0(\"c_CallSign\"),\"left\")\n",
    "                                 .filter(\"c_CallSign is null\")\n",
    "                                 .select(\"CallSign\")\n",
    "println(s\"Nb missed airports : ${data_missed_airp.count()}\")\n",
    "data_missed_airp.show(false)\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Fichier des relevés mensuels (monthly.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly data : 1225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_monthly: org.apache.spark.sql.DataFrame = [c_WBAN: string, YearMonth: string ... 48 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "val data_monthly = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                             .load(pathQ+date1+\"monthly.txt\")\n",
    "                             .withColumnRenamed(\"WBAN\",\"c_WBAN\")\n",
    "\n",
    "println(s\"monthly data : ${data_monthly.count()}\")\n",
    "\n",
    "/* Finaliser les données mensuelles à récupérer \n",
    "\n",
    "data_monthly.printSchema\n",
    "data_monthly.select(\"Wban\",\"YearMonth\",\"AvgMaxTemp\",\"AvgMinTemp\",\"AvgTemp\",\"HeatingDegreeDays\",\n",
    "                    \"HDDSeasonToDate\",\"ThunderstormDays\",\"HeavyFogDays\").show(false)\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Fichier des relevés quotidiens (daily.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily data : 36750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_daily: org.apache.spark.sql.DataFrame = [c_WBAN: string, YearMonthDay: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "val data_daily = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                             .load(pathQ+date1+\"daily.txt\")\n",
    "                             .withColumnRenamed(\"WBAN\",\"c_WBAN\")\n",
    "                             .select(\"c_WBAN\",\"YearMonthDay\",\"Tmax\",\"Tmin\",\"Tavg\",\"DewPoint\",\"WetBulb\", \"Heat\",\n",
    "                                     \"Cool\",\"Sunrise\",\"Sunset\",\"CodeSum\",\"Depth\",\"Water1\", \"SnowFall\",\n",
    "                                     \"StnPressure\", \"SeaLevel\", \"ResultSpeed\", \"ResultDir\", \"AvgSpeed\", \"Max5Speed\",\n",
    "                                     \"Max5Dir\", \"Max2Speed\", \"Max2Dir\")\n",
    "println(s\"daily data : ${data_daily.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Fichier des relevés horaires (hourly.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+-----+--------------------+-----+-----+-----+\n",
      "|   T|  H| Wd| Ws|    P|                   S|    V|WType|RType|\n",
      "+----+---+---+---+-----+--------------------+-----+-----+-----+\n",
      "|-4.0| 78|000|  0|21.52|              OVC026|10.00|     |   AA|\n",
      "|-3.6| 85|000|  0|21.52|              OVC026|10.00|     |   AA|\n",
      "|-3.6| 85|000|  0|21.53|              OVC026|10.00|     |   AA|\n",
      "|-3.3| 82|140|  3|21.52|              OVC026|10.00|     |   AA|\n",
      "|-3.6| 85|000|  0|21.53|              OVC026|10.00|     |   AA|\n",
      "|-3.6| 85|000|  0|21.53|              OVC026|10.00|     |   AA|\n",
      "|-3.6| 85|000|  0|21.52|              OVC026|10.00|     |   AA|\n",
      "|-3.6| 85|000|  0|21.52|              OVC026|10.00|     |   AA|\n",
      "|-2.9| 89|250|  8|21.52|              OVC026|10.00|  -SN|   AA|\n",
      "|-2.2| 82|260|  8|21.52|BKN026 BKN030 OVC060|10.00|  -SN|   AA|\n",
      "|-2.2| 82|260|  6|21.53|BKN024 BKN032 OVC060|10.00|  -SN|   AA|\n",
      "|-2.2| 82|270|  8|21.54|       BKN024 OVC032|10.00|     |   AA|\n",
      "|-2.2| 82|260|  8|21.54|       BKN024 OVC035|10.00|  -SN|   AA|\n",
      "|-2.2| 82|260| 13|21.55|SCT026 SCT032 OVC060|10.00|  -SN|   AA|\n",
      "|-2.2| 82|260| 10|21.55|SCT026 BKN031 BKN060|10.00|     |   AA|\n",
      "|-2.2| 82|260| 13|21.55|SCT024 SCT029 SCT040|10.00|     |   AA|\n",
      "|-2.2| 82|270| 14|21.55|       SCT025 SCT030|10.00|     |   AA|\n",
      "|-2.2| 82|270| 14|21.56|                 CLR|10.00|     |   AA|\n",
      "|-1.6| 75|320|  7|21.56|SCT023 BKN031 BKN038|10.00|     |   AA|\n",
      "|-1.6| 75|300|  9|21.57|       BKN023 OVC034|10.00|     |   AA|\n",
      "+----+---+---+---+-----+--------------------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- c_WBAN: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StationType: string (nullable = true)\n",
      " |-- SkyCondition: string (nullable = true)\n",
      " |-- SkyConditionFlag: string (nullable = true)\n",
      " |-- Visibility: string (nullable = true)\n",
      " |-- VisibilityFlag: string (nullable = true)\n",
      " |-- WeatherType: string (nullable = true)\n",
      " |-- WeatherTypeFlag: string (nullable = true)\n",
      " |-- DryBulbFarenheit: string (nullable = true)\n",
      " |-- DryBulbFarenheitFlag: string (nullable = true)\n",
      " |-- DryBulbCelsius: string (nullable = true)\n",
      " |-- DryBulbCelsiusFlag: string (nullable = true)\n",
      " |-- WetBulbFarenheit: string (nullable = true)\n",
      " |-- WetBulbFarenheitFlag: string (nullable = true)\n",
      " |-- WetBulbCelsius: string (nullable = true)\n",
      " |-- WetBulbCelsiusFlag: string (nullable = true)\n",
      " |-- DewPointFarenheit: string (nullable = true)\n",
      " |-- DewPointFarenheitFlag: string (nullable = true)\n",
      " |-- DewPointCelsius: string (nullable = true)\n",
      " |-- DewPointCelsiusFlag: string (nullable = true)\n",
      " |-- RelativeHumidity: string (nullable = true)\n",
      " |-- RelativeHumidityFlag: string (nullable = true)\n",
      " |-- WindSpeed: string (nullable = true)\n",
      " |-- WindSpeedFlag: string (nullable = true)\n",
      " |-- WindDirection: string (nullable = true)\n",
      " |-- WindDirectionFlag: string (nullable = true)\n",
      " |-- ValueForWindCharacter: string (nullable = true)\n",
      " |-- ValueForWindCharacterFlag: string (nullable = true)\n",
      " |-- StationPressure: string (nullable = true)\n",
      " |-- StationPressureFlag: string (nullable = true)\n",
      " |-- PressureTendency: string (nullable = true)\n",
      " |-- PressureTendencyFlag: string (nullable = true)\n",
      " |-- PressureChange: string (nullable = true)\n",
      " |-- PressureChangeFlag: string (nullable = true)\n",
      " |-- SeaLevelPressure: string (nullable = true)\n",
      " |-- SeaLevelPressureFlag: string (nullable = true)\n",
      " |-- RecordType: string (nullable = true)\n",
      " |-- RecordTypeFlag: string (nullable = true)\n",
      " |-- HourlyPrecip: string (nullable = true)\n",
      " |-- HourlyPrecipFlag: string (nullable = true)\n",
      " |-- Altimeter: string (nullable = true)\n",
      " |-- AltimeterFlag: string (nullable = true)\n",
      " |-- c_Month: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_hourly: org.apache.spark.sql.DataFrame = [c_WBAN: string, Date: string ... 43 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "val data_hourly = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                       .load(pathQ+date1+\"hourly.txt\")\n",
    "                       .withColumnRenamed(\"WBAN\",\"c_WBAN\")\n",
    "                       .withColumn(\"c_Month\",substring($\"Date\", 1, 6))\n",
    "                       /*.select(\"c_WBAN\",\"Date\",\"c_Month\",\"Time\",\"StationType\",\"SkyCondition\",\"Visibility\",\"WeatherType\",\n",
    "                               \"DryBulbFarenheit\",\"WetBulbFarenheit\",\"DewPointFarenheit\",\n",
    "                               \"WindSpeed\",\"WindDirection\",\"StationPressure\",\"SeaLevelPressure\",\"HourlyPrecip\")*/\n",
    "\n",
    "//print(\"hourly data : ${data_hourly.count()} \")\n",
    "\n",
    "data_hourly.select(\"WetBulbCelsius\", \"RelativeHumidity\",\"WindDirection\",\"WindSpeed\",\"StationPressure\",\n",
    "                   \"SkyCondition\", \"Visibility\", \"WeatherType\", \"RecordType\")\n",
    "           .withColumnRenamed(\"WetBulbCelsius\",\"T\")\n",
    "           .withColumnRenamed(\"RelativeHumidity\",\"H\")\n",
    "           .withColumnRenamed(\"WindDirection\", \"Wd\")\n",
    "           .withColumnRenamed(\"WindSpeed\", \"Ws\")\n",
    "           .withColumnRenamed(\"StationPressure\", \"P\")\n",
    "           .withColumnRenamed(\"SkyCondition\", \"S\")\n",
    "           .withColumnRenamed(\"Visibility\", \"V\")\n",
    "           .withColumnRenamed(\"WeatherType\", \"WType\")\n",
    "           .withColumnRenamed(\"RecordType\", \"RType\")\n",
    "           .show()\n",
    "/* Article : \n",
    "T : Temperature (WetBulbCelsius), H : Humidity (RelativeHumidity), Wd : wind direction (WindDirection), \n",
    "Ws : wind speed (WindSpeed), P : barometric pressure (StationPressure)\n",
    "S : Sky Condition (SkyCondition), V : visibility (Visibility), D : weather phenomena descriptor (WeatherType, Record Type ?)\n",
    "\n",
    "\n",
    "Non reprises : HourlyPrecip, SeaLevelPressure\n",
    "*/\n",
    "data_hourly.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (v) Fichier des précipitations (precip.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* L'information semble être déjà reprise dans la variable HourlyPrecip du fichier hourly.txt\n",
    "\n",
    "val data_precip = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                             .load(pathQ+date1+\"precip.txt\")\n",
    "\n",
    "data_hourly.select(\"Wban\", \"Date\", \"Time\", \"HourlyPrecip\").where($\"HourlyPrecip\" !== \" \")\n",
    ".join(data_precip.select(\"Wban\",\"YearMonthDay\",\"Hour\",\"Precipitation\"), \n",
    "     data_hourly(\"Wban\")===data_precip(\"Wban\") && data_hourly(\"Date\")===data_precip(\"YearMonthDay\"), \"inner\")\n",
    ".where($\"Precipitation\" !== \" \")\n",
    ".show(80,false)\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (vi) => Dataframe Weather Observation File (WOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOF_201311 : 296924"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_meteo_a: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 50 more fields]\n",
       "data_meteo_b: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 73 more fields]\n",
       "WOF_201311: org.apache.spark.sql.DataFrame = [CallSign: string, WBAN: string ... 119 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DATAFRAME DE SYNTHESE regroupant l'ensemble des informations météo pertinentes\n",
    "val data_meteo_a = data_station.join(data_hourly, data_station(\"WBAN\") === data_hourly(\"c_WBAN\"), \"inner\")\n",
    "                               .drop(\"c_WBAN\")\n",
    "//println(s\"meteo_a : ${data_meteo_a.count()}\")\n",
    "\n",
    "val data_meteo_b = data_meteo_a.join(data_daily, data_meteo_a(\"WBAN\") === data_daily(\"c_WBAN\") &&\n",
    "                                                 data_meteo_a(\"Date\") === data_daily(\"YearMonthDay\"), \"left\")\n",
    "                               .drop(\"c_WBAN\")\n",
    "//println(s\"meteo_b : ${data_meteo_b.count()}\")\n",
    "\n",
    "val WOF_201311= data_meteo_b.join(data_monthly, data_meteo_b(\"WBAN\") === data_monthly(\"c_WBAN\") &&\n",
    "                                                data_meteo_b(\"c_Month\") === data_monthly(\"yearMonth\"), \"left\")\n",
    "                            .drop(\"c_WBAN\",\"c_Month\", \"YearMonth\",\"YearMonthDay\")\n",
    "print(s\"WOF_201311 : ${WOF_201311.count()}\")\n",
    "// WOT_201311.printSchema       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb Wban absents data daily : 784\n",
      "Nb Wban absents data monthly :27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_h: org.apache.spark.sql.DataFrame = [WBAN: string, Date: string]\n",
       "data_d: org.apache.spark.sql.DataFrame = [WBAN: string, Date: string]\n",
       "data_sub: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [WBAN: string, Date: string]\n",
       "data_h2: org.apache.spark.sql.DataFrame = [WBAN: string, c_Month: string]\n",
       "data_m: org.apache.spark.sql.DataFrame = [WBAN: string, c_Month: string]\n",
       "data_sub2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [WBAN: string, c_Month: string]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Remarque : présence de couples WBAN/Date dans le fichier hourly mais absents de daily ou de monthly.\n",
    "\n",
    "A voir si l'on se restreint aux observations pour lesquelles nous disposons à la fois des données horaires,\n",
    "quotidiennes et mensuelles. Ici le périmètre est celui - plus large des données horaires avec valeurs\n",
    "manquantes sur les daily ou monthly donc */\n",
    "\n",
    "// écarts hourly et daily\n",
    "val data_h = data_meteo_a.select(\"WBAN\",\"Date\")\n",
    "val data_d = data_daily.withColumnRenamed(\"c_WBAN\", \"WBAN\")\n",
    "                               .withColumnRenamed(\"YearMonthDay\",\"Date\")\n",
    "                               .select(\"WBAN\",\"Date\")\n",
    "\n",
    "val data_sub = data_h.except(data_d)\n",
    "print(\"Nb Wban absents data daily : \")\n",
    "println(data_sub.count())\n",
    "\n",
    "// écarts hourly et monthly\n",
    "val data_h2 = data_meteo_a.select(\"WBAN\",\"c_Month\")\n",
    "val data_m = data_monthly.withColumnRenamed(\"c_WBAN\", \"WBAN\")\n",
    "                               .withColumnRenamed(\"YearMonth\",\"c_Month\")\n",
    "                               .select(\"WBAN\",\"c_Month\")\n",
    "\n",
    "val data_sub2 = data_h2.except(data_m)\n",
    "print(\"Nb Wban absents data monthly :\")\n",
    "println(data_sub2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2/2 - Mise en forme des données météo => Weather Observation Table (WOT)\n",
    "Cette étape se déroule en deux temps :\n",
    "* Ajustement du périmètre des relevés : uniquement les stations et les jours pour lesquels les relevés horaires sont disponibles chaque heure sur 24 heures (0 à 23h).\n",
    "* Création d'une colonne Weather_h (Array) reprenant les 24 relevés horaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+----+-------+----+---+---+---+-----+--------------------+-----+-----+-----+\n",
      "|CallSign|State|    Date|Time|GHeight|   T|  H| Wd| Ws|    P|                   S|    V|WType|RType|\n",
      "+--------+-----+--------+----+-------+----+---+---+---+-----+--------------------+-----+-----+-----+\n",
      "|     MEI|   MS|20131101|0056|    294|17.4| 97|280|  7|29.52|SCT010 BKN017 OVC060|10.00|     |   SP|\n",
      "|     MEI|   MS|20131101|0058|    294|17.6| 93|280|  8|29.52|FEW010 BKN014 OVC060|10.00|     |   AA|\n",
      "|     MEI|   MS|20131101|0113|    294|17.4| 97|250|  8|29.52|FEW010 BKN015 BKN020|10.00|     |   SP|\n",
      "|     MEI|   MS|20131101|0123|    294|16.7| 90|270|  5|29.51|FEW012 SCT020 BKN055|10.00|     |   SP|\n",
      "|     MEI|   MS|20131101|0158|    294|16.5| 93|250|  5|29.51|                 CLR|10.00|     |   AA|\n",
      "|     MEI|   MS|20131101|0258|    294|15.7| 97|220|  5|29.51|       FEW010 SCT050|10.00|     |   AA|\n",
      "|     MEI|   MS|20131101|0316|    294|16.1|100|000|  0|29.51|       FEW008 SCT050|10.00|     |   SP|\n",
      "|     MEI|   MS|20131101|0330|    294|16.1|100|000|  0|29.51|       BKN008 BKN050|10.00|     |   SP|\n",
      "|     MEI|   MS|20131101|0356|    294|16.1|100|300|  7|29.51|FEW007 BKN013 BKN050|10.00|     |   SP|\n",
      "|     MEI|   MS|20131101|0358|    294|15.4| 93|310|  7|29.51|SCT011 SCT016 BKN050|10.00|     |   AA|\n",
      "+--------+-----+--------+----+-------+----+---+---+---+-----+--------------------+-----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "echan_hourly: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Echantillon de travail : on se limite ici aux infos météo sélectionnées\n",
    "\n",
    "val echan_hourly = WOF_201311.select(\"CallSign\", \"State\", \"Date\", \"Time\",\n",
    "                                     \"GroundHeight\",\n",
    "                                     \"WetBulbCelsius\",\"RelativeHumidity\",\"WindDirection\",\"WindSpeed\",\"StationPressure\",\n",
    "                                     \"SkyCondition\", \"Visibility\", \"WeatherType\", \"RecordType\")\n",
    "                             .withColumnRenamed(\"WetBulbCelsius\",\"T\")\n",
    "                             .withColumnRenamed(\"RelativeHumidity\",\"H\")\n",
    "                             .withColumnRenamed(\"WindDirection\", \"Wd\")\n",
    "                             .withColumnRenamed(\"WindSpeed\", \"Ws\")\n",
    "                             .withColumnRenamed(\"StationPressure\", \"P\")\n",
    "                             .withColumnRenamed(\"SkyCondition\", \"S\")\n",
    "                             .withColumnRenamed(\"Visibility\",\"V\")\n",
    "                             .withColumnRenamed(\"WeatherType\",\"WType\")\n",
    "                             .withColumnRenamed(\"RecordType\",\"RType\")\n",
    "                             .withColumnRenamed(\"GroundHeight\",\"GHeight\")\n",
    "\n",
    "echan_hourly.show(10)\n",
    "//println(echan_hourly.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Limitation du périmètre aux stations/Date avec 24 relevés horaires\n",
    "\n",
    "En vue du croisement avec les données sur les vols :\n",
    "* Création de la variable DateH  correspondant à Date en format date\n",
    "\n",
    "* Création de la variable TimeH correspondant à l'heure pleine la plus proche de l'heure de relevé (Time)\n",
    "\n",
    "* Limitation aux dates pour lesquelles il existe 24 relevés météo par jour (un par heure). En effet, dans certains cas, il y a des relevés intermédiaires ou il y en a moins que 24. La variable _t_ correspond au numéro de l'heure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb stations avec relevé météo sur 24h : 204600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.unix_timestamp\n",
       "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
       "import org.apache.spark.sql.functions.{concat, lit, udf}\n",
       "import org.apache.spark.sql.functions.{hour, minute, second}\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "w_deltaNb: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@47eeb923\n",
       "w_deltaRg: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@31cf40d6\n",
       "hourly: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 16 more fields]\n",
       "hourly2: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 15 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp\n",
    "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
    "import org.apache.spark.sql.functions.{concat, lit, udf}\n",
    "import org.apache.spark.sql.functions.{hour,minute,second}\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "// Fonctions d'agrégation (window)\n",
    "val w_deltaNb = Window\n",
    "                    .partitionBy(\"CallSign\",\"Date\",\"Delta\")\n",
    "                    .orderBy($\"Delta\" .asc)\n",
    "\n",
    "val w_deltaRg = Window\n",
    "                    .partitionBy(\"CallSign\",\"Date\")\n",
    "                    .orderBy($\"Delta\" .asc)\n",
    "\n",
    "val hourly = echan_hourly.withColumn(\"DateH\",to_date(unix_timestamp($\"Date\",\"yyyyMMdd\").cast(\"timestamp\")))\n",
    "                         .withColumn(\"Time00\",concat(substring($\"Time\",1,2),lit(\"00\")))\n",
    "                         .withColumn(\"DTime\", concat(from_unixtime(unix_timestamp($\"Date\",\"yyyyMMdd\"),\"yyyy-MM-dd\"), lit(\" \"), \n",
    "                                                     from_unixtime(unix_timestamp($\"Time\",\"HHmm\"),\"HH:mm:ss\")).cast(\"timestamp\"))\n",
    "                         .withColumn(\"DTimeH\", concat(from_unixtime(unix_timestamp($\"Date\",\"yyyyMMdd\"),\"yyyy-MM-dd\"), lit(\" \"),\n",
    "                                                      from_unixtime(unix_timestamp($\"Time00\",\"HHmm\"),\"HH:mm:ss\")).cast(\"timestamp\"))                  \n",
    "                         .withColumn(\"Delta\",minute($\"DTime\")-minute($\"DTimeH\"))\n",
    "                         .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\" .asc)\n",
    "                         .withColumn(\"CountDelta\",count(\"Delta\") over w_deltaNb )\n",
    "                         .where($\"CountDelta\"===24)\n",
    "                         .drop(\"Time00\",\"Dtime\")\n",
    "\n",
    "// On ne conserve qu'un relevé horaire de 24 heures lorsqu'il y en a plusieurs sur une même journée\n",
    "\n",
    "val hourly2 = hourly.withColumn(\"RgDelta\", rank() over w_deltaRg )\n",
    "                    .where($\"RgDelta\" ===1)               \n",
    "                    .withColumn(\"_t_\", hour($\"DTimeH\"))\n",
    "                    .drop(\"Delta\",\"CountDelta\",\"RgDelta\")\n",
    "\n",
    "println(s\"Nb stations avec relevé météo sur 24h : ${hourly2.count()}\")\n",
    "/*\n",
    "hourly2.select(\"CallSign\",\"Date\",\"DateH\",\"Time\",\"DTimeH\",\"_t_\")\n",
    "       .where($\"CallSign\"===\"DTW\").show(25, false)\n",
    "hourly2.printSchema()\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Création de la colonne Weather_h reprenant sous forme d'un Array les 24 relevés météo\n",
    "WOT_201311 est une table de niveau station / Date en heure pleine (DateH) donnant les infos météo sous la forme d'un Array. 3 colonnes : CallSign, DateH et Weather_h\n",
    "\n",
    "* Dans un premier temps, la colonne SkyCondition, codifiant les couches nuageuses identifiées, est retraitée.\n",
    "    \n",
    "    Une couche nuageuse est caractérisée par un code en 3 lettres (CLR, OVC, SCT, ...) suivi d'un code de 3 chiffres correspondant à la hauteur en centaine de milliers de pieds (015 = 1500 pieds). Au code CLR (Clear) n'est pas associé de hauteur.\n",
    "    * La colonne SkyCondition est splitée en plusiers colonnes (\"SC_1\", \"SC_2\", \"SC_3\") correspondant aux différentes couches nuageuses mesurées (jusqu'à 3 colonnes pour une station automatique). Le nombre de couches nuageuses est stocké dans une colonne \"NumLayer\".\n",
    "    * Chaque colonne correspondant à une couche nuageuse est elle-même splitée en 2 (SC_1_code et SC_1_high) afin de séparer le code de 3 lettres (CLR, OVC, SCT...) et la hauteur de la couche. En cas de ciel dégagé, le code CLR est marqué \"000\" ainsi que la variable caractérisant la hauteur. Le code de couverture nuageuse est ensuite indexé, par ordre alphabétique décroissant (\"SC_1_code_ix).\n",
    "\n",
    "* Le même procédé est appliqué pour la colonne Weather Type (WType) qui contient 3 catégories d'informations et un niveau d'intensité (cf. code METAR) : une description du temps, des précipitation et de l'obsctruction éventuelle du ciel. \n",
    "    * Il peut même exister une information sur la situation aux alentours. A noter que ces informations ne sont pas uniques : par exemple il peut y avoir jusque 3 descriptions du temps, des précipitations ou de l'obstruction du ciel  et nous avons considéré ici la première disponible.\n",
    "    * nous construisons alors pour chaque relevé météo 4 variables : IN (INtensity), VC (ViCinity : alentours), DR (weather DescRiption), PR (PRecipitation), OB (OBscuration).\n",
    "\n",
    "* Dans un second temps, les infos météo de type numérique (windspeed, station pressure, NumLayer,SC_1_high... ) sont transformées en Double.\n",
    "\n",
    "La colonne Weather_h peut être construite à partir de 3 listes de données météo :\n",
    "* les données de base (base)\n",
    "* les données de base complétées de SkyCondition\n",
    "* les données de base, de SkyCondition et de WType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table skyCond_splitted : 204600 obs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.Column\n",
       "skyCond: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 16 more fields]\n",
       "splitCode: (DFrame: org.apache.spark.sql.DataFrame, df_col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame\n",
       "numColsSC: Int = 3\n",
       "skyCond_first_split: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 19 more fields]\n",
       "listColsSC: List[String]\n",
       "resultDF: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 25 more fields]\n",
       "skyCond_splitted: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Splitting of SkyCondition column => skyCond_splitted\n",
    "\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val skyCond = hourly2.withColumn(\"SkyC\", split($\"S\", \" \"))\n",
    "\n",
    "// Function to split code part and layer high from SkyCondition observation\n",
    "def splitCode(DFrame : DataFrame, df_col : Column): DataFrame = {\n",
    "    val fields = List( (df_col.toString+\"_code\",\"CLR\"),(df_col.toString+\"_high\",\"000\") )\n",
    "\n",
    "    fields.foldLeft( (DFrame,1) ){ \n",
    "    (tempdf, field) =>\n",
    "    val newdf = tempdf._1.withColumn(field._1, when(df_col ===\"CLR\",\"000\").\n",
    "                                               otherwise(substring(df_col, tempdf._2,3)))\n",
    "                         .withColumn(field._1, when(col(field._1) .isNull,\"000\").\n",
    "                                               otherwise(col(field._1)))    \n",
    "    (newdf, tempdf._2 + 3)\n",
    "  }._1\n",
    "}\n",
    "\n",
    "// first split \n",
    "val numColsSC = skyCond\n",
    "                 .withColumn(\"SkyC_size\", size($\"SkyC\"))\n",
    "                 .agg(max($\"SkyC_size\"))\n",
    "                 .head()\n",
    "                 .getInt(0)\n",
    "    \n",
    "val skyCond_first_split = skyCond.select(col(\"*\") +:\n",
    "                            (1 until numColsSC + 1).map(i => $\"SkyC\".getItem(i-1).as(s\"S_$i\")): _*)\n",
    "                          .withColumn(\"NumLayer\",size($\"SkyC\"))\n",
    "                          .drop($\"SkyC\")\n",
    "\n",
    "// second split, colonnes \"SC_i\", i=1 to numColsSC - utilisation de la fonction splitCode\n",
    "def listColsSC = for (i <- (1 to numColsSC).toList) yield (\"S_\"+i)\n",
    "\n",
    "var resultDF = skyCond_first_split\n",
    "for (x <- listColsSC) {\n",
    "    resultDF = splitCode(resultDF,col(x))\n",
    "}\n",
    "val skyCond_splitted = resultDF.drop(listColsSC:_*)\n",
    "\n",
    "println(s\"Table skyCond_splitted : ${skyCond_splitted.count()} obs.\")\n",
    "/*skyCond_splitted.select(\"CallSign\",\"Date\",\"Time\",\"S\", \"NumLayer\",\"S_1_code\", \"S_1_high\", \n",
    "                        \"S_2_code\", \"S_2_high\", \"S_3_code\", \"S_3_high\").show(10)*/\n",
    "//skyCond_splitted.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table wType_splitted : 204600 obs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StringType, IntegerType}\n",
       "import org.apache.spark.sql.functions.regexp_extract\n",
       "wType: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 23 more fields]\n",
       "numColsWT: Int = 3\n",
       "wType_first_split: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 26 more fields]\n",
       "splitWT: (DFrame: org.apache.spark.sql.DataFrame, df_col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame\n",
       "listColsWT: List[String]\n",
       "listD: List[String]\n",
       "listP: List[String]\n",
       "listO: List[String]\n",
       "resultDF: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 83 more fields]\n",
       "wType_splitted: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 27 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Splitting of Weather Type column => wType_splitted\n",
    "\n",
    "import org.apache.spark.sql.types.{StringType, IntegerType}\n",
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "\n",
    "val wType = skyCond_splitted.withColumn(\"WType_array\", split($\"WType\", \" \"))\n",
    "\n",
    "// first split \n",
    "val numColsWT = wType\n",
    "                 .withColumn(\"WType_array_size\", size($\"WType_array\"))\n",
    "                 .agg(max($\"WType_array_size\"))\n",
    "                 .head()\n",
    "                 .getInt(0)\n",
    "    \n",
    "val wType_first_split = wType.select(col(\"*\") +:\n",
    "                            (1 until numColsWT + 1).map(i => $\"WType_array\".getItem(i-1).as(s\"WT_$i\")): _*)\n",
    "\n",
    "// second split\n",
    "def splitWT(DFrame : DataFrame, df_col : Column): DataFrame = {\n",
    "    val fields = List(df_col.toString+\"_1\", df_col.toString+\"_2\", df_col.toString+\"_3\")\n",
    "\n",
    "    fields.foldLeft( (DFrame,1) ){ \n",
    "    (tempdf, field) =>\n",
    "        val newdf = tempdf._1.withColumn(field, substring(df_col,tempdf._2,2))                                               \n",
    "        (newdf, tempdf._2 + 2)\n",
    "  }._1\n",
    "}\n",
    "\n",
    "def listColsWT = for (i <- (1 to numColsWT).toList) yield (\"WT_\"+i)\n",
    "def listD = List(\"MI\",\"PR\",\"BC\",\"DR\",\"BL\",\"SH\",\"TS\",\"FZ\",\"XX\")\n",
    "def listP = List(\"RA\",\"DZ\",\"SN\",\"SG\",\"IC\",\"PL\",\"GR\",\"GS\",\"UP\")\n",
    "def listO = List(\"FG\",\"VA\",\"BR\",\"HZ\",\"DU\",\"FU\",\"SA\",\"PY\")\n",
    "\n",
    "var resultDF = wType_first_split\n",
    "\n",
    "for (x <- listColsWT) {\n",
    "    val WT_size = WType_first_split\n",
    "                   .withColumn(\"WT_size\", length(col(x)))\n",
    "                   .agg(max($\"WT_size\"))\n",
    "                   .head()\n",
    "                   .getInt(0)\n",
    "    \n",
    "    resultDF = resultDF.withColumn(x.toString+\"_I\",regexp_extract(col(x), \"[-+]\", 0))\n",
    "                       .withColumn(x.toString+\"_VC\", when(substring(col(x),1,2)===\"VC\",substring(col(x),3,WT_size)).otherwise(\"00\") )\n",
    "                       .withColumn(\"new_\"+x.toString,when( (col(x.toString+\"_I\") ===\"+\" || col(x.toString+\"_I\") ===\"-\"),\n",
    "                                                       substring(col(x),2,WT_size))\n",
    "                                                     .when(substring(col(x),1,2) === \"VC\",\"00\")\n",
    "                                                     .otherwise(col(x)) )\n",
    "\n",
    "    resultDF = splitWT(resultDF,col(\"new_\"+x.toString))\n",
    "    \n",
    "    for (y <- (1 until 4)) {\n",
    "        resultDF = resultDF\n",
    "                      .withColumn(\"I\"+x.toString.slice(2,4), when(col(x.toString+\"_I\")===\"+\",\"ST\")\n",
    "                                                    .when(col(x.toString+\"_I\")===\"-\",\"LO\")\n",
    "                                                    .otherwise(\"00\"))\n",
    "                      .withColumnRenamed(x.toString+\"_VC\",\"VC\"+x.toString.slice(2,4)) \n",
    "                      .withColumn(\"D\"+x.toString.slice(2,4)+\"_\"+y,when(col(\"new_\"+x.toString+\"_\"+y).isin(listD:_*),\n",
    "                                                        col(\"new_\"+x.toString+\"_\"+y) )\n",
    "                                                    .otherwise(\"00\"))\n",
    "                      .withColumn(\"P\"+x.toString.slice(2,4)+\"_\"+y,when(col(\"new_\"+x.toString+\"_\"+y).isin(listP:_*),\n",
    "                                                        col(\"new_\"+x.toString+\"_\"+y) )\n",
    "                                                    .otherwise(\"00\"))\n",
    "                      .withColumn(\"O\"+x.toString.slice(2,4)+\"_\"+y,when(col(\"new_\"+x.toString+\"_\"+y).isin(listO:_*),\n",
    "                                                        col(\"new_\"+x.toString+\"_\"+y) )\n",
    "                                                     .otherwise(\"00\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "for (y <- (1 until 4)) {\n",
    "    resultDF = resultDF\n",
    "                .withColumn(\"D_\"+y, when(col(\"D_\"+y+\"_1\") !== \"00\",col(\"D_\"+y+\"_1\")).otherwise(when(col(\"D_\"+y+\"_2\") !== \"00\",col(\"D_\"+y+\"_2\")).otherwise(col(\"D_\"+y+\"_3\"))))\n",
    "                .withColumn(\"P_\"+y, when(col(\"P_\"+y+\"_1\") !== \"00\",col(\"P_\"+y+\"_1\")).otherwise(when(col(\"P_\"+y+\"_2\") !== \"00\",col(\"P_\"+y+\"_2\")).otherwise(col(\"P_\"+y+\"_3\"))))\n",
    "                .withColumn(\"O_\"+y, when(col(\"O_\"+y+\"_1\") !== \"00\",col(\"O_\"+y+\"_1\")).otherwise(when(col(\"O_\"+y+\"_2\") !== \"00\",col(\"O_\"+y+\"_2\")).otherwise(col(\"O_\"+y+\"_3\"))))\n",
    "}\n",
    "\n",
    "// restriction to the first descriptor found\n",
    "val wType_splitted = resultDF\n",
    "                        .withColumn(\"DR\", when($\"D_1\" !==\"00\",$\"D_1\").when($\"D_2\" !== \"00\", $\"D_2\").otherwise($\"D_3\"))\n",
    "                        .withColumn(\"PR\", when($\"P_1\" !==\"00\",$\"P_1\").when($\"P_2\" !== \"00\", $\"P_2\").otherwise($\"P_3\"))\n",
    "                        .withColumn(\"OB\", when($\"O_1\" !==\"00\",$\"O_1\").when($\"O_2\" !== \"00\", $\"O_2\").otherwise($\"O_3\"))\n",
    "                        .withColumn(\"IN\", when($\"I_1\" !==\"00\",$\"I_1\").when($\"I_2\" !== \"00\", $\"I_2\").otherwise($\"I_3\"))\n",
    "                        .withColumn(\"VC\", when($\"VC_1\" !==\"00\",$\"VC_1\").when($\"VC_2\" !== \"00\", $\"VC_2\").otherwise($\"VC_3\"))\n",
    "                        .drop(\"WT_1\",\"WT_2\",\"WT_3\",\"WT_1_I\", \"WT_2_I\",\"WT_3_I\",\"new_WT_1\",\"new_WT_2\",\"new_WT_3\",\n",
    "                              \"D_1_1\", \"P_1_1\", \"O_1_1\", \"new_WT_1_1\", \"D_2_1\", \"P_2_1\", \"O_2_1\", \"new_WT_2_1\",\n",
    "                              \"D_3_1\", \"P_3_1\", \"O_3_1\", \"new_WT_3_1\",\n",
    "                              \"D_1_2\", \"P_1_2\", \"O_1_2\", \"new_WT_1_2\", \"D_2_2\", \"P_2_2\", \"O_2_2\", \"new_WT_2_2\",\n",
    "                              \"D_3_2\", \"P_3_2\", \"O_3_2\", \"new_WT_3_2\",\n",
    "                              \"D_1_3\", \"P_1_3\", \"O_1_3\", \"new_WT_1_3\", \"D_2_3\", \"P_2_3\", \"O_2_3\", \"new_WT_2_3\",\n",
    "                              \"D_3_3\", \"P_3_3\", \"O_3_3\", \"new_WT_3_3\",\n",
    "                              \"I_1\",\"I_2\",\"I_3\",\"D_1\",\"D_2\",\"D_3\",\"O_1\",\"O_2\",\"O_3\",\"P_1\",\"P_2\",\"P_3\",\n",
    "                              \"VC_1\",\"VC_2\",\"VC_3\",\"WType_array\")\n",
    "\n",
    "println(s\"Table wType_splitted : ${wType_splitted.count()} obs.\")\n",
    "/*wType_splitted.select(\"WType\",\"VC\",\"DR\",\"IN\",\"PR\",\"OB\")\n",
    "                  .where($\"WType\" !==\" \")\n",
    "                  .where($\"VC\" !== \"00\" )\n",
    "                  .show()\n",
    "*/\n",
    "//wType_splitted.printSchema"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "METAR WEATHER CODES :\n",
    "\n",
    "Intensity \t- \tLight intensity\n",
    "Intensity \t(blank) \tModerate intensity\n",
    "Intensity \t+ \tHeavy intensity\n",
    "Intensity \tVC \tIn the vicinity \n",
    "\n",
    "Descriptor  MI \tShallow (French: Mince)\n",
    "Descriptor \tPR \tPartial\n",
    "Descriptor \tBC \tPatches (French: Bancs)\n",
    "Descriptor \tDR \tLow drifting (chasse basse)\n",
    "Descriptor \tBL \tBlowing (chasse haute)\n",
    "Descriptor \tSH \tShowers (averses)\n",
    "Descriptor \tTS \tThunderstorm (orage)\n",
    "Descriptor \tFZ \tFreezing (se congélant)\n",
    "Descriptor  XX  Violent\n",
    "\n",
    "Precipitation \tRA \tRain\n",
    "Precipitation \tDZ \tDrizzle  (bruine)\n",
    "Precipitation \tSN \tSnow\n",
    "Precipitation \tSG \tSnow Grains (neige en grains)\n",
    "Precipitation \tIC \tIce Crystals (cristaux de glace)\n",
    "Precipitation \tPL \tIce Pellets  (granules de glace)\n",
    "Precipitation \tGR \tHail (in the US includes Small Hail)[1] (French: Grêle)\n",
    "Precipitation \tGS \tGraupel, Snow Pellets and/or Small Hail (not in the US)[2] (French: Grésil)\n",
    "Precipitation \tUP \tUnknown Precipitation \n",
    "\n",
    "Obscuration \tFG \tFog\n",
    "Obscuration \tVA \tVolcanic Ash (cendres volcaniques)\n",
    "Obscuration \tBR \tMist (French: Brume)\n",
    "Obscuration \tHZ \tHaze (Brume sèche)\n",
    "Obscuration \tDU \tWidespread Dust (poussière)\n",
    "Obscuration \tFU \tSmoke (French: Fumée)\n",
    "Obscuration \tSA \tSand\n",
    "Obscuration \tPY \tSpray \n",
    "\n",
    "Other \tSQ \tSquall\n",
    "Other \tPO \tDust (French: Poussière) or Sand Whirls\n",
    "Other \tDS \tDuststorm\n",
    "Other \tSS \tSandstorm\n",
    "Other \tFC \tFunnel Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table hourly2_indexed : 204600 obs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "listColsSC_code: List[String]\n",
       "listColsSCWT: List[String] = List(S_1_code, S_2_code, S_3_code, IN, VC, DR, PR, OB)\n",
       "arrayColsSCWT: Array[String] = Array(S_1_code, S_2_code, S_3_code, IN, VC, DR, PR, OB)\n",
       "SCindexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_66d3c9d360fe, strIdx_4846ed1ed5c8, strIdx_be4a97f3d60c, strIdx_e35ef3ac5fde, strIdx_f6d124e9c6ca, strIdx_4bb58ef0f801, strIdx_307d9c82398b, strIdx_5fad8473e7e9)\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_0d82ca9d688f\n",
       "hourly2_indexed: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 35 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// string index the categorical attributes S_i_code, IN, VC, DR, PR, OB => hourly2_indexed\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "def listColsSC_code = for (i <- (1 to numColsSC).toList) yield (\"S_\"+i+\"_code\")\n",
    "val listColsSCWT = listColsSC_code++List(\"IN\", \"VC\", \"DR\", \"PR\", \"OB\")\n",
    "val arrayColsSCWT = listColsSCWT.toArray\n",
    "\n",
    "val SCindexers = arrayColsSCWT.map\n",
    "{field => \n",
    "    new StringIndexer()\n",
    "    .setInputCol(field)\n",
    "    .setStringOrderType(\"alphabetDesc\") \n",
    "    .setOutputCol(field+\"_x\") \n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "                  .setStages(SCindexers)\n",
    "val hourly2_indexed = pipeline.fit(wType_splitted).transform(wType_splitted)\n",
    "\n",
    "/*hourly2_indexed.select(\"CallSign\",\"Date\",\"Time\",\"NumLayer\",\"S_1_code_x\",\"S_2_code_x\",\"S_3_code_x\",\n",
    "                       \"VC_x\",\"IN_x\",\"DR_x\",\"PR_x\",\"OB_x\").show(50)*/\n",
    "\n",
    "println(s\"Table hourly2_indexed : ${hourly2_indexed.count()} obs.\")\n",
    "//hourly2_indexed.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table hourly2_double : 204600 obs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listColsSC_high: List[String]\n",
       "listColsOther: List[String] = List(T, H, Wd, Ws, P, V, NumLayer)\n",
       "listColsNum: List[String] = List(T, H, Wd, Ws, P, V, NumLayer, S_1_high, S_2_high, S_3_high)\n",
       "hourly2_double: org.apache.spark.sql.DataFrame = [CallSign: string, State: string ... 35 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Tranformation of numeric measures from string to double => hourly2_double\n",
    "\n",
    "def listColsSC_high = for (i <- (1 to numColsSC).toList) yield (\"S_\"+i+\"_high\")\n",
    "val listColsOther = List(\"T\",\"H\",\"Wd\",\"Ws\",\"P\",\"V\",\"NumLayer\")\n",
    "val listColsNum = listColsOther ++ listColsSC_high\n",
    "\n",
    "val hourly2_double = listColsNum.foldLeft(hourly2_indexed){ \n",
    "                     (tempdf, colName) => tempdf.withColumn(colName, col(colName).cast(\"Double\")) }\n",
    "\n",
    "println(s\"Table hourly2_double : ${hourly2_double.count()} obs.\")\n",
    "//hourly2_double.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- GHeight: string (nullable = true)\n",
      " |-- T: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- Wd: double (nullable = true)\n",
      " |-- Ws: double (nullable = true)\n",
      " |-- P: double (nullable = true)\n",
      " |-- S: string (nullable = true)\n",
      " |-- V: double (nullable = true)\n",
      " |-- WType: string (nullable = true)\n",
      " |-- RType: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- DTimeH: timestamp (nullable = true)\n",
      " |-- _t_: integer (nullable = true)\n",
      " |-- NumLayer: double (nullable = false)\n",
      " |-- S_1_code: string (nullable = true)\n",
      " |-- S_1_high: double (nullable = true)\n",
      " |-- S_2_code: string (nullable = true)\n",
      " |-- S_2_high: double (nullable = true)\n",
      " |-- S_3_code: string (nullable = true)\n",
      " |-- S_3_high: double (nullable = true)\n",
      " |-- DR: string (nullable = true)\n",
      " |-- PR: string (nullable = true)\n",
      " |-- OB: string (nullable = true)\n",
      " |-- IN: string (nullable = false)\n",
      " |-- VC: string (nullable = true)\n",
      " |-- S_1_code_x: double (nullable = false)\n",
      " |-- S_2_code_x: double (nullable = false)\n",
      " |-- S_3_code_x: double (nullable = false)\n",
      " |-- IN_x: double (nullable = false)\n",
      " |-- VC_x: double (nullable = false)\n",
      " |-- DR_x: double (nullable = false)\n",
      " |-- PR_x: double (nullable = false)\n",
      " |-- OB_x: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourly2_double.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "listColsBase: List[String]\n",
       "listColsSC_x: List[String]\n",
       "listColsSCond: List[String] = List(Numlayer, S_1_high, S_2_high, S_3_high, SC_1_code_x, SC_2_code_x, SC_3_code_x)\n",
       "listColsWT_x: List[String]\n",
       "needed_cols: List[String] = List(T, H, Wd, Ws, P, V, Numlayer, S_1_high, S_2_high, S_3_high, SC_1_code_x, SC_2_code_x, SC_3_code_x, IN_x, VC_x, DR_x, PR_x, OB_x)\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//List of basis weather data\n",
    "def listColsBase = List(\"T\",\"H\",\"Wd\",\"Ws\",\"P\",\"V\")\n",
    "\n",
    "//List of weather data linked to skyCondition\n",
    "def listColsSC_x = for (i <- (1 to numColsSC).toList) yield (\"S_\"+i+\"_code_x\")\n",
    "val listColsSCond = List(\"Numlayer\")++listColsSC_high++listColsSC_x\n",
    "\n",
    "//List of weather data linked to wType (possibility to restrict column to \"VC_x\" and \"DR_x\")\n",
    "def listColsWT_x = List(\"IN_x\", \"VC_x\", \"DR_x\", \"PR_x\", \"OB_x\")\n",
    "\n",
    "// Colonnes retenues\n",
    "val needed_cols : List[String] = (listColsBase ++ listColsSCond ++ listColsWT_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+----------+----+-------------------+---+------------------------------------------------------------------------------------------------------+\n",
      "|CallSign|State|GHeight|DateH     |Time|DTimeH             |_t_|Weather                                                                                               |\n",
      "+--------+-----+-------+----------+----+-------------------+---+------------------------------------------------------------------------------------------------------+\n",
      "|ABE     |PA   |390    |2013-11-01|0051|2013-11-01 00:00:00|0  |[15.5, 84.0, 200.0, 13.0, 29.3, 10.0, 1.0, 16.0, 0.0, 0.0, 4.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]    |\n",
      "|ABE     |PA   |390    |2013-11-01|0151|2013-11-01 01:00:00|1  |[15.9, 87.0, 200.0, 11.0, 29.27, 5.0, 3.0, 33.0, 47.0, 55.0, 6.0, 4.0, 1.0, 2.0, 3.0, 7.0, 2.0, 4.0]  |\n",
      "|ABE     |PA   |390    |2013-11-01|0251|2013-11-01 02:00:00|2  |[16.1, 84.0, 200.0, 15.0, 29.23, 10.0, 1.0, 28.0, 0.0, 0.0, 2.0, 5.0, 5.0, 1.0, 3.0, 7.0, 2.0, 5.0]   |\n",
      "|ABE     |PA   |390    |2013-11-01|0351|2013-11-01 03:00:00|3  |[16.8, 81.0, 210.0, 15.0, 29.19, 10.0, 3.0, 23.0, 33.0, 95.0, 6.0, 4.0, 1.0, 1.0, 3.0, 7.0, 2.0, 5.0] |\n",
      "|ABE     |PA   |390    |2013-11-01|0451|2013-11-01 04:00:00|4  |[17.4, 81.0, 200.0, 16.0, 29.15, 10.0, 3.0, 22.0, 50.0, 100.0, 4.0, 4.0, 1.0, 1.0, 3.0, 7.0, 2.0, 5.0]|\n",
      "|ABE     |PA   |390    |2013-11-01|0551|2013-11-01 05:00:00|5  |[17.6, 78.0, 200.0, 20.0, 29.13, 10.0, 3.0, 23.0, 29.0, 37.0, 1.0, 4.0, 1.0, 2.0, 3.0, 7.0, 6.0, 5.0] |\n",
      "|ABE     |PA   |390    |2013-11-01|0651|2013-11-01 06:00:00|6  |[17.8, 76.0, 210.0, 20.0, 29.12, 10.0, 2.0, 28.0, 34.0, 0.0, 6.0, 1.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]  |\n",
      "|ABE     |PA   |390    |2013-11-01|0751|2013-11-01 07:00:00|7  |[14.5, 84.0, 250.0, 13.0, 29.19, 10.0, 2.0, 24.0, 55.0, 0.0, 1.0, 1.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]  |\n",
      "|ABE     |PA   |390    |2013-11-01|0851|2013-11-01 08:00:00|8  |[15.0, 84.0, 220.0, 7.0, 29.19, 10.0, 1.0, 65.0, 0.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]    |\n",
      "|ABE     |PA   |390    |2013-11-01|0951|2013-11-01 09:00:00|9  |[14.7, 81.0, 240.0, 14.0, 29.2, 10.0, 2.0, 75.0, 100.0, 0.0, 4.0, 1.0, 5.0, 1.0, 3.0, 7.0, 2.0, 5.0]  |\n",
      "|ABE     |PA   |390    |2013-11-01|1051|2013-11-01 10:00:00|10 |[15.2, 75.0, 230.0, 14.0, 29.2, 10.0, 1.0, 110.0, 0.0, 0.0, 6.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]   |\n",
      "|ABE     |PA   |390    |2013-11-01|1151|2013-11-01 11:00:00|11 |[15.1, 70.0, 240.0, 11.0, 29.19, 10.0, 2.0, 30.0, 100.0, 0.0, 4.0, 4.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0] |\n",
      "|ABE     |PA   |390    |2013-11-01|1251|2013-11-01 12:00:00|12 |[14.9, 61.0, 240.0, 21.0, 29.17, 10.0, 1.0, 120.0, 0.0, 0.0, 4.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]  |\n",
      "|ABE     |PA   |390    |2013-11-01|1351|2013-11-01 13:00:00|13 |[14.9, 57.0, 240.0, 21.0, 29.17, 10.0, 2.0, 45.0, 110.0, 0.0, 4.0, 0.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0] |\n",
      "|ABE     |PA   |390    |2013-11-01|1451|2013-11-01 14:00:00|14 |[14.8, 49.0, 240.0, 20.0, 29.17, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]    |\n",
      "|ABE     |PA   |390    |2013-11-01|1551|2013-11-01 15:00:00|15 |[13.5, 44.0, 250.0, 14.0, 29.16, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]    |\n",
      "|ABE     |PA   |390    |2013-11-01|1651|2013-11-01 16:00:00|16 |[12.6, 47.0, 240.0, 10.0, 29.19, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]    |\n",
      "|ABE     |PA   |390    |2013-11-01|1751|2013-11-01 17:00:00|17 |[9.6, 67.0, 230.0, 6.0, 29.21, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]      |\n",
      "|ABE     |PA   |390    |2013-11-01|1851|2013-11-01 18:00:00|18 |[9.7, 62.0, 250.0, 5.0, 29.24, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]      |\n",
      "|ABE     |PA   |390    |2013-11-01|1951|2013-11-01 19:00:00|19 |[10.2, 58.0, 0.0, 0.0, 29.25, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]       |\n",
      "|ABE     |PA   |390    |2013-11-01|2051|2013-11-01 20:00:00|20 |[9.1, 67.0, 0.0, 0.0, 29.27, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]        |\n",
      "|ABE     |PA   |390    |2013-11-01|2151|2013-11-01 21:00:00|21 |[8.1, 77.0, 0.0, 0.0, 29.3, 10.0, 1.0, 75.0, 0.0, 0.0, 6.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]        |\n",
      "|ABE     |PA   |390    |2013-11-01|2251|2013-11-01 22:00:00|22 |[8.3, 80.0, 340.0, 6.0, 29.27, 10.0, 1.0, 80.0, 0.0, 0.0, 6.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]     |\n",
      "|ABE     |PA   |390    |2013-11-01|2351|2013-11-01 23:00:00|23 |[9.1, 83.0, 310.0, 5.0, 29.26, 10.0, 1.0, 80.0, 0.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]     |\n",
      "|ABE     |PA   |390    |2013-11-02|0051|2013-11-02 00:00:00|0  |[9.1, 77.0, 0.0, 0.0, 29.26, 10.0, 1.0, 70.0, 0.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]       |\n",
      "+--------+-----+-------+----------+----+-------------------+---+------------------------------------------------------------------------------------------------------+\n",
      "only showing top 25 rows\n",
      "\n",
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- GHeight: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- DTimeH: timestamp (nullable = true)\n",
      " |-- _t_: integer (nullable = true)\n",
      " |-- Weather: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, concat_ws}\n",
       "listColsBase: List[String]\n",
       "listColsSC_x: List[String]\n",
       "listColsSCond: List[String] = List(Numlayer, S_1_high, S_2_high, S_3_high, S_1_code_x, S_2_code_x, S_3_code_x)\n",
       "listColsWT_x: List[String]\n",
       "needed_cols: List[String] = List(T, H, Wd, Ws, P, V, Numlayer, S_1_high, S_2_high, S_3_high, S_1_code_x, S_2_code_x, S_3_code_x, IN_x, VC_x, DR_x, PR_x, OB_x)\n",
       "hourly3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string, State: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Concatenation of meteo caracteristics in an array \"Weather\" => hourly3\n",
    "import org.apache.spark.sql.functions.{col,concat_ws}\n",
    "\n",
    "\n",
    "//List of basis weather data\n",
    "def listColsBase = List(\"T\",\"H\",\"Wd\",\"Ws\",\"P\",\"V\")\n",
    "\n",
    "//List of weather data linked to skyCondition\n",
    "def listColsSC_x = for (i <- (1 to numColsSC).toList) yield (\"S_\"+i+\"_code_x\")\n",
    "val listColsSCond = List(\"Numlayer\")++listColsSC_high++listColsSC_x\n",
    "\n",
    "//List of weather data linked to wType (possibility to restrict column to \"VC_x\" and \"DR_x\")\n",
    "def listColsWT_x = List(\"IN_x\", \"VC_x\", \"DR_x\", \"PR_x\", \"OB_x\")\n",
    "\n",
    "// Colonnes retenues (to be ajusted if needed)\n",
    "val needed_cols : List[String] = (listColsBase ++ listColsSCond ++ listColsWT_x)\n",
    "\n",
    "val hourly3 = hourly2_double.withColumn(\"Weather\",array(needed_cols.map(col): _*))\n",
    "                            .select(\"CallSign\",\"State\",\"GHeight\",\"DateH\", \"Time\",\"DTimeH\",\"_t_\",\"Weather\")\n",
    "                            .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "hourly3.show(25, false)\n",
    "hourly3.printSchema\n",
    "\n",
    "\n",
    "// vectorization of weather observation in a unique Weather column : IMPOSSIBLE WITH COLLECT_LIST\n",
    "/*import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "def listSC_code = for (i <- (1 to numCols).toList) yield (\"SC_\"+i+\"_code_ix\")\n",
    "val cols = (listSC_code ++ fields).toArray\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(cols)\n",
    "  .setOutputCol(\"Weather\")\n",
    "\n",
    "val hourly3 = assembler.transform(hourly2_double)\n",
    "                       .select(\"CallSign\",\"DateH\", \"Time\",\"TimeH\",\"_t_\",\"Weather\")\n",
    "                       .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "*/\n",
    "\n",
    "// Concatenation in a struct \n",
    "/*import org.apache.spark.sql.functions.struct\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def listSC_code = for (i <- (1 to numCols).toList) yield (\"SC_\"+i+\"_code_ix\")\n",
    "val needed_cols : List[String] = (listSC_code ++ fields)\n",
    "\n",
    "// Varable Weather regroupant, pour chaque heure, les infos météo retenues\n",
    "val hourly3 = hourly2_double.withColumn(\"Weather\", struct(needed_cols.map(col): _*))\n",
    "                            .select(\"CallSign\",\"DateH\", \"Time\",\"TimeH\",\"_t_\",\"Weather\")\n",
    "                            .sort($\"CallSign\" .asc,$\"DateH\" .asc, $\"Time\")\n",
    "*/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table WOT, nb stations / Date : 8525\n",
      "+--------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CallSign|DateH     |Weather_h                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+--------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|ABE     |2013-11-01|[[15.5, 84.0, 200.0, 13.0, 29.3, 10.0, 1.0, 16.0, 0.0, 0.0, 4.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [15.9, 87.0, 200.0, 11.0, 29.27, 5.0, 3.0, 33.0, 47.0, 55.0, 6.0, 4.0, 1.0, 2.0, 3.0, 7.0, 2.0, 4.0], [16.1, 84.0, 200.0, 15.0, 29.23, 10.0, 1.0, 28.0, 0.0, 0.0, 2.0, 5.0, 5.0, 1.0, 3.0, 7.0, 2.0, 5.0], [16.8, 81.0, 210.0, 15.0, 29.19, 10.0, 3.0, 23.0, 33.0, 95.0, 6.0, 4.0, 1.0, 1.0, 3.0, 7.0, 2.0, 5.0], [17.4, 81.0, 200.0, 16.0, 29.15, 10.0, 3.0, 22.0, 50.0, 100.0, 4.0, 4.0, 1.0, 1.0, 3.0, 7.0, 2.0, 5.0], [17.6, 78.0, 200.0, 20.0, 29.13, 10.0, 3.0, 23.0, 29.0, 37.0, 1.0, 4.0, 1.0, 2.0, 3.0, 7.0, 6.0, 5.0], [17.8, 76.0, 210.0, 20.0, 29.12, 10.0, 2.0, 28.0, 34.0, 0.0, 6.0, 1.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [14.5, 84.0, 250.0, 13.0, 29.19, 10.0, 2.0, 24.0, 55.0, 0.0, 1.0, 1.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [15.0, 84.0, 220.0, 7.0, 29.19, 10.0, 1.0, 65.0, 0.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [14.7, 81.0, 240.0, 14.0, 29.2, 10.0, 2.0, 75.0, 100.0, 0.0, 4.0, 1.0, 5.0, 1.0, 3.0, 7.0, 2.0, 5.0], [15.2, 75.0, 230.0, 14.0, 29.2, 10.0, 1.0, 110.0, 0.0, 0.0, 6.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [15.1, 70.0, 240.0, 11.0, 29.19, 10.0, 2.0, 30.0, 100.0, 0.0, 4.0, 4.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [14.9, 61.0, 240.0, 21.0, 29.17, 10.0, 1.0, 120.0, 0.0, 0.0, 4.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [14.9, 57.0, 240.0, 21.0, 29.17, 10.0, 2.0, 45.0, 110.0, 0.0, 4.0, 0.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [14.8, 49.0, 240.0, 20.0, 29.17, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [13.5, 44.0, 250.0, 14.0, 29.16, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [12.6, 47.0, 240.0, 10.0, 29.19, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [9.6, 67.0, 230.0, 6.0, 29.21, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [9.7, 62.0, 250.0, 5.0, 29.24, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [10.2, 58.0, 0.0, 0.0, 29.25, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [9.1, 67.0, 0.0, 0.0, 29.27, 10.0, 1.0, 0.0, 0.0, 0.0, 7.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [8.1, 77.0, 0.0, 0.0, 29.3, 10.0, 1.0, 75.0, 0.0, 0.0, 6.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [8.3, 80.0, 340.0, 6.0, 29.27, 10.0, 1.0, 80.0, 0.0, 0.0, 6.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0], [9.1, 83.0, 310.0, 5.0, 29.26, 10.0, 1.0, 80.0, 0.0, 0.0, 2.0, 5.0, 5.0, 2.0, 3.0, 7.0, 6.0, 5.0]]|\n",
      "+--------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- Weather_h: array (nullable = true)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WOT_201311: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string, DateH: date ... 1 more field]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Concatenation of all 24 hourly weather observations in a array \"Weather_h\" => WOT_201311\n",
    "val WOT_201311 = hourly3\n",
    "  .groupBy($\"CallSign\",$\"DateH\")\n",
    "  .agg(collect_list(\"Weather\").as(\"Weather_h\"))\n",
    "  .sort($\"CallSign\" .asc,$\"DateH\" .asc)\n",
    "\n",
    "println(s\"Table WOT, nb stations / Date : ${WOT_201311.count()}\")\n",
    "WOT_201311.show(1, false) \n",
    "WOT_201311.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Préparation des données de vols => Flight Table (FT)\n",
    "La Table des données de vols contient les informations sur les vols et est limitée aux vols non détournés (diverted) ou annulés (canceled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examen des variables :\n",
    "* DepTime et ArrTime : heure de départ effective\n",
    "\n",
    "* CRSDepTimee et CRSArrTime : heures de départ et d'arrivée prévues\n",
    "    * Computerized reservation system (CRS) departure/arrival time is the scheduled departure/arrival time of the flight,\n",
    "    * wheels off is the time when the wheels of the aircraftleave the ground at the origin airport, and \n",
    "    * wheels on is the time when the wheels of the aircraft touch theground at the destination airport.\n",
    "\n",
    "* The departure delay of  an  aircraft  is  the  difference  between  the  actual departure  time  and  the  CRS  departure time of the flight. \n",
    "\n",
    "* Arrival delay equals actual arrival time minusthe scheduled arrival time.\n",
    "\n",
    "* \"dep_time_blk\" : créneau horaire. ex. crs_dep_time=1301 => 1300-1359\n",
    "\n",
    "* différence entre dep_delay et dep_delay_new ? ici dep_delay peut être négatif et traduire un vol en avance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1/2 - Limitation du périmètre des vols\n",
    "* Limitation aux vols pours lesquels correspond une sation météo au départ ou à l'arrivée\n",
    "* Limitation aux vols non annulés (canceled) et détournés (diverted)\n",
    "* Récupération, à partir des données de stations (data_station), de la TimeZone pour les aéroports de départ et d'arrivée et calcul du Time_lag entre aéroports\n",
    "* Création d'une clé entre les vols au départ et les vols à l'arrivée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de vols non annulés ou non détournés : 498420\n",
      "Nb de vols non annulés ou non détournés avec station météo : 497177\n",
      "+--------------------+----------+------+-----------+-------+----+-------+-----------------+--------+\n",
      "|                 cle|FlightDate|Origin|Tail_Number|DepTime|Dest|ArrTime|ActualElapsedTime|Time_lag|\n",
      "+--------------------+----------+------+-----------+-------+----+-------+-----------------+--------+\n",
      "|2013-11-01N8747BD...|2013-11-01|   DTW|     N8747B|   2126| BGM|   2238|            72.00|     0.0|\n",
      "|2013-11-03N8883ED...|2013-11-03|   DTW|     N8883E|   2136| BGM|   2251|            75.00|     0.0|\n",
      "|2013-11-04N8709AD...|2013-11-04|   DTW|     N8709A|   2145| BGM|   2259|            74.00|     0.0|\n",
      "|2013-11-05N8918BD...|2013-11-05|   DTW|     N8918B|   2136| BGM|   2245|            69.00|     0.0|\n",
      "|2013-11-06N8914AD...|2013-11-06|   DTW|     N8914A|   2133| BGM|   2242|            69.00|     0.0|\n",
      "+--------------------+----------+------+-----------+-------+----+-------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_vols1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: string, Quarter: string ... 108 more fields]\n",
       "data_vols2: org.apache.spark.sql.DataFrame = [Year: string, Quarter: string ... 112 more fields]\n",
       "FT_201311: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 20 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// novembre 2013\n",
    "\n",
    "val data_vols1 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                      .load(pathA+date1+\"_T_ONTIME_REPORTING_2.csv\")\n",
    "                      .where( ($\"Diverted\" !== 1) || ($\"Cancelled\" != 1) )\n",
    "                      .filter(\"Tail_Number is not null\")\n",
    "                      .filter(\"DepTime is not null\")\n",
    "\n",
    "print(\"Nb de vols non annulés ou non détournés : \")\n",
    "println(data_vols1.count())\n",
    "\n",
    "\n",
    "// Limitation aux vols pour lesquels une station météo est trouvée pour l'aroport de départ et d'arrivée\n",
    "// Calcul du décalage horaire (\"Time_lag\") entre le lieu de départ et d'arrivée\n",
    "val data_vols2  = data_vols1.join(data_station.select(\"CallSign\",\"TimeZone\"), data_vols1(\"Origin\") === data_station(\"CallSign\"),\"inner\")\n",
    "                           .drop(\"CallSign\")\n",
    "                           .withColumnRenamed(\"TimeZone\", \"dep_TZone\")\n",
    "                           .join(data_station.select(\"CallSign\",\"TimeZone\"), data_vols1(\"Dest\") === data_station(\"CallSign\"),\"inner\")\n",
    "                           .drop(\"CallSign\")\n",
    "                           .withColumnRenamed(\"TimeZone\", \"arr_TZone\")\n",
    "                           .withColumn(\"Time_lag\",$\"arr_TZone\"-$\"dep_TZone\")\n",
    "                           .withColumn(\"Cle\", concat($\"FlightDate\", $\"Tail_Number\",$\"Origin\",$\"DepTime\",$\"Dest\",$\"ArrTime\"))\n",
    "\n",
    "println(s\"Nb de vols non annulés ou non détournés avec station météo : ${data_vols2.count()}\")\n",
    "data_vols2.select(\"cle\",\"FlightDate\",\"Origin\", \"Tail_Number\", \"DepTime\",\n",
    "                                   \"Dest\",\"ArrTime\",\"ActualElapsedTime\", \"Time_lag\").show(5)\n",
    "\n",
    "// Table des vols\n",
    "val FT_201311 = data_vols2.select(\"cle\",\"FlightDate\",\"Origin\",\"Tail_Number\", \"Dest\",\"DepTime\",\"CRSDepTime\",\"DepDelay\",\n",
    "                                  \"DepDel15\", \"DepartureDelayGroups\",\"ArrTime\",\"CRSArrTime\",\"ArrDelay\",\"ArrDel15\",\n",
    "                                  \"ActualElapsedTime\",\"Distance\",\"Time_lag\",\n",
    "                                  \"WeatherDelay\",\"NASDelay\",\"SecurityDelay\",\"CarrierDelay\",\"LateAircraftDelay\")                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2/2 - Préparation de la table des vols avant jointure avec les données météo => flightD et flightA\n",
    "\n",
    "En vue du croisement avec les données météo sur les douze dernières heures, deux tables sont créés l'une pour les vols au départ (flightD), l'autre pour les vols à l'arrivée (flightA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Gestion des dates pour les vols\n",
    "\n",
    "La gestion des dates et des heures concernent à la fois les informations au départ et les informations à l'arrivée.\n",
    "La date d'arrivée n'est pas renseignée. Elle est reconstituée (ADate) à partir de la date de départ, de l'heure de départ, de la durée totale du vol (actual_elapsed_time) et du time_lag entre les deux aéroports.\n",
    "Afin de pouvoir croiser les données de vols avec les relevés météo, des heures de référence pour le départ et l'arrivée (DTimeF et ATimeF) sont créées : elles correspondent à l'heure pleine immédiatement inférieure.\n",
    "\n",
    "* Création de la variable DDate correspondant à fl_date en format date\n",
    "\n",
    "* Création de la variable ADate correspondant à la date d'arrivée \n",
    "\n",
    "* Création de la variable DDateF (departure) et ADateF (arrival) correspondant à la date de référence (heure pleine immédiatement inférieure) pour le rapprochement avec les relevés météo\n",
    "    * si l'horaire de vol est après 10h, DateF = fl_date ;\n",
    "    * pour les vols dont une partie des douze dernières heures couvrent le jour précédent, nous créons une nouvelle observation avec DateF au jour précédent (passage de 502507 obs à 706897 obs).\n",
    "\n",
    "* Création de la variable DTime / ATime correspondant à l'heure de départ et DTimeF / ATimeF correspondant à l'heure pleine la plus proche de l'heure de départ\n",
    "\n",
    "* La variable _d_ (departure) et _a_ (arrival) correspond au numéro de l'heure de départ et _d0_ / _a0_ au numéro de l'heure la plus petite du jour de départ(DDateF) ou d'arrivée (ADateF) pour laquelle un relevé météo est à récupérer :\n",
    "    * si l'heure de départ de référence est DTimeF=12:00, pour ce jour : _d_=12 et _d0_=1 et les relevés horaires 1, ..., 12 seront nécessaires (idem pour _a_ et _a0_);\n",
    "    * si l'heure de départ de référence est DTimeF=08:00, pour ce jour : le numéro de l'heure de départ est _d_ = 8 et _d0_=0 et les relevés horaires 0,1,...,8 seront nécessaires (idem pour _a_ et _a0_). A noter que dans ce cas, il sera nécessaire de créer un enregistrement complémentaire avec la date du jour précédent et _d_=23 et _d0_=21 (relevés 21,22,23) seront nécessaires ( voir (ii) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract de vols au départ : \n",
      "+----------+-------+-------------------+----------+-------------------+---+----+-----+------+\n",
      "|DDate     |DepTime|DTime              |DDateF    |DTimeF             |_d_|_d0_|_ddb_|_ddb0_|\n",
      "+----------+-------+-------------------+----------+-------------------+---+----+-----+------+\n",
      "|2013-11-28|0959   |2013-11-28 09:59:00|2013-11-28|2013-11-28 09:00:00|9  |0   |23   |22    |\n",
      "|2013-11-27|0956   |2013-11-27 09:56:00|2013-11-27|2013-11-27 09:00:00|9  |0   |23   |22    |\n",
      "|2013-11-26|0955   |2013-11-26 09:55:00|2013-11-26|2013-11-26 09:00:00|9  |0   |23   |22    |\n",
      "|2013-11-25|0956   |2013-11-25 09:56:00|2013-11-25|2013-11-25 09:00:00|9  |0   |23   |22    |\n",
      "|2013-11-22|0956   |2013-11-22 09:56:00|2013-11-22|2013-11-22 09:00:00|9  |0   |23   |22    |\n",
      "+----------+-------+-------------------+----------+-------------------+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.unix_timestamp\n",
       "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
       "import org.apache.spark.sql.functions.{concat, lit}\n",
       "import org.apache.spark.sql.functions.{hour, minute, second}\n",
       "data_vols3: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 15 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp\n",
    "import org.apache.spark.sql.functions.{to_date, to_timestamp}\n",
    "import org.apache.spark.sql.functions.{concat, lit}\n",
    "import org.apache.spark.sql.functions.{hour,minute,second}\n",
    "\n",
    "// gestion des dates pour le départ : DDate, DTime, DDateF, DTimeF, _d_ et _d0_\n",
    "val data_vols3 = data_vols2.select(\"cle\",\"FlightDate\",\"Origin\", \"Tail_Number\", \"DepTime\",\n",
    "                                   \"Dest\",\"ArrTime\",\"ActualElapsedTime\", \"Time_lag\")\n",
    "                           .withColumn(\"DDate\",to_date(unix_timestamp($\"FlightDate\",\"yyyy-MM-dd\").cast(\"timestamp\")))\n",
    "                           .withColumn(\"DDateF\",to_date(unix_timestamp($\"FlightDate\",\"yyyy-MM-dd\").cast(\"timestamp\")))\n",
    "                           .withColumn(\"DTime\", concat(from_unixtime(unix_timestamp($\"DDate\",\"yyyyMMdd\"),\"yyyy-MM-dd\"), lit(\" \"), \n",
    "                                                     from_unixtime(unix_timestamp($\"DepTime\",\"HHmm\"),\"HH:mm:ss\")).cast(\"timestamp\"))\n",
    "                           .withColumn(\"Time00\",concat(substring($\"DepTime\",1,2),lit(\"00\")))\n",
    "                           .withColumn(\"DTimeF\", concat(from_unixtime(unix_timestamp($\"DDate\",\"yyyyMMdd\"),\"yyyy-MM-dd\"), lit(\" \"),\n",
    "                                                      from_unixtime(unix_timestamp($\"Time00\",\"HHmm\"),\"HH:mm:ss\")).cast(\"timestamp\"))                     \n",
    "                           .withColumn(\"_d_\", hour($\"DTimeF\"))\n",
    "                           .withColumn(\"_d0_\", when($\"_d_\" > 11, $\"_d_\"-11).otherwise(0))\n",
    "                           .withColumn(\"_ddb_\",when($\"_d_\" <11, 23))            // renseigné si chevauchement sur deux dates\n",
    "                           .withColumn(\"_ddb0_\", when($\"_d_\" <11, $\"_d_\"+13))   // renseigné si chevauchement sur deux dates           \n",
    "                           .drop(\"Time00\")\n",
    "\n",
    "println(\"Extract de vols au départ : \")\n",
    "data_vols3.select(\"DDate\",\"DepTime\",\"DTime\",\"DDateF\",\"DTimeF\",\"_d_\", \"_d0_\", \"_ddb_\", \"_ddb0_\").show(5, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract de vols à l'arrivée : \n",
      "+----+----------+-------+-------+----------+-------------------+---+----+-----+------+\n",
      "|Dest|DDate     |DepTime|ArrTime|ADateF    |ATimeF             |_a_|_a0_|_adb_|_adb0_|\n",
      "+----+----------+-------+-------+----------+-------------------+---+----+-----+------+\n",
      "|BGM |2013-11-01|2126   |2238   |2013-11-01|2013-11-01 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-03|2136   |2251   |2013-11-03|2013-11-03 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-04|2145   |2259   |2013-11-04|2013-11-04 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-05|2136   |2245   |2013-11-05|2013-11-05 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-06|2133   |2242   |2013-11-06|2013-11-06 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-07|2136   |2251   |2013-11-07|2013-11-07 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-08|2136   |2303   |2013-11-08|2013-11-08 23:00:00|23 |12  |null |null  |\n",
      "|BGM |2013-11-10|2136   |2249   |2013-11-10|2013-11-10 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-11|2143   |2249   |2013-11-11|2013-11-11 22:00:00|22 |11  |null |null  |\n",
      "|BGM |2013-11-12|2142   |2309   |2013-11-12|2013-11-12 23:00:00|23 |12  |null |null  |\n",
      "+----+----------+-------+-------+----------+-------------------+---+----+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_vols4: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 25 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// gestion des dates pour l'arrivée : ADate, ATime, ADateF, ATimeF, _a_ et _a0_\n",
    "val data_vols4 = data_vols3.withColumn(\"departure_time\",from_unixtime(unix_timestamp($\"DepTime\",\"HHmm\"),\"HH:mm:ss\"))\n",
    "                           .withColumn(\"Time0\",from_unixtime(unix_timestamp(lit(\"2359\"),\"HHmm\"),\"HH:mm:ss\"))\n",
    "                           .withColumn(\"Timeto0\",round( (unix_timestamp($\"Time0\",\"HH:mm:ss\") - \n",
    "                                                 unix_timestamp($\"departure_time\",\"HH:mm:ss\")+1)/60, 2))\n",
    "                           .withColumn(\"CorrElapsedTime\",$\"ActualElapsedTime\" + $\"Time_lag\"*60)\n",
    "                           .withColumn(\"ADate\", when($\"CorrElapsedTime\" > $\"TimeTo0\", date_add(to_date($\"DDate\",\"yyyy-MM-dd\"),+1))\n",
    "                                                .otherwise($\"DDate\"))\n",
    "                           .withColumn(\"ADateF\",$\"ADate\")\n",
    "                           .withColumn(\"ATime\", concat(from_unixtime(unix_timestamp($\"ADate\",\"yyyyMMdd\"),\"yyyy-MM-dd\"), lit(\" \"), \n",
    "                                                     from_unixtime(unix_timestamp($\"ArrTime\",\"HHmm\"),\"HH:mm:ss\")).cast(\"timestamp\"))\n",
    "                           .withColumn(\"Time00\",concat(substring($\"ArrTime\",1,2),lit(\"00\")))\n",
    "                           .withColumn(\"ATimeF\", concat(from_unixtime(unix_timestamp($\"ADate\",\"yyyyMMdd\"),\"yyyy-MM-dd\"), lit(\" \"),\n",
    "                                                      from_unixtime(unix_timestamp($\"Time00\",\"HHmm\"),\"HH:mm:ss\")).cast(\"timestamp\"))\n",
    "                           .withColumn(\"_a_\", hour($\"ATimeF\"))\n",
    "                           .withColumn(\"_a0_\", when($\"_a_\" > 11, $\"_a_\"-11).otherwise(0))\n",
    "                           .withColumn(\"_adb_\",when($\"_a_\" <11, 23))\n",
    "                           .withColumn(\"_adb0_\", when($\"_a_\" <11, $\"_a_\"+13))            \n",
    "                           .drop(\"Time0\",\"TimeTo0\",\"departure_time\")\n",
    "\n",
    "println(\"Extract de vols à l'arrivée : \")\n",
    "data_vols4.select(\"Dest\",\"DDate\",\"DepTime\",\"ArrTime\",\"ADateF\",\"ATimeF\",\"_a_\", \"_a0_\", \"_adb_\", \"_adb0_\").show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Création des enregistrements supplémentaires\n",
    "Lorque les douzes heures qui précèdent l'heure de départ ou l'heure d'arrivée des vols chevauchent le jour qui précède le départ ou l'arrivée, de nouveaux enregistrements sont créés avec comme DDateF ou ADateF la date du jours qui précède. Ces enregistrements permettront de récupérer les données météo de la veille.\n",
    "Deux tables sont créées :\n",
    "* flightD pour les informations de départ ;\n",
    "* filghtA pour les informations d'arrivée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightD. Nb vols/Date au départ après enrichissement : 668074 \n",
      "\n",
      "Extract de vols au départ, aéroport ADK : \n",
      "+----------+----------+----------+------+-----------+-------+-------------------+-------------------+---+----+\n",
      "|FlightDate|DDate     |DDateF    |Origin|Tail_Number|DepTime|DTime              |DTimeF             |_d_|_d0_|\n",
      "+----------+----------+----------+------+-----------+-------+-------------------+-------------------+---+----+\n",
      "|2013-11-03|2013-11-03|2013-11-03|ADK   |N771AS     |1639   |2013-11-03 16:39:00|2013-11-03 16:00:00|16 |5   |\n",
      "|2013-11-10|2013-11-10|2013-11-10|ADK   |N799AS     |1815   |2013-11-10 18:15:00|2013-11-10 18:00:00|18 |7   |\n",
      "|2013-11-07|2013-11-07|2013-11-07|ADK   |N771AS     |1828   |2013-11-07 18:28:00|2013-11-07 18:00:00|18 |7   |\n",
      "|2013-11-21|2013-11-21|2013-11-21|ADK   |N713AS     |1733   |2013-11-21 17:33:00|2013-11-21 17:00:00|17 |6   |\n",
      "|2013-11-24|2013-11-24|2013-11-24|ADK   |N799AS     |1753   |2013-11-24 17:53:00|2013-11-24 17:00:00|17 |6   |\n",
      "+----------+----------+----------+------+-----------+-------+-------------------+-------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Extract de vols au départ, nouveaux enregistrements : \n",
      "+----------+----------+----------+------+-----------+-------+-------------------+-------------------+---+----+\n",
      "|FlightDate|DDate     |DDateF    |Origin|Tail_Number|DepTime|DTime              |DTimeF             |_d_|_d0_|\n",
      "+----------+----------+----------+------+-----------+-------+-------------------+-------------------+---+----+\n",
      "|2013-11-01|2013-11-01|2013-10-31|DTW   |N8747B     |0958   |2013-11-01 09:58:00|2013-11-01 09:00:00|23 |22  |\n",
      "|2013-11-03|2013-11-03|2013-11-02|DTW   |N8794B     |0955   |2013-11-03 09:55:00|2013-11-03 09:00:00|23 |22  |\n",
      "|2013-11-04|2013-11-04|2013-11-03|DTW   |N8390A     |0955   |2013-11-04 09:55:00|2013-11-04 09:00:00|23 |22  |\n",
      "|2013-11-05|2013-11-05|2013-11-04|DTW   |N8903A     |0952   |2013-11-05 09:52:00|2013-11-05 09:00:00|23 |22  |\n",
      "|2013-11-06|2013-11-06|2013-11-05|DTW   |N8913A     |1012   |2013-11-06 10:12:00|2013-11-06 10:00:00|23 |23  |\n",
      "+----------+----------+----------+------+-----------+-------+-------------------+-------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vols_dep: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 11 more fields]\n",
       "vols_dep_new: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 11 more fields]\n",
       "flightD: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols au départ\n",
    "val vols_dep = data_vols4.select(\"cle\",\"FlightDate\",\"DDate\",\"DDateF\",\"Origin\",\"Tail_Number\",\"DepTime\",\"DTime\",\n",
    "                                   \"DTimeF\",\"_d_\",\"_d0_\", \"_ddb_\", \"_ddb0_\") \n",
    "\n",
    "// Nouvel enregistrement pour les vols dont l'heure de départ est inférieure à 12h (DDateF moins un jour)\n",
    "val vols_dep_new = vols_dep.where($\"_d_\"<11)\n",
    "                           .withColumn(\"_d0_\",$\"_ddb0_\")\n",
    "                           .withColumn(\"_d_\",$\"_ddb_\")\n",
    "                           .withColumn(\"DDateF1\", date_add(to_date($\"DDateF\",\"yyyy-MM-dd\"),-1))\n",
    "                           .withColumn(\"DDateF\",$\"DDateF1\")\n",
    "                           .drop(\"DDateF1\")          \n",
    "\n",
    "val flightD = vols_dep.union(vols_dep_new)\n",
    "                      .select(\"cle\",\"FlightDate\",\"DDate\",\"DDateF\",\"Origin\",\"Tail_Number\",\"DepTime\",\"DTime\",\n",
    "                              \"DTimeF\",\"_d_\",\"_d0_\")\n",
    "\n",
    "println(s\"Table flightD. Nb vols/Date au départ après enrichissement : ${flightD.count()} \\n\")\n",
    "println(\"Extract de vols au départ, aéroport ADK : \")\n",
    "flightD.where($\"Origin\"===\"ADK\").drop(\"cle\").show(5, false)\n",
    "println(\"Extract de vols au départ, nouveaux enregistrements : \")\n",
    "flightD.where($\"DDate\" !== $\"DDateF\").drop(\"cle\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightA. Nb vols/Date à l'arrivée après enrichissement : 610650 \n",
      "\n",
      "Extract de vols à l'arrivée, aéroport ADK : \n",
      "+----------+----------+----------+----+-----------+-------+-------------------+-------------------+---+----+\n",
      "|FlightDate|ADate     |ADateF    |Dest|Tail_Number|ArrTime|ATime              |ATimeF             |_a_|_a0_|\n",
      "+----------+----------+----------+----+-----------+-------+-------------------+-------------------+---+----+\n",
      "|2013-11-03|2013-11-03|2013-11-03|ADK |N771AS     |1544   |2013-11-03 15:44:00|2013-11-03 15:00:00|15 |4   |\n",
      "|2013-11-10|2013-11-10|2013-11-10|ADK |N799AS     |1633   |2013-11-10 16:33:00|2013-11-10 16:00:00|16 |5   |\n",
      "|2013-11-07|2013-11-07|2013-11-07|ADK |N771AS     |1601   |2013-11-07 16:01:00|2013-11-07 16:00:00|16 |5   |\n",
      "|2013-11-21|2013-11-21|2013-11-21|ADK |N713AS     |1549   |2013-11-21 15:49:00|2013-11-21 15:00:00|15 |4   |\n",
      "|2013-11-24|2013-11-24|2013-11-24|ADK |N799AS     |1645   |2013-11-24 16:45:00|2013-11-24 16:00:00|16 |5   |\n",
      "+----------+----------+----------+----+-----------+-------+-------------------+-------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Extract de vols à l'arrivée, nouveaux enregistrements : \n",
      "+----------+----------+----------+----+-----------+-------+-------------------+-------------------+---+----+\n",
      "|FlightDate|ADate     |ADateF    |Dest|Tail_Number|ArrTime|ATime              |ATimeF             |_a_|_a0_|\n",
      "+----------+----------+----------+----+-----------+-------+-------------------+-------------------+---+----+\n",
      "|2013-11-17|2013-11-18|2013-11-17|BGM |N8477R     |0019   |2013-11-18 00:19:00|2013-11-18 00:00:00|23 |13  |\n",
      "|2013-11-10|2013-11-10|2013-11-09|BGM |N601XJ     |1055   |2013-11-10 10:55:00|2013-11-10 10:00:00|23 |23  |\n",
      "|2013-11-17|2013-11-17|2013-11-16|BGM |N8938A     |1057   |2013-11-17 10:57:00|2013-11-17 10:00:00|23 |23  |\n",
      "|2013-11-19|2013-11-19|2013-11-18|BGM |N8541D     |1059   |2013-11-19 10:59:00|2013-11-19 10:00:00|23 |23  |\n",
      "|2013-11-26|2013-11-26|2013-11-25|BGM |N840AY     |1059   |2013-11-26 10:59:00|2013-11-26 10:00:00|23 |23  |\n",
      "+----------+----------+----------+----+-----------+-------+-------------------+-------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vols_arr: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 11 more fields]\n",
       "vols_arr_new: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 11 more fields]\n",
       "flightA: org.apache.spark.sql.DataFrame = [cle: string, FlightDate: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols à l'arrivée\n",
    "\n",
    "val vols_arr = data_vols4.select(\"cle\",\"FlightDate\",\"ATime\",\"ADate\",\"ADateF\",\"ATimeF\",\"Dest\",\"Tail_Number\", \"ArrTime\",\n",
    "                                 \"_a_\", \"_a0_\", \"_adb_\", \"_adb0_\")\n",
    "\n",
    "// Nouvel enregistrement pour les vols dont l'heure d'arrivée est inférieure à 12h (ADateF moins un jour)\n",
    "val vols_arr_new = vols_arr.where($\"_a_\"<11)\n",
    "                           .withColumn(\"_a0_\",$\"_adb0_\")\n",
    "                           .withColumn(\"_a_\",$\"_adb_\")\n",
    "                           .withColumn(\"ADateF1\", date_add(to_date($\"ADateF\",\"yyyy-MM-dd\"),-1))\n",
    "                           .withColumn(\"ADateF\",$\"ADateF1\")\n",
    "                           .drop(\"ADateF1\")          \n",
    "\n",
    "val flightA = vols_arr.union(vols_arr_new)\n",
    "                      .select(\"cle\",\"FlightDate\",\"ADate\",\"ADateF\",\"Dest\",\"Tail_Number\",\"ArrTime\",\"ATime\",\n",
    "                              \"ATimeF\",\"_a_\",\"_a0_\")\n",
    "\n",
    "println(s\"Table flightA. Nb vols/Date à l'arrivée après enrichissement : ${flightA.count()} \\n\")\n",
    "println(\"Extract de vols à l'arrivée, aéroport ADK : \")\n",
    "flightA.where($\"Dest\"===\"ADK\").drop(\"cle\").show(5, false)\n",
    "println(\"Extract de vols à l'arrivée, nouveaux enregistrements : \")\n",
    "flightA.where($\"ADate\" !== $\"ADateF\").drop(\"cle\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Table des données de vols enrichie des données météo => JT_201311\n",
    "Construction du dataframe reprenant les vols avec un colonne Weather_d donnant les relevés météo des douze dernières heures précédant l'heure de départ et Weather_a donnant les relevés météo des douze heures précédant l'heure d'arrivée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1/4 : croisement entre le dataframe des vols et le dataframe reprenant le relevé météo par heure\n",
    "Les données météo (table WOT_201311) sont rapprochées des données de vol pour le départ (table flightD) et pour l'arrivée (table flightA). Le rapprochement est fait par aéroport et date.\n",
    "\n",
    "ATTENTION, le croisement correspondant à la première date du jour du mois n'est pas géré ici. Si l'on travaille par mois, il faut préalablement ajouter les relevés météo du dernier jour du mois précédent et du premier jour du mois suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallSign: string (nullable = true)\n",
      " |-- DateH: date (nullable = true)\n",
      " |-- Weather_h: array (nullable = true)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n",
      "8525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_hourly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallSign: string, DateH: date ... 1 more field]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_hourly = WOT_201311\n",
    "df_hourly.printSchema\n",
    "println(df_hourly.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightD_H. Nb vols/Date au départ avec relevé météo : 624273 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightD_H: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, fl_date: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols au départ\n",
    "\n",
    "val flightD_H = flightD.join(df_hourly, flightD(\"Origin\") === df_hourly(\"CallSign\") &&\n",
    "                                         flightD(\"DDateF\") === df_hourly(\"DateH\"), \"inner\" )\n",
    "                       .withColumnRenamed(\"Weather_h\",\"Weather_d\")\n",
    "                       .drop(\"DateH\", \"CallSign\",\"Date\")\n",
    "                       .sort($\"origin\" .asc, $\"DDateF\" .asc, $\"Tail_Number\" .asc, $\"DTime\" .asc)\n",
    "\n",
    "println(s\"Table flightD_H. Nb vols/Date au départ avec relevé météo : ${flightD_H.count()} \\n\")\n",
    "\n",
    "//flightD_H.select(\"fl_date\",\"DDateF\",\"origin\", \"tail_num\",\"DTime\",\"_d_\",\"_d0_\",\"Weather_d\").show(1,false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightA_H. Nb vols/Date à l'arrivée avec relevé météo : 570760 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightA_H: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, fl_date: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cas des vols à l'arrivée\n",
    "\n",
    "val flightA_H = flightA.join(df_hourly, flightA(\"Dest\") === df_hourly(\"CallSign\") &&\n",
    "                                        flightA(\"ADateF\") === df_hourly(\"DateH\"), \"inner\" )\n",
    "                       .withColumnRenamed(\"Weather_h\",\"Weather_a\")\n",
    "                       .drop(\"DateH\", \"CallSign\",\"Date\")\n",
    "                       .sort($\"Dest\" .asc, $\"ADateF\" .asc, $\"Tail_number\" .asc, $\"ATime\" .asc)\n",
    "\n",
    "\n",
    "println(s\"Table flightA_H. Nb vols/Date à l'arrivée avec relevé météo : ${flightA_H.count()} \\n\")\n",
    "\n",
    "//flightA_H.select(\"fl_date\",\"ADateF\",\"dest\", \"tail_num\",\"ATime\",\"_a_\",\"_a0_\",\"Weather_a\").show(1,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2/4 : élimination des relevés météo qui ne correspondant pas aux douze dernières heures précédant le départ ou l'arrivée du vol\n",
    "Pour éliminer les relevés météo non nécessaires, les relevés horaires sont repositionnés en ligne à ligne (explode).\n",
    "Seuls les vols avec 12 relevés horaires disponibles sont retenus (ce qui suppose que les relevés horaires existent pour ce vol sur deux dates adajcentes lorsque les vols partent ou arrivent après 10h du matin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightD_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : 5535744 \n",
      "\n",
      "Exemple de vol au départ : \n",
      "\n",
      "+------+--------+----------+---+----+---+---+--------------------+\n",
      "|origin|dep_time|   fl_date|_d_|_d0_|_t_|obs|           Weather_t|\n",
      "+------+--------+----------+---+----+---+---+--------------------+\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  0| 12|[4.0, 5.0, 5.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  1| 12|[6.0, 4.0, 1.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  2| 12|[2.0, 5.0, 5.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  3| 12|[6.0, 4.0, 1.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  4| 12|[4.0, 4.0, 1.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  5| 12|[1.0, 4.0, 1.0, 2...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  6| 12|[6.0, 1.0, 5.0, 2...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  7| 12|[1.0, 1.0, 5.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  8| 12|[2.0, 5.0, 5.0, 7...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0|  9| 12|[4.0, 1.0, 5.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0| 10| 12|[6.0, 5.0, 5.0, 1...|\n",
      "|   ABE|    1140|2013-11-01| 11|   0| 11| 12|[4.0, 4.0, 5.0, 1...|\n",
      "+------+--------+----------+---+----+---+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{explode, flatten}\n",
       "w_hourD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@2eda0e7d\n",
       "w_obsD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3d98249b\n",
       "flightD_H_explode: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, origin: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{explode, flatten}\n",
    "\n",
    "// cas des vols au départ\n",
    "\n",
    "val w_hourD = Window\n",
    "                .partitionBy(\"origin\",\"DDate\",\"DDateF\",\"tail_num\",\"DTime\")\n",
    "                .orderBy($\"DTime\" .asc)\n",
    "val w_obsD = Window\n",
    "                .partitionBy(\"origin\",\"DDate\",\"tail_num\",\"DTime\")\n",
    "                .orderBy($\"DTime\" .asc)\n",
    "\n",
    "val flightD_H_explode = flightD_H.withColumn(\"Weather_t\",explode($\"Weather_d\"))\n",
    "                                 .withColumn(\"rn\", row_number over w_hourD)\n",
    "                                 .withColumn(\"_t_\", $\"rn\"-1 )\n",
    "                                 .where($\"_t_\">=$\"_d0_\" && $\"_t_\"<= $\"_d_\")  // sélection des relevés horaires\n",
    "                                 .withColumn(\"obs\", count(\"DTime\") over w_obsD)\n",
    "                                 .where($\"obs\" === 12)                       // limitation à 12 relevés horaires\n",
    "                                 .select(\"cle\",\"Origin\",\"DepTime\",\"FlightDate\",\"DDate\",\"DDateF\",\"Tail_Number\",\"DTime\",\"_d_\",\"_d0_\",\"_t_\",\"obs\",\"Weather_t\")\n",
    "                                 .sort($\"Origin\" .asc, $\"FlightDate\" .asc, $\"Tail_Number\" .asc, $\"DTime\" .asc, $\"DDateF\" .asc, $\"_t_\" .asc)\n",
    "\n",
    "println(s\"Table flightD_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : ${flightD_H_explode.count()} \\n\")\n",
    "\n",
    "println(\"Exemple de vol au départ : \\n\")\n",
    "flightD_H_explode.select(\"Origin\",\"DepTime\",\"FlightDate\",\"_d_\",\"_d0_\",\"_t_\",\"obs\",\"Weather_t\")\n",
    "                 .where($\"Origin\"===\"ABE\" && $\"DepTime\" === \"1140\" && $\"Tail_Number\"===\"N29906\")\n",
    "                 .show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table flightA_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : 5557104 \n",
      "\n",
      "Exemple de vol à l'arrivée : \n",
      "\n",
      "+----+--------+----------+----------+--------+-------------------+---+----+---+---+--------------------+\n",
      "|dest|arr_time|   fl_date|    ADateF|tail_num|              ATime|_a_|_a0_|_t_|obs|           Weather_t|\n",
      "+----+--------+----------+----------+--------+-------------------+---+----+---+---+--------------------+\n",
      "| ABQ|    0806|2013-11-02|2013-11-01|  N607SW|2013-11-02 08:06:00| 23|  21| 21| 12|[7.0, 5.0, 5.0, 9...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-01|  N607SW|2013-11-02 08:06:00| 23|  21| 22| 12|[7.0, 5.0, 5.0, 7...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-01|  N607SW|2013-11-02 08:06:00| 23|  21| 23| 12|[7.0, 5.0, 5.0, 6...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  0| 12|[7.0, 5.0, 5.0, 5...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  1| 12|[7.0, 5.0, 5.0, 5...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  2| 12|[7.0, 5.0, 5.0, 6...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  3| 12|[7.0, 5.0, 5.0, 5...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  4| 12|[7.0, 5.0, 5.0, 6...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  5| 12|[7.0, 5.0, 5.0, 0...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  6| 12|[7.0, 5.0, 5.0, 6...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  7| 12|[7.0, 5.0, 5.0, 0...|\n",
      "| ABQ|    0806|2013-11-02|2013-11-02|  N607SW|2013-11-02 08:06:00|  8|   0|  8| 12|[7.0, 5.0, 5.0, 0...|\n",
      "+----+--------+----------+----------+--------+-------------------+---+----+---+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_hourA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@4dcffa40\n",
       "w_obsA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@70fdde03\n",
       "flightA_H_explode: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cle: string, dest: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cas des vols à l'arrivée\n",
    "\n",
    "val w_hourA = Window\n",
    "                .partitionBy(\"Dest\",\"ADate\",\"ADateF\",\"Tail_Number\",\"ATime\")\n",
    "                .orderBy($\"ATime\" .asc)\n",
    "val w_obsA = Window\n",
    "                .partitionBy(\"Dest\",\"ADate\",\"Tail_Number\",\"ATime\")\n",
    "                .orderBy($\"ATime\" .asc)\n",
    "\n",
    "val flightA_H_explode = flightA_H.withColumn(\"Weather_t\",explode($\"Weather_a\"))\n",
    "                                 .withColumn(\"rn\", row_number over w_hourA)\n",
    "                                 .withColumn(\"_t_\", $\"rn\"-1 )\n",
    "                                 .where($\"_t_\">=$\"_a0_\" && $\"_t_\"<= $\"_a_\")\n",
    "                                 .withColumn(\"obs\", count(\"ATime\") over w_obsA)\n",
    "                                 .where($\"obs\"===12)\n",
    "                                 .select(\"cle\",\"Dest\",\"ArrTime\",\"FlightDate\",\"ADate\",\"ADateF\",\"Tail_Number\",\"ATime\",\"_a_\",\"_a0_\",\"_t_\",\"obs\",\"Weather_t\")\n",
    "                                 .sort($\"Dest\" .asc, $\"FlightDate\" .asc, $\"Tail_Number\" .asc, $\"ATime\" .asc, $\"ADateF\" .asc, $\"_t_\" .asc)\n",
    "\n",
    "println(s\"Table flightA_H_explode. Nb vols/Dates au départ avec 12 relevés horaires : ${flightA_H_explode.count()} \\n\")\n",
    "\n",
    "println(\"Exemple de vol à l'arrivée : \\n\")\n",
    "flightA_H_explode.select(\"Dest\",\"ArrTime\",\"FlightDate\",\"ADateF\",\"Tail_Number\",\"ATime\",\"_a_\",\"_a0_\",\"_t_\",\"obs\",\"Weather_t\")\n",
    "                 .where($\"Dest\"===\"ABQ\" && $\"ArrTime\" === \"0806\" && $\"Tail_Number\"===\"N607SW\")\n",
    "                 .show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3/4 : vols avec leurs 12 relevés météo\n",
    "On reconstitue pour chaque vol la colonne \"Weather_d\" (départ) et \"Weather_a\" (arrivée) regroupant les 12 derniers relevés météo disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb vols/Date après récupération 12 dernières heures infos météo : 461312 \n",
      "\n",
      "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cle                           |Weather_d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-18N678CAABE0805ATL1007|[[2.0, 5.0, 5.0, 10.0, 29.42, 63.0, 50.0, 40.5, 1.0, 14.0, 0.0, 0.0], [2.0, 5.0, 5.0, 8.0, 29.38, 63.0, 50.0, 40.5, 1.0, 14.0, 0.0, 0.0], [2.0, 5.0, 5.0, 14.0, 29.32, 63.0, 50.0, 40.5, 1.0, 14.0, 0.0, 0.0], [2.0, 5.0, 5.0, 11.0, 29.27, 63.0, 47.0, 40.5, 1.0, 14.0, 0.0, 0.0], [6.0, 4.0, 1.0, 14.0, 29.22, 63.0, 47.0, 40.5, 3.0, 14.0, 22.0, 26.0], [1.0, 1.0, 5.0, 14.0, 29.21, 63.0, 47.0, 40.5, 2.0, 8.0, 12.0, 0.0], [4.0, 1.0, 5.0, 5.0, 29.24, 63.0, 47.0, 40.5, 2.0, 49.0, 70.0, 0.0], [2.0, 5.0, 5.0, 13.0, 29.25, 63.0, 47.0, 40.5, 1.0, 95.0, 0.0, 0.0], [4.0, 0.0, 5.0, 10.0, 29.25, 63.0, 47.0, 40.5, 2.0, 100.0, 120.0, 0.0], [7.0, 5.0, 5.0, 0.0, 29.26, 63.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 6.0, 29.28, 63.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 10.0, 29.29, 63.0, 47.0, 40.5, 1.0, 0.0, 0.0, 0.0]]|\n",
      "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_weatherD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6bf5a273\n",
       "w_rnD: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@2866e37\n",
       "flightD_H12: org.apache.spark.sql.DataFrame = [cle: string, Weather_d: array<array<double>>]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vols au départ\n",
    "\n",
    "val w_weatherD = Window\n",
    "                .partitionBy(\"Origin\",\"FlightDate\",\"Tail_Number\")\n",
    "                .orderBy($\"DepTime\" .asc)\n",
    "\n",
    "val w_rnD = Window\n",
    "           .partitionBy(\"origin\",\"FlightDate\",\"Tail_Number\",\"DepTime\")\n",
    "           .orderBy($\"DepTime\" .asc)\n",
    "\n",
    "val flightD_H12 = flightD_H_explode.withColumn(\"Weather_d\", collect_list(\"Weather_t\") over w_weatherD)\n",
    "                                   .withColumn(\"rn\", row_number over w_rnD )\n",
    "                                   .where($\"rn\"===12)\n",
    "                                   .select(\"cle\",\"Weather_d\")\n",
    "\n",
    "println(s\"Nb vols/Date après récupération 12 dernières heures infos météo : ${flightD_H12.count()} \\n\")\n",
    "      \n",
    "flightD_H12.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb vols/Date après récupération 12 dernières heures infos météo : 463090 \n",
      "\n",
      "+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|c_cle                         |Weather_a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-21N976EVATL1449ABE1643|[[7.0, 5.0, 5.0, 0.0, 30.17, 50.0, 21.0, 40.5, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 0.0, 30.17, 50.0, 21.0, 40.5, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 0.0, 30.16, 50.0, 21.0, 40.5, 1.0, 0.0, 0.0, 0.0], [6.0, 5.0, 5.0, 0.0, 30.18, 50.0, 21.0, 40.5, 1.0, 30.0, 0.0, 0.0], [6.0, 1.0, 5.0, 6.0, 30.17, 50.0, 21.0, 40.5, 2.0, 29.0, 60.0, 0.0], [2.0, 5.0, 5.0, 5.0, 30.14, 50.0, 21.0, 40.5, 1.0, 31.0, 0.0, 0.0], [4.0, 5.0, 5.0, 3.0, 30.11, 50.0, 21.0, 40.5, 1.0, 60.0, 0.0, 0.0], [6.0, 5.0, 5.0, 0.0, 30.08, 50.0, 21.0, 40.5, 1.0, 55.0, 0.0, 0.0], [2.0, 5.0, 5.0, 0.0, 30.05, 50.0, 21.0, 40.5, 1.0, 50.0, 0.0, 0.0], [2.0, 5.0, 5.0, 0.0, 30.05, 50.0, 21.0, 40.5, 1.0, 50.0, 0.0, 0.0], [2.0, 5.0, 5.0, 3.0, 30.03, 50.0, 21.0, 40.5, 1.0, 44.0, 0.0, 0.0], [2.0, 5.0, 5.0, 0.0, 30.02, 50.0, 21.0, 40.5, 1.0, 38.0, 0.0, 0.0]]|\n",
      "+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w_weatherA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1ace1677\n",
       "w_rnA: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@60ae517a\n",
       "flightA_H12: org.apache.spark.sql.DataFrame = [c_cle: string, Weather_a: array<array<double>>]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Vols à l'arrivée\n",
    "\n",
    "val w_weatherA = Window\n",
    "                .partitionBy(\"Dest\",\"FlightDate\",\"Tail_Number\")\n",
    "                .orderBy($\"ArrTime\" .asc)\n",
    "\n",
    "val w_rnA = Window\n",
    "           .partitionBy(\"Dest\",\"FlightDate\",\"Tail_Number\",\"ArrTime\")\n",
    "           .orderBy($\"ArrTime\" .asc)\n",
    "\n",
    "val flightA_H12 = flightA_H_explode.withColumn(\"Weather_a\", collect_list(\"Weather_t\") over w_weatherA)\n",
    "                                   .withColumn(\"rn\", row_number over w_rnA )\n",
    "                                   .where($\"rn\"===12)\n",
    "                                   .withColumnRenamed(\"cle\",\"c_cle\")\n",
    "                                   .select(\"c_cle\",\"Weather_a\")\n",
    "\n",
    "println(s\"Nb vols/Date après récupération 12 dernières heures infos météo : ${flightA_H12.count()} \\n\")\n",
    "      \n",
    "flightA_H12.show(1,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4/4 : regroupement des vols (départ / arrivée) via la clé => JT_201311\n",
    "Les tables des données météo associées aux vols au départ et à l'arrivée sont regroupées avec la table des vols FT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433248\n",
      "Table JT_201311. Nb vols/Date après récupération des données météo : 433248 \n",
      "+------------------------------+----------+------+--------+----+--------+------------+---------+-------------+---------+---------------+--------+------------+---------+-------------+---------+-------------------+--------+--------+-------------+---------+--------------+-------------+-------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cle                           |fl_date   |origin|tail_num|dest|dep_time|crs_dep_time|dep_delay|dep_delay_new|dep_del15|dep_delay_group|arr_time|crs_arr_time|arr_delay|arr_delay_new|arr_del15|ACTUAL_ELAPSED_TIME|distance|Time_lag|weather_delay|nas_delay|security_delay|carrier_delay|late_aircraft_delay|Weather_d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Weather_a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+------------------------------+----------+------+--------+----+--------+------------+---------+-------------+---------+---------------+--------+------------+---------+-------------+---------+-------------------+--------+--------+-------------+---------+--------------+-------------+-------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2013-11-01N11176DEN1738SLC1959|2013-11-01|DEN   |N11176  |SLC |1738    |1742        |-4.00    |0.00         |0.00     |-1             |1959    |1911        |48.00    |48.00        |1.00     |141.00             |391.00  |0.0     |0.00         |48.00    |0.00          |0.00         |0.00               |[[6.0, 1.0, 5.0, 7.0, 24.54, 51.0, 27.0, 41.1, 2.0, 80.0, 90.0, 0.0], [4.0, 4.0, 5.0, 5.0, 24.55, 51.0, 27.0, 41.1, 2.0, 75.0, 85.0, 0.0], [1.0, 5.0, 5.0, 0.0, 24.57, 51.0, 27.0, 41.1, 1.0, 80.0, 0.0, 0.0], [4.0, 4.0, 5.0, 13.0, 24.6, 51.0, 27.0, 41.1, 2.0, 70.0, 110.0, 0.0], [6.0, 4.0, 5.0, 0.0, 24.63, 51.0, 27.0, 41.1, 2.0, 70.0, 100.0, 0.0], [6.0, 5.0, 5.0, 11.0, 24.65, 51.0, 27.0, 41.1, 1.0, 95.0, 0.0, 0.0], [6.0, 4.0, 5.0, 11.0, 24.68, 51.0, 27.0, 41.1, 2.0, 80.0, 100.0, 0.0], [1.0, 4.0, 1.0, 11.0, 24.69, 51.0, 27.0, 41.1, 3.0, 60.0, 70.0, 85.0], [6.0, 4.0, 5.0, 18.0, 24.71, 51.0, 27.0, 41.1, 2.0, 45.0, 55.0, 0.0], [1.0, 4.0, 5.0, 16.0, 24.7, 51.0, 27.0, 41.1, 2.0, 49.0, 60.0, 0.0], [6.0, 4.0, 5.0, 11.0, 24.69, 51.0, 27.0, 41.1, 2.0, 50.0, 70.0, 0.0], [4.0, 4.0, 4.0, 9.0, 24.69, 51.0, 27.0, 41.1, 3.0, 55.0, 75.0, 85.0], [6.0, 4.0, 5.0, 0.0, 24.63, 51.0, 27.0, 41.1, 2.0, 70.0, 100.0, 0.0], [6.0, 5.0, 5.0, 11.0, 24.65, 51.0, 27.0, 41.1, 1.0, 95.0, 0.0, 0.0], [6.0, 4.0, 5.0, 11.0, 24.68, 51.0, 27.0, 41.1, 2.0, 80.0, 100.0, 0.0], [1.0, 4.0, 1.0, 11.0, 24.69, 51.0, 27.0, 41.1, 3.0, 60.0, 70.0, 85.0], [6.0, 4.0, 5.0, 18.0, 24.71, 51.0, 27.0, 41.1, 2.0, 45.0, 55.0, 0.0], [1.0, 4.0, 5.0, 16.0, 24.7, 51.0, 27.0, 41.1, 2.0, 49.0, 60.0, 0.0], [6.0, 4.0, 5.0, 11.0, 24.69, 51.0, 27.0, 41.1, 2.0, 50.0, 70.0, 0.0], [4.0, 4.0, 4.0, 9.0, 24.69, 51.0, 27.0, 41.1, 3.0, 55.0, 75.0, 85.0], [1.0, 5.0, 5.0, 7.0, 24.7, 51.0, 27.0, 41.1, 1.0, 70.0, 0.0, 0.0], [1.0, 5.0, 5.0, 3.0, 24.7, 51.0, 27.0, 41.1, 1.0, 70.0, 0.0, 0.0], [1.0, 5.0, 5.0, 5.0, 24.71, 51.0, 27.0, 41.1, 1.0, 70.0, 0.0, 0.0], [4.0, 5.0, 5.0, 7.0, 24.72, 51.0, 27.0, 41.1, 1.0, 70.0, 0.0, 0.0]]|[[4.0, 4.0, 5.0, 0.0, 25.98, 60.0, 43.0, 44.4, 2.0, 45.0, 60.0, 0.0], [4.0, 0.0, 5.0, 0.0, 25.98, 60.0, 43.0, 44.4, 2.0, 45.0, 55.0, 0.0], [4.0, 0.0, 5.0, 0.0, 25.98, 60.0, 43.0, 44.4, 2.0, 45.0, 60.0, 0.0], [1.0, 5.0, 5.0, 0.0, 25.97, 60.0, 43.0, 44.4, 1.0, 50.0, 0.0, 0.0], [1.0, 5.0, 5.0, 6.0, 25.95, 60.0, 43.0, 44.4, 1.0, 46.0, 0.0, 0.0], [4.0, 5.0, 5.0, 0.0, 25.94, 60.0, 43.0, 44.4, 1.0, 45.0, 0.0, 0.0], [4.0, 5.0, 5.0, 6.0, 25.93, 60.0, 43.0, 44.4, 1.0, 50.0, 0.0, 0.0], [4.0, 5.0, 5.0, 5.0, 25.92, 60.0, 43.0, 44.4, 1.0, 50.0, 0.0, 0.0], [7.0, 5.0, 5.0, 3.0, 25.92, 60.0, 43.0, 44.4, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 3.0, 25.92, 60.0, 43.0, 44.4, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 3.0, 25.92, 60.0, 43.0, 44.4, 1.0, 0.0, 0.0, 0.0], [7.0, 5.0, 5.0, 8.0, 25.91, 60.0, 43.0, 44.4, 1.0, 0.0, 0.0, 0.0]]|\n",
      "+------------------------------+----------+------+--------+----+--------+------------+---------+-------------+---------+---------------+--------+------------+---------+-------------+---------+-------------------+--------+--------+-------------+---------+--------------+-------------+-------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDA_H12: org.apache.spark.sql.DataFrame = [c_cle: string, Weather_d: array<array<double>> ... 1 more field]\n",
       "JT_201311: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDA_H12 = flightD_H12.join(flightA_H12, flightD_H12(\"cle\")===flightA_H12(\"c_cle\"), \"inner\")\n",
    "                              .drop(\"c_cle\")\n",
    "                              .withColumnRenamed(\"cle\",\"c_cle\")\n",
    "\n",
    "val JT_201311 = FT_201311.join(flightDA_H12, \n",
    "                               FT_201311(\"cle\") === flightDA_H12(\"c_cle\"), \"inner\")\n",
    "                              .drop(\"c_cle\")\n",
    "\n",
    "print(s\"Table JT_201311. Nb vols/Date après récupération des données météo : ${JT_201311.count()} \\n\")\n",
    "JT_201311.show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cle: string (nullable = true)\n",
      " |-- fl_date: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- tail_num: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- crs_dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- dep_delay_new: string (nullable = true)\n",
      " |-- dep_del15: string (nullable = true)\n",
      " |-- dep_delay_group: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- crs_arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- arr_delay_new: string (nullable = true)\n",
      " |-- arr_del15: string (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- Time_lag: double (nullable = true)\n",
      " |-- weather_delay: string (nullable = true)\n",
      " |-- nas_delay: string (nullable = true)\n",
      " |-- security_delay: string (nullable = true)\n",
      " |-- carrier_delay: string (nullable = true)\n",
      " |-- late_aircraft_delay: string (nullable = true)\n",
      " |-- Weather_d: array (nullable = true)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- Weather_a: array (nullable = true)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JT_201311.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Label and target data creation\n",
    "\n",
    "Evaluation of random forest algorithm is done on a combination of 4 parameters :\n",
    "* Target dataset (D1, D2, D3, D4)\n",
    "    * D1 contains only delayed flights due to extreme value weather or NAS, or a combination of them\n",
    "    * D2 contains delayed flights due to extreme value weather + NAS delay if NAS delay > DT\n",
    "    * D3 containes delayed flights due to extreme value weather or NAS, even if not exclusively\n",
    "    * D4 contains all delayed flights\n",
    "* Delay Threshold (DT)\n",
    "    We consider 5 delay threshold (DT) in minutes : DT = 15,30,45,60,90)\n",
    "* Number of hourly weather observations at origin airport (m) : m = 0,1,3,5,7,9,11\n",
    "* Number of hourly weather observations at destination airport (n) : n = 0,1,3,45,7,9,11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "produceTarget: (DFrame: org.apache.spark.sql.DataFrame, dt: Int)org.apache.spark.sql.DataFrame\n",
       "DT: Array[Int] = Array(15, 30, 45, 60, 90)\n",
       "resultDF: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 44 more fields]\n",
       "FT_target_201311: org.apache.spark.sql.DataFrame = [cle: string, fl_date: string ... 44 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fonction construisant les variables cibles (target) selon le seuil de retard donné en paramètre\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def produceTarget(DFrame : DataFrame, dt : Int): DataFrame = {\n",
    "    DFrame.withColumn(\"target_4_\"+dt.toString, when(($\"arr_delay\" >= dt) && ($\"arr_delay\" !== $\"late_aircraft_delay\"), 1).otherwise(0))\n",
    "                            .withColumn(\"target_3_\"+dt.toString, when((col(\"target_4_\"+dt.toString) === 1) && (($\"weather_delay\" > 0) || ($\"nas_delay\" > 0)), 1).otherwise(0))\n",
    "                            .withColumn(\"target_2_\"+dt.toString, when((col(\"target_4_\"+dt.toString) === 1) && (($\"weather_delay\" > 0) || ($\"nas_delay\"> dt)), 1).otherwise(0))\n",
    "                            .withColumn(\"target_1_\"+dt.toString, when((col(\"target_4_\"+dt.toString) === 1) && (($\"weather_delay\" > 0) || ($\"nas_delay\" > 0)) && \n",
    "                                                             ($\"security_delay\" === 0) && ($\"carrier_delay\" === 0), 1).otherwise(0))\n",
    "}\n",
    "\n",
    "// Table JT enrichie des indicateurs de retard\n",
    "\n",
    "val DT = Array(15,30, 45, 60, 90)\n",
    "\n",
    "var resultDF = JT_201311\n",
    "for (i <- 0 to (DT.size-1)) {\n",
    "    resultDF = produceTarget(resultDF,DT(i))\n",
    "}\n",
    "val FT_target_201311 = resultDF\n",
    "\n",
    "\n",
    "/*FT_target_201311.where($\"target_4_30\" === 1)\n",
    "                .withColumnRenamed(\"late_aircraft_delay\",\"l_a_d\")\n",
    "                .withColumnRenamed(\"security_delay\",\"s_d\")\n",
    "                .withColumnRenamed(\"weather_delay\",\"w_d\")\n",
    "                .where($\"target_2_30\"=== 1)\n",
    "                .select(\"arr_delay\",\n",
    "                        \"w_d\",\"nas_delay\",\"s_d\",\"carrier_delay\",\"l_a_d\",\n",
    "                        \"target_1_30\", \"target_2_30\", \"target_3_30\", \"target_4_30\"\n",
    "                             ).show(10)*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Application modèle d'apprentissage supervisé - Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cle: string (nullable = true)\n",
      " |-- fl_date: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- tail_num: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- crs_dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- dep_delay_new: string (nullable = true)\n",
      " |-- dep_del15: string (nullable = true)\n",
      " |-- dep_delay_group: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- crs_arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- arr_delay_new: string (nullable = true)\n",
      " |-- arr_del15: string (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- Time_lag: double (nullable = true)\n",
      " |-- weather_delay: string (nullable = true)\n",
      " |-- nas_delay: string (nullable = true)\n",
      " |-- security_delay: string (nullable = true)\n",
      " |-- carrier_delay: string (nullable = true)\n",
      " |-- late_aircraft_delay: string (nullable = true)\n",
      " |-- Weather_d: array (nullable = true)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- Weather_a: array (nullable = true)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- target_4_15: integer (nullable = false)\n",
      " |-- target_3_15: integer (nullable = false)\n",
      " |-- target_2_15: integer (nullable = false)\n",
      " |-- target_1_15: integer (nullable = false)\n",
      " |-- target_4_30: integer (nullable = false)\n",
      " |-- target_3_30: integer (nullable = false)\n",
      " |-- target_2_30: integer (nullable = false)\n",
      " |-- target_1_30: integer (nullable = false)\n",
      " |-- target_4_45: integer (nullable = false)\n",
      " |-- target_3_45: integer (nullable = false)\n",
      " |-- target_2_45: integer (nullable = false)\n",
      " |-- target_1_45: integer (nullable = false)\n",
      " |-- target_4_60: integer (nullable = false)\n",
      " |-- target_3_60: integer (nullable = false)\n",
      " |-- target_2_60: integer (nullable = false)\n",
      " |-- target_1_60: integer (nullable = false)\n",
      " |-- target_4_90: integer (nullable = false)\n",
      " |-- target_3_90: integer (nullable = false)\n",
      " |-- target_2_90: integer (nullable = false)\n",
      " |-- target_1_90: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FT_target_201311.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Threshold : 15 min\n",
    "// target : target_1\n",
    "// m = 3\n",
    "// n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// selection of label and features\n",
    "import org.apache.spark.ml.linalg._\n",
    "\n",
    "val convertUDF = udf((array : Seq[Double]) => {\n",
    "  Vectors.dense(array.toArray)\n",
    "})\n",
    "\n",
    "val m_d = 1\n",
    "val n_a = 1\n",
    "def Cols_d = for (i <- (0 to m_d-1).toList) yield (\"W_d_\"+i)\n",
    "def Cols_a = for (i <- (0 to n_a-1).toList) yield (\"W_a_\"+i)\n",
    "def listCols = Cols_d ++ Cols_a\n",
    "\n",
    "val FT_201311_model = FT_target_201311.select(col(\"target_1_15\") +: col(\"Weather_a\") +:\n",
    "                            (0 until m_d).map(i => $\"Weather_d\".getItem(11-i).as(s\"W_d_$i\")): _*)\n",
    "                                      .select(col(\"*\") +:\n",
    "                            (0 until n_a).map(i => $\"Weather_a\".getItem(11-i).as(s\"W_a_$i\")): _* )\n",
    "                                      .drop(\"Weather_a\")\n",
    "                                      .withColumn(\"label\", col(\"target_1_15\").cast(\"Double\"))\n",
    "                                      .withColumn(\"Weather\",concat(listCols.map(c => col(c)): _*))\n",
    "                                      .withColumn(\"features\", convertUDF($\"Weather\"))\n",
    "                                      .select(\"label\",\"features\")\n",
    "\n",
    "// sampling\n",
    "val Array(trainingData, testData) = FT_201311_model.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "println(s\"Taille de trainingData : ${trainingData.count()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.050641556926666054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
       "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_32ee11bc60e0\n",
       "dtModel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel: uid=dtc_32ee11bc60e0, depth=5, numNodes=15, numClasses=2, numFeatures=48\n",
       "treeModel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel: uid=dtc_32ee11bc60e0, depth=5, numNodes=15, numClasses=2, numFeatures=48\n",
       "predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n",
       "import org.apache.spark.ml.evaluation...\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// decision tree classifier\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val dt = new DecisionTreeClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  /*.setImpurity(\"entropy\").setMaxBins(maxCat)*/\n",
    "\n",
    "// train model\n",
    "val dtModel = dt.fit(trainingData)\n",
    "\n",
    "val treeModel = dtModel.asInstanceOf[DecisionTreeClassificationModel]\n",
    "//println(s\"Learned classification tree model:\\n ${treeModel.toDebugString}\")\n",
    "\n",
    "// make predictions\n",
    "val predictions = dtModel.transform(testData)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")\n",
    "\n",
    "// feature importance\n",
    "model.featureImportances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
